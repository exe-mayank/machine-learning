{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "1cf75ca9-bede-4357-9f1a-bdd5c467d33c",
      "cell_type": "markdown",
      "source": "1- What is Simple Linear Regression\n\nSimple Linear Regression is a fundamental statistical technique used to explore and model the linear relationship between two continuous variables. Imagine you have two sets of measurements, and you suspect that changes in one set might be related to changes in the other. Simple linear regression helps you formalize this relationship by finding the best-fitting straight line through the data points when you plot them on a graph.\n\nOne variable is designated as the independent variable, which is thought to influence the other variable, called the dependent variable. The regression line essentially summarizes the trend in the data, indicating how the dependent variable changes on average as the independent variable increases or decreases.\n\nThe goal of this method is to determine the equation of this line, which allows you to predict the value of the dependent variable for a given value of the independent variable. The \"best fit\" is typically determined by minimizing the overall difference between the actual data points and the points on the regression line.\n\nSimple linear regression is widely applied in various fields. For instance, you might use it to understand how advertising spending relates to sales revenue, how study time affects exam scores, or how temperature influences energy consumption. It provides a straightforward way to quantify the association between two variables and make predictions based on that association. However, it's important to remember that this method assumes a linear relationship and that other factors might also influence the dependent variable.",
      "metadata": {}
    },
    {
      "id": "faabb9a1-15de-4e5d-9fa4-bacd3d576a0a",
      "cell_type": "markdown",
      "source": "2- What are the key assumptions of Simple Linear Regression\n\nThe key assumptions of Simple Linear Regression are:\n\nLinearity: There is a linear relationship between the independent variable (x) and the dependent variable (y). This means that the change in y for a unit change in x is constant.   \n\n Independence of Errors: The residuals (the differences between the observed and predicted values) are independent of each other. This implies that the error for one data point does not influence the error for another data point. This is particularly important for time series data.   \n\nHomoscedasticity: The variance of the residuals is constant across all levels of the independent variable. In other words, the spread of the errors should be roughly the same for all values of x.   \n\n Normality of Residuals: The residuals are normally distributed. This assumption is primarily important for hypothesis testing and constructing confidence intervals for the model parameters. For large sample sizes, the Central Limit Theorem can mitigate the impact of deviations from normality.   \n\nNo or Little Multicollinearity: In simple linear regression, there is only one independent variable, so multicollinearity (high correlation between independent variables) is not a concern. However, this becomes relevant in multiple linear regression with two or more independent variables.   \n\nThese assumptions are crucial for the validity and reliability of the simple linear regression model. Violations of these assumptions can lead to biased parameter estimates, inefficient predictions, and unreliable statistical inferences. Therefore, it's important to assess these assumptions before interpreting the results of a linear regression analysis.   \n\n\n",
      "metadata": {}
    },
    {
      "id": "5a701481-ead8-4e91-a8dc-ef2eafee06d1",
      "cell_type": "markdown",
      "source": "3- What does the coefficient m represent in the equation Y=mX+c\n\nIn the equation Y=mX+c, the coefficient m represents the slope of the line.   \n\nHere's what that means:\n\nRate of Change: The slope (m) indicates how much the dependent variable (Y) changes for every one-unit increase in the independent variable (X).   \n\nDirection of the Relationship:\n\nIf m is positive, it means that as X increases, Y also tends to increase (a positive relationship). The line will go upwards from left to right.\nIf m is negative, it means that as X increases, Y tends to decrease (a negative or inverse relationship). The line will go downwards from left to right.   \nIf m is zero, it means there is no linear relationship between X and Y (the line will be horizontal).\nSteepness of the Line: The absolute value of m indicates the steepness of the line. A larger absolute value of m means a steeper line, indicating a stronger rate of change in Y for a unit change in X. A smaller absolute value means a flatter line, indicating a weaker rate of change.   \n\nIn the context of Simple Linear Regression where the equation is often written as  \ny =a+bx (or similar), the slope coefficient is analogous to 'b' in that notation. It quantifies the impact of the independent variable on the dependent variable.",
      "metadata": {}
    },
    {
      "id": "339399dd-400b-45fb-bec3-14794d2013f6",
      "cell_type": "markdown",
      "source": "4. What does the intercept c represent in the equation Y=mX+c\n\nIn the equation Y=mX+c, the intercept c represents the y-intercept of the line.\n\nHere's what that means:\n\nValue of Y when X is Zero: The y-intercept (c) is the value of the dependent variable (Y) when the independent variable (X) is equal to zero. It's the point where the regression line crosses the y-axis.\n\nStarting Point (in some contexts): In certain real-world scenarios, the y-intercept can have a meaningful interpretation as a baseline value or a starting point when the independent variable has no effect. For example, if Y represents the cost of a service and X represents the number of units used, c might represent a fixed cost incurred even if no units are used.\n\nNot Always Meaningful: However, it's important to note that the y-intercept might not always have a practical or meaningful interpretation in the context of the data. If X=0 is outside the reasonable range of the independent variable, or if a value of zero for X doesn't make sense in the real world, then the y-intercept serves more as a mathematical anchor for the regression line rather than a directly interpretable value.\n\nIn the context of Simple Linear Regression where the equation is often written as  \ny^=a+bx (or similar), the y-intercept coefficient is analogous to 'a' in that notation. It's the constant term in the regression equation.",
      "metadata": {}
    },
    {
      "id": "405c9e86-2a5e-4732-b8d9-4e4a0b055269",
      "cell_type": "markdown",
      "source": "5. How do we calculate the slope m in Simple Linear Regression\n\nIn Simple Linear Regression, the slope (m) represents the rate of change of the dependent variable (Y) with respect to the independent variable (X). Theoretically, it quantifies how much Y is expected to change for a one-unit increase in X, assuming a linear relationship exists between them.\n\nThe calculation of the slope is rooted in the principle of minimizing the errors between the observed data points and the fitted regression line. Using the Ordinary Least Squares (OLS) method, the slope is derived by finding the line that minimizes the sum of the squared vertical distances between the actual Y values and the predicted Y values ( \nY^) on the line.\n\nMathematically, the formula for the slope m arises from the partial derivatives of the sum of squared errors with respect to m and setting it to zero to find the minimum. This process yields the formula that involves the covariance between X and Y and the variance of X.\n\nConceptually, the slope captures the strength and direction of the linear association. A positive slope indicates a positive relationship (as X increases, Y tends to increase), while a negative slope indicates a negative 1  or inverse relationship (as X increases, Y tends to decrease). The magnitude of the slope reflects the steepness of the line and thus the extent to which a change in X influences Y. A steeper line (larger absolute value of m) implies a stronger impact of X on Y.   \n",
      "metadata": {}
    },
    {
      "id": "afde5c3f-cd9d-483f-ab60-f5f9688cd19f",
      "cell_type": "markdown",
      "source": "6. What is the purpose of the least squares method in Simple Linear Regression\n\nThe purpose of the least squares method in Simple Linear Regression is to find the best-fitting straight line for a given set of data points by minimizing the sum of the squared differences between the observed values of the dependent variable and the values predicted by the linear model.   \n\nHere's a breakdown of why this is important:\n\nFinding the \"Best Fit\": When you plot data points, there are infinitely many lines you could potentially draw through them. The least squares method provides a systematic and objective way to determine the single line that best represents the linear relationship between the variables.   \n\nMinimizing Errors: The vertical distance between each data point and the regression line represents the error or residual for that point. The least squares method squares these errors and then finds the line that makes the total sum of these squared errors as small as possible.   \n\nWhy Squared Errors?\n\nAvoids Cancellation: Squaring the errors ensures that both positive and negative deviations contribute positively to the total error, preventing them from canceling each other out.\nPenalizes Larger Errors More: Larger deviations from the line are penalized more heavily due to the squaring, making the method sensitive to outliers but also ensuring that the line is pulled more towards the majority of the data.   \nMathematical Convenience: Squaring leads to a smoother and mathematically tractable function to minimize using calculus.\nEstimating Coefficients: By minimizing the sum of squared errors, the least squares method provides the formulas to calculate the optimal values for the slope (m or b) and the y-intercept (c or a) of the regression line (Y=mX+c). These coefficients define the specific line of best fit for the data.   \n\nIn essence, the least squares method provides a rigorous and widely accepted criterion for fitting a linear model to data. It aims to create a line that is as close as possible to all the data points, in a well-defined mathematical sense, allowing for meaningful interpretation and prediction of the relationship between the variables.   ",
      "metadata": {}
    },
    {
      "id": "91776031-cbdb-4aa8-882b-0631de238736",
      "cell_type": "markdown",
      "source": "7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression\n\nIn Simple Linear Regression, the coefficient of determination (R²) is a statistical measure that represents the proportion of the variance in the dependent variable that is predictable from the independent variable. It essentially tells you how well the regression line \"fits\" the data.   \n\nHere's a breakdown of its interpretation:\n\nRange: R² values range from 0 to 1. It is often expressed as a percentage (0% to 100%).   \n\nInterpretation based on value:\n\nR² = 0 (or 0%): The model explains none of the variability in the dependent variable. The independent variable offers no predictive power for the dependent variable. The regression line is essentially a horizontal line at the mean of the dependent variable.   \n0 < R² < 1 (or 0% < R² < 100%): The model explains some portion of the variability in the dependent variable. A higher R² indicates that a larger proportion of the variance in the dependent variable is accounted for by the independent variable and the regression model. For example, an R² of 0.70 (or 70%) means that 70% of the variation in the dependent variable can be explained by the independent variable, and the remaining 30% is due to other factors or unexplained variation.   \nR² = 1 (or 100%): The model explains all of the variability in the dependent variable. The independent variable perfectly predicts the dependent variable, and all data points lie exactly on the regression line. This is rare in real-world scenarios.   \n\"Goodness of Fit\": A higher R² generally indicates a better fit of the linear model to the data. It suggests that the independent variable is a good predictor of the dependent variable.\n\nVariance Explained: R² can be interpreted as the percentage of the total variation in the dependent variable that is \"explained\" or \"accounted for\" by the linear relationship with the independent variable.   ",
      "metadata": {}
    },
    {
      "id": "b55cdc8a-8e76-4478-be2b-89b7076608e8",
      "cell_type": "markdown",
      "source": "8. What is Multiple Linear Regression\n\nMultiple Linear Regression is a statistical technique used to model the relationship between a single dependent variable 1  and two or more independent variables. The goal is to find a linear relationship that best predicts the dependent variable based on the values of the independent variables.   \n\n\nUnlike simple linear regression, which examines the relationship between only two variables, multiple linear regression allows us to understand how several factors simultaneously influence an outcome. It helps us predict the value of the dependent variable by considering the combined effects of multiple predictors.\n\nEach independent variable in the model has a coefficient that estimates its impact on the dependent variable, while holding all other independent variables constant. This is a key aspect, as it allows us to isolate the unique contribution of each predictor.\n\nMultiple linear regression is used for various purposes, including predicting future values, explaining the relationships between variables, and identifying which independent variables are the most influential predictors of the dependent variable. However, it's important to consider assumptions like linearity, independence of errors, equal variance of errors, and the absence of strong correlations between the independent variables to ensure the reliability of the model's results. The coefficient of determination (R²) indicates how well the model as a whole fits the data, representing the proportion of the variance in the dependent variable explained by all the independent variables together.",
      "metadata": {}
    },
    {
      "id": "5d2aedcc-09dd-42ad-842a-71ff733d82d4",
      "cell_type": "markdown",
      "source": "9 - What is the main difference between Simple and Multiple Linear Regression\n\nSimple Linear Regression focuses on understanding and predicting the relationship between a single independent variable and a single dependent variable. It aims to find the best-fitting straight line that describes how the dependent variable changes as the independent variable changes. Think of it as examining a direct, linear connection between two things, like how studying time might relate to test scores.\n\nMultiple Linear Regression, however, expands on this by considering the influence of multiple independent variables on a single dependent variable. It allows us to model more complex scenarios where an outcome is likely affected by several different factors acting simultaneously. For instance, predicting house prices might involve considering factors like square footage, number of bedrooms, location, and age of the property.\n\nA crucial aspect of multiple linear regression is that it allows us to assess the unique contribution of each independent variable to the dependent variable, while holding the effects of other independent variables constant. This helps in understanding the individual impact of each predictor. While simple linear regression gives us a straightforward slope indicating the change in the dependent variable for a unit change in the independent variable, multiple linear regression provides multiple coefficients, each associated with a specific independent variable and representing its isolated effect.\n\nIn essence, simple linear regression explores a bivariate linear relationship, while multiple linear regression explores a multivariate linear relationship. The choice between the two depends on the complexity of the relationship being studied and the number of potential factors influencing the dependent variable. Real-world phenomena are often influenced by multiple factors, making multiple linear regression a more versatile and powerful tool in many situations.",
      "metadata": {}
    },
    {
      "id": "151361fb-2032-4df6-a7b3-423dd920928c",
      "cell_type": "markdown",
      "source": "10. What are the key assumptions of Multiple Linear Regression\n\nThe key assumptions of Multiple Linear Regression are:\n\nLinearity: There is a linear relationship between the dependent variable and each of the independent variables. This implies that the effect of each independent variable on the dependent variable is constant across all values of the independent variables.   \n\n Independence of Errors (No Autocorrelation): The residuals (the differences between the observed and predicted values) are independent of each other. This means that the error for one observation should not influence the error for any other observation. This is particularly important in time series data.   \n\n Homoscedasticity: The variance of the residuals is constant across all levels of the independent variables. In simpler terms, the spread of the errors should be roughly the same for all combinations of the predictor variable values.   \n\n Normality of Residuals: The residuals are normally distributed. This assumption is primarily needed for statistical inference (hypothesis testing and confidence intervals). The Central Limit Theorem helps to relax this assumption in large samples.   \n\nNo or Little Multicollinearity: The independent variables should not be highly correlated with each other. High multicollinearity can make it difficult to determine the individual effect of each predictor on the dependent variable and can inflate the standard errors of the coefficients, leading to unreliable significance tests.   \n\nViolations of these assumptions can lead to biased or inefficient parameter estimates and unreliable statistical inferences. Therefore, it is important to assess these assumptions when building and interpreting multiple linear regression models. Techniques for detecting and addressing violations exist, such as examining residual plots, using statistical tests, and transforming variables.   \n\n",
      "metadata": {}
    },
    {
      "id": "a54a183d-6b24-4b44-bc54-9dbf7a9edb4d",
      "cell_type": "markdown",
      "source": "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model\n\nHeteroscedasticity, also spelled heteroskedasticity, refers to a situation in statistical modeling, particularly in regression analysis, where the variance of the residuals (the error terms) is not constant across all levels of the independent variables. In simpler terms, the spread of the errors differs as the values of the predictors change.   \n\nHow Heteroscedasticity Affects the Results of a Multiple Linear Regression Model:\n\nThe presence of heteroscedasticity violates one of the key assumptions of Ordinary Least Squares (OLS) regression, which assumes homoscedasticity (constant variance of errors). While it doesn't cause bias in the coefficient estimates themselves, it significantly affects the reliability of statistical inferences derived from the model:   \n\nInefficient Coefficient Estimates: Although the OLS estimators remain unbiased and consistent in the presence of heteroscedasticity (meaning they will tend towards the true population values with a large enough sample size), they are no longer the Best Linear Unbiased Estimators (BLUE). This means that there are other unbiased estimators with smaller variances, making OLS less efficient.\n\nBiased Standard Errors: The most damaging consequence of heteroscedasticity is that the standard errors of the regression coefficients are biased and inconsistent. OLS assumes a constant error variance when calculating these standard errors. If this assumption is violated, the calculated standard errors will be incorrect. They can be either underestimated or overestimated depending on the specific pattern of heteroscedasticity.   \n\nUnreliable Hypothesis Testing: Because the standard errors are biased, the t-statistics and F-statistics used for hypothesis testing will also be unreliable.\n\nUnderestimated Standard Errors: If the standard errors are underestimated, the calculated t-values will be inflated, leading to a higher chance of rejecting the null hypothesis when it is actually true (increased Type I error rate - concluding a significant effect when there isn't one).   \nOverestimated Standard Errors: If the standard errors are overestimated, the calculated t-values will be deflated, leading to a lower chance of rejecting the null hypothesis when it is false (increased Type II error rate - failing to detect a significant effect when there is one).\n Invalid Confidence Intervals: Confidence intervals for the regression coefficients, which are based on the standard errors, will also be incorrect. If standard errors are underestimated, the confidence intervals will be too narrow, potentially excluding the true population parameter. If standard errors are overestimated, the confidence intervals will be too wide, reducing the precision of the estimates.   \n\nMisleading Conclusions: Ultimately, the biased standard errors and unreliable hypothesis tests can lead to incorrect conclusions about the significance and magnitude of the relationships between the independent and dependent variables. You might incorrectly identify variables as statistically significant or insignificant.\n\nIn summary, heteroscedasticity doesn't change the estimated coefficients themselves in a systematic way, but it undermines our ability to make accurate statistical inferences about those coefficients due to the unreliable standard errors. This makes it difficult to determine which predictors truly have a significant impact on the dependent variable.",
      "metadata": {}
    },
    {
      "id": "2514e467-5717-44eb-b637-5b82eda89d23",
      "cell_type": "markdown",
      "source": "12. How can you improve a Multiple Linear Regression model with high multicollinearity\n\nYou can improve a Multiple Linear Regression model suffering from high multicollinearity using several strategies:\n\nRemove Highly Correlated Predictors:\n\nIdentify pairs or groups of independent variables that have a high correlation (e.g., using a correlation matrix or Variance Inflation Factor - VIF).   \nRemove one or more of these correlated variables from the model. The choice of which variable to remove might be based on domain knowledge or which variable is theoretically less important. Be cautious, as removing important predictors can lead to omitted variable bias.   \nCombine Correlated Variables:\n\nIf the correlated variables represent a similar underlying construct, consider combining them into a single, new variable. This could involve averaging them, summing them, or creating an interaction term if theoretically justified.\nTechniques like Principal Component Analysis (PCA) or factor analysis can be used to reduce the dimensionality of the dataset by creating a smaller set of uncorrelated components that capture most of the variance. These components can then be used as predictors in the regression model. However, the interpretability of the original variables is lost.   \nIncrease Sample Size:\n\nMulticollinearity is often a problem in smaller datasets. Increasing the sample size can sometimes help to reduce the impact of multicollinearity by providing more independent variation in the predictors.\nUse Regularization Techniques:\n\nRidge Regression (L2 Regularization): Adds a penalty term to the least squares objective function that is proportional to the square of the magnitude of the coefficients. This shrinks the coefficients towards zero, reducing their sensitivity to multicollinearity without necessarily removing any variables.   \nLasso Regression (L1 Regularization): Adds a penalty term proportional to the absolute value of the coefficients. Lasso can also shrink coefficients to zero, effectively performing variable selection and potentially removing some of the collinear variables.   \nCentering the Data:\n\nSubtracting the mean from each independent variable (centering) can sometimes reduce multicollinearity that arises from interaction terms or polynomial terms involving the same underlying variable. It doesn't eliminate the correlation between the original variables but can make the model estimation more stable.   \nPartial Least Squares (PLS) Regression:\n\nPLS is a technique that is particularly useful when dealing with high multicollinearity and a large number of predictors. It aims to find components (similar to PCA) that are also correlated with the dependent variable, making it more focused on prediction than PCA.   \nDo Nothing (If Prediction is the Primary Goal):\n\nIf the primary goal of the model is prediction and not inference about the individual coefficients, multicollinearity might not be a major concern as long as the model provides good predictive power. The overall model fit (R²) and predictions might still be reliable. However, the interpretation of individual coefficients will remain problematic.\nThe best approach often involves a combination of these techniques, guided by the specific context of the data and the research question. It's crucial to carefully evaluate the trade-offs of each method and consider the interpretability and validity of the resulting model.",
      "metadata": {}
    },
    {
      "id": "066c9190-9bbb-416e-9785-91b236a9e4c7",
      "cell_type": "markdown",
      "source": "13. What are some common techniques for transforming categorical variables for use in regression models\n\nDummy Coding (One-Hot Encoding):\n\nCreates a new binary (0 or 1) variable for each category of the original categorical variable.   \nIf a data point belongs to a specific category, the corresponding dummy variable gets a '1', and all other dummy variables for that original variable get a '0'.\nFor a categorical variable with k levels, you typically create k−1 dummy variables to avoid multicollinearity (the \"dummy variable trap\"). One category is chosen as the reference category, and its effect is captured in the intercept.   \nEffect Coding (Deviation Coding):\n\nSimilar to dummy coding, but instead of using 0 and 1, it uses 1, 0, and -1.   \nFor each category (except the reference), a new variable is created: '1' if the data point belongs to that category, '0' if it belongs to any other non-reference category, and '-1' if it belongs to the reference category.\nThe coefficients in effect coding represent the difference of each group's mean from the overall mean of the dependent variable.\nLabel Encoding (Ordinal Encoding):\n\nAssigns a unique numerical label (integer) to each category.   \nThis method is suitable for ordinal categorical variables where there is an inherent order or ranking among the categories (e.g., \"Low,\" \"Medium,\" \"High\").   \nHowever, it should be used with caution for nominal (unordered) categorical variables as it can introduce an artificial order that the model might incorrectly interpret.\nBinary Encoding:\n\nEach category is first assigned a numerical label (like in label encoding).   \nThen, these integers are converted into their binary representations.\nEach binary digit becomes a new column.\nThis method can be more efficient than one-hot encoding when dealing with categorical variables with a large number of levels, as it typically creates fewer new variables.\nCount or Frequency Encoding:\n\nReplaces each category with the count or frequency of that category in the dataset.\nThis can be useful when the prevalence of a category might be informative. However, it can lead to the same numerical value for different categories, potentially losing some information.\nTarget Encoding (Mean Encoding):\n\nReplaces each category with the mean of the target variable for that category.\nThis can capture the relationship between the categorical variable and the dependent variable.   \nHowever, it is prone to overfitting, especially with categories that have few observations. Techniques like cross-validation and adding noise can help mitigate this.\nThe choice of which technique to use depends on several factors, including:\n\nThe nature of the categorical variable: Is it nominal or ordinal?\nThe number of categories: High cardinality variables can lead to a large number of new variables with one-hot encoding.\nThe specific regression model being used: Some models might handle certain encodings better than others.\nThe potential for introducing multicollinearity: One-hot encoding can lead to this if not handled carefully (by dropping one category).\nThe interpretability of the results: Different encodings can lead to different interpretations of the coefficients.   \n\n",
      "metadata": {}
    },
    {
      "id": "74692b41-efd3-47b3-bb07-5d64358bc123",
      "cell_type": "markdown",
      "source": "14. What is the role of interaction terms in Multiple Linear Regression\n\nInteraction terms in Multiple Linear Regression enhance the model's ability to represent reality by accounting for situations where the relationship between an independent variable and the dependent variable is not constant across all levels of another independent variable. 1  They move beyond simple additive effects, allowing for conditional effects.   \n\nWithout interaction terms, the model assumes that the impact of each predictor on the outcome is independent of the other predictors. Interaction terms address scenarios where the effect of one predictor depends on the value of another predictor.\n\nConsider the example of advertising and seasonality on sales. A model without interaction would assume that the impact of a $100 increase in advertising is the same whether it's peak season or off-season. However, an interaction term between advertising and seasonality allows the model to recognize that the same $100 advertising spend might yield a larger increase in sales during peak season compared to the off-season.\n\nBy including interaction terms, we can model these combined or synergistic effects, where the joint impact of two variables is greater than the sum of their individual impacts, or dampening or antagonistic effects, where the joint impact is less than the sum of their individual impacts.\n\nIn essence, interaction terms provide a more nuanced and accurate representation of complex relationships, allowing the model to capture how the influence of one variable is moderated by another. This leads to more realistic models and potentially improved predictive accuracy in situations where variables do not act independently.",
      "metadata": {}
    },
    {
      "id": "00945944-8431-4e67-b866-58938bb40e0e",
      "cell_type": "markdown",
      "source": "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression\n\nIn Simple Linear Regression, the intercept is the predicted value of the dependent variable when the single independent variable is zero. If zero is a plausible and relevant value for the independent variable within the context of your study, the intercept can provide a meaningful baseline or starting point for the dependent variable.\n\nHowever, in Multiple Linear Regression, the intercept represents the predicted value of the dependent variable when all independent variables are simultaneously equal to zero. The key difference lies in the increased number of conditions that must be met for this interpretation to be valid. In many real-world scenarios with multiple predictors, the situation where all predictors are exactly zero is either practically impossible, nonsensical, or falls far outside the observed range of the data.\n\nTherefore, while the mathematical definition remains the same, the real-world interpretability and relevance of the intercept often decrease as the number of independent variables in the regression model increases. In multiple regression, the intercept frequently serves more as a necessary constant to correctly position the regression plane (or hyperplane in higher dimensions) in the data space, rather than representing a directly interpretable observation.\n\nConsider highlighting that the meaningfulness of the intercept is tied to the practical significance and likelihood of all independent variables being zero within the specific context of the data being analyzed.",
      "metadata": {}
    },
    {
      "id": "16640d1b-0d52-4a2b-88aa-3fb1f4b302b6",
      "cell_type": "markdown",
      "source": "16. What is the significance of the slope in regression analysis, and how does it affect predictions\n\nThe slope in regression analysis holds significant importance as it quantifies the rate of change of the dependent variable for a one-unit increase in the independent variable. It essentially describes the direction and strength of the linear relationship between the variables.   \n\nHere's a breakdown of its significance and impact on predictions:\n\nSignificance of the Slope:\n\nDirection of the Relationship:\n\nA positive slope indicates a direct relationship: as the independent variable increases, the dependent variable tends to increase.\nA negative slope indicates an inverse relationship: as the independent variable increases, the dependent variable tends to decrease.   \nA slope of zero suggests no linear relationship between the variables; changes in the independent variable do not predict changes in the dependent variable.   \nMagnitude of the Effect: The absolute value of the slope indicates the steepness of the regression line and the magnitude of the change in the dependent variable for a one-unit change in the independent variable. A larger absolute slope signifies a more substantial change in the dependent variable for a unit change in the independent variable.   \n\nUnits of Change: The slope's units are the units of the dependent variable per unit of the independent variable, providing a real-world interpretation of the rate of change. For example, if the independent variable is study hours and the dependent variable is test scores, a slope of 5 means that for every additional hour of study, the test score is predicted to increase by 5 points, on average.   \n\nHow the Slope Affects Predictions:\n\nThe slope is a crucial component of the regression equation in multiple linear regression and directly influences the predicted values of the dependent variable:\n\nDetermining the Change: When you input a value for the independent variable(s) into the regression equation, the slope determines how much the predicted value of the dependent variable will change relative to the intercept. For every one-unit increase in the independent variable (or each independent variable in multiple regression), the predicted value of the dependent variable will increase or decrease by the amount of the slope coefficient associated with that independent variable (holding other variables constant in multiple regression).   \n\nMaking Estimates: The slope allows us to estimate the value of the dependent variable for values of the independent variable(s) that were not directly observed in the original data, within the range of the model's applicability.   \n\nInfluencing the Line of Best Fit: The calculated slope, along with the intercept, defines the specific position and orientation of the line (or hyperplane in multiple regression) that best fits the observed data according to the least squares criterion. A different slope would result in a different line and thus different predictions.\n\nIn summary, the slope in regression analysis is vital for understanding the nature and strength of the linear relationship between variables and is the key factor in determining how changes in the independent variable(s) translate into changes in the predicted value of the dependent variable. It is essential for making meaningful predictions using the regression model.",
      "metadata": {}
    },
    {
      "id": "106f15aa-e173-4789-93c2-97afde6c609d",
      "cell_type": "markdown",
      "source": "17. How does the intercept in a regression model provide context for the relationship between variables\n\nThe intercept in a regression model provides a crucial baseline or starting point for understanding the relationship between the variables. It essentially anchors the regression line (or hyperplane) and gives a predicted value of the dependent variable when all independent variables are at a specific value (typically zero). This baseline can offer valuable context in several ways:   \n\n1. Establishing a Reference Point:\n\nThe intercept serves as a reference value from which the effects of the independent variables are measured. It tells you what the model predicts when the influences of the predictors are absent (i.e., when their values are zero). This baseline can be meaningful in understanding the inherent level of the dependent variable before any predictor comes into play.   \n2. Providing a Starting Value for Predictions:\n\nWhen using the regression equation for prediction, the intercept is the initial value that is then adjusted based on the values and coefficients of the independent variables. It sets the \"ground level\" for the predicted outcome.\n3. Offering Practical Interpretation (When Zero is Meaningful):\n\nIn cases where a value of zero for the independent variable(s) is realistic and within the scope of the data, the intercept can have a direct, real-world interpretation. For example:\nIf predicting sales based on advertising spend, an intercept of $10,000 might suggest a baseline sales level of $10,000 even with no advertising.\nIf predicting plant height based on fertilizer amount, an intercept of 5 cm might represent the initial height of the plant before any fertilizer is applied.\n4. Highlighting the Importance of Predictors:\n\nBy establishing what the dependent variable is predicted to be when the predictors are absent, the intercept implicitly highlights the change in the dependent variable that is attributed to the independent variables. The difference between the intercept and the predicted value when predictors are present underscores the impact of those predictors.\n5. Contextualizing the Range of the Relationship:\n\nEven when zero is not directly meaningful, the intercept helps define the overall level at which the linear relationship is operating within the observed range of the data. It positions the regression line within the data space, providing context for how the independent variables influence the dependent variable across that range.",
      "metadata": {}
    },
    {
      "id": "e5adcde7-7f0a-4221-95fa-780ba6a428de",
      "cell_type": "markdown",
      "source": "18. What are the limitations of using R² as a sole measure of model performance\n\nWhile R² (the coefficient of determination) is a widely used and intuitive measure of how well a regression model fits the observed data, relying solely on it to assess model performance has several significant limitations:   \n\nR² Doesn't Indicate if the Model is Correctly Specified: A high R² doesn't guarantee that the chosen linear model is the most appropriate for the relationship between the variables. There might be a non-linear relationship that a linear model with a high R² is poorly approximating. Visual inspection of residual plots is crucial to assess linearity assumptions.   \n\nR² Increases with the Number of Predictors (Even Spurious Ones): In multiple linear regression, R² will always increase (or stay the same) as you add more independent variables to the model, even if those variables are not truly related to the dependent variable. This can lead to overfitting, where the model fits the noise in the sample data rather than the underlying relationship, resulting in poor performance on new, unseen data. Adjusted R² attempts to address this by penalizing the addition of unnecessary predictors.   \n\n R² Doesn't Indicate Causation: A high R² only signifies a strong linear association between the variables. It does not imply a causal relationship. Correlation does not equal causation, and there might be confounding variables or a reverse causal relationship that the R² value doesn't reveal.   \n\nR² Doesn't Assess Prediction Accuracy on New Data: R² is calculated on the training data. A high R² on the training set doesn't guarantee good predictive performance on new, unseen data. Overfit models can have a high R² on training data but perform poorly on test data. Measures like Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and Mean Absolute Error (MAE) on a separate test set provide a more direct assessment of prediction accuracy.   \n\n R² is Sensitive to Outliers: Outliers can disproportionately influence the regression line and, consequently, the R² value. A single influential outlier can either inflate or deflate R², potentially giving a misleading impression of the model's fit to the majority of the data.   \n\nR² Doesn't Tell You About the Significance of Coefficients: A high R² indicates that the model as a whole explains a large proportion of the variance, but it doesn't tell you whether the individual independent variables are statistically significant predictors. Hypothesis testing on the individual coefficients (using t-tests and p-values) is necessary for this assessment.\n\n R² Can Be Misleading in Certain Contexts: In some fields or with certain types of data (e.g., time series data with trends), a high R² might be easily achieved even with a poorly specified model or spurious relationships.",
      "metadata": {}
    },
    {
      "id": "92ee0c89-8964-405b-beb9-2dcb05057c36",
      "cell_type": "markdown",
      "source": "19. How would you interpret a large standard error for a regression\n\nA large standard error for a regression coefficient signifies substantial uncertainty and imprecision in our estimate of that coefficient's true value in the population. It implies that the coefficient derived from our sample data might not be a reliable reflection of the actual relationship. This imprecision manifests as a wider confidence interval, suggesting a broad range within which the true coefficient likely lies, potentially even encompassing zero. Consequently, it becomes challenging to assert a statistically significant effect of the independent variable on the dependent variable, as the test statistic will be smaller and the p-value larger.\n\nSeveral factors can contribute to a large standard error, including a small sample size, high inherent variability in the data, the presence of multicollinearity among predictors (in multiple regression), omitted relevant variables, misspecification of the model's functional form, and errors in the measurement of variables.\n\nThe implications of a large standard error are significant for the interpretation of the regression results. It undermines our confidence in the estimated magnitude of the effect and our ability to establish the statistical significance of the predictor. Drawing definitive conclusions about the relationship becomes problematic. A large standard error often necessitates further investigation, potentially requiring a larger dataset, addressing issues like multicollinearity, re-evaluating the model specification, or improving the quality of the data to obtain more stable and reliable estimates of the regression coefficients. Ultimately, a large standard error highlights a considerable degree of uncertainty in the estimated relationship, urging caution in drawing strong inferences.    ",
      "metadata": {}
    },
    {
      "id": "10d5a683-920b-491c-bb03-a0eeea475382",
      "cell_type": "markdown",
      "source": "20. - How can heteroscedasticity be identified in residual plots, and why is it important to address it\n   \nHeteroscedasticity in residual plots can be identified by observing a non-constant variance of the residuals as the fitted values of the dependent variable increase. Instead of a random scattering of points with roughly equal spread around the horizontal zero line, you might see patterns such as:\n\nFunnel Shape: The residuals might start with a narrow spread at lower fitted values and then fan out as the fitted values increase (or vice versa). This is a classic indicator of heteroscedasticity.\nCone Shape: Similar to a funnel, the spread of residuals might converge or diverge as the fitted values change, resembling a cone.\nOther Non-Random Patterns: Any systematic change in the dispersion of the residuals as you move along the x-axis (fitted values) suggests heteroscedasticity.\nWhy is it important to address heteroscedasticity?\n\nAddressing heteroscedasticity is crucial because it violates a key assumption of Ordinary Least Squares (OLS) regression, which assumes that the variance of the error terms is constant (homoscedasticity). While heteroscedasticity doesn't bias the coefficient estimates themselves, it has significant negative impacts on the reliability of statistical inferences:\n\nInefficient Coefficient Estimates: OLS estimators are no longer the Best Linear Unbiased Estimators (BLUE). Other estimators could provide more precise estimates.\nBiased Standard Errors: The standard errors of the regression coefficients are estimated incorrectly. They can be either underestimated or overestimated.\nUnreliable Hypothesis Testing: Biased standard errors lead to unreliable t-statistics and F-statistics. Underestimated standard errors can inflate t-values, leading to a higher chance of incorrectly rejecting the null hypothesis (Type I error). Overestimated standard errors can deflate t-values, increasing the chance of failing to reject a false null hypothesis (Type II error).\nInvalid Confidence Intervals: Confidence intervals for the coefficients, which are based on standard errors, will be inaccurate, either too narrow or too wide, leading to incorrect conclusions about the range of plausible values for the true coefficients.\nMisleading Conclusions: Ultimately, the unreliable statistical inferences can lead to incorrect conclusions about the significance and impact of the independent variables on the dependent variable.\nTherefore, identifying and addressing heteroscedasticity is essential to ensure the validity and reliability of your regression analysis, particularly when making inferences about the relationships between variables and their statistical significance. Techniques to address it include data transformations, using weighted least squares, or employing heteroscedasticity-consistent standard errors.",
      "metadata": {}
    },
    {
      "id": "d7d45b9b-8a78-41d5-8cb2-a6021a2a0a1d",
      "cell_type": "markdown",
      "source": "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²\n\nA high R² but a low adjusted R² in a Multiple Linear Regression model suggests that the model explains a large proportion of the variance in the dependent variable within the sample data, but this explanatory power doesn't generalize well to new, unseen data. Here's a breakdown of what this discrepancy implies:\n\nOverfitting: The most likely reason for this scenario is overfitting. The model has included too many independent variables, some of which might be spurious or not truly related to the dependent variable in the broader population. While these extra variables might coincidentally correlate with the dependent variable in the training data, leading to a high R², they don't add real predictive power.\n\nPenalty for Complexity: Adjusted R² differs from R² by penalizing the inclusion of additional independent variables in the model. It increases only if the new variable improves the model's fit more than would be expected by chance. If a variable doesn't add significant explanatory power, the adjusted R² will either increase by a smaller amount than R² or even decrease.\n\nPoor Generalizability: A large difference between R² and adjusted R² indicates that the model is likely too tailored to the specific nuances and noise of the training dataset. When applied to new data, the model's performance (how well it predicts the dependent variable) is expected to be worse than suggested by the high R² value.\n\nInflated Variance Explained: The high R² gives an overly optimistic view of the model's explanatory power in the population. The adjusted R² provides a more realistic estimate of how much variance would be explained if the model were applied to a new dataset.\n\nIn essence, a high R² with a low adjusted R² is a warning sign that the model might be overly complex and may not be a reliable predictor for data it hasn't seen before. It suggests that adding more variables has improved the fit to the training data, but not in a meaningful way that generalizes to the broader population. In such cases, it's often advisable to reconsider the model by potentially removing less important variables or using techniques like regularization to improve its generalizability.",
      "metadata": {}
    },
    {
      "id": "eea1f6a5-8a1b-42be-b12e-8bf7724e8b4f",
      "cell_type": "markdown",
      "source": "22. - Why is it important to scale variables in Multiple Linear Regression\n\nIt is important to scale variables in Multiple Linear Regression for several key reasons:\n\n Preventing Dominance by Variables with Larger Scales: If independent variables have vastly different ranges, the variable with the larger scale might disproportionately influence the model's learning and the size of the coefficients. Scaling ensures that all variables contribute more equally to the model. For instance, income in thousands and age in years have different scales, and without scaling, income might dominate the model.   \n\nImproving Algorithm Convergence: For optimization algorithms like gradient descent, which are used to find the best regression coefficients, features on a similar scale help the algorithm converge faster and more efficiently towards the global minimum of the cost function. Unscaled features can lead to an elongated error surface, causing the algorithm to take longer and potentially oscillate.\n\n Facilitating Regularization: In regularized regression techniques like Ridge and Lasso, penalties are applied to the magnitude of the coefficients. If variables are not scaled, those with larger scales might be unfairly penalized or have their influence disproportionately reduced compared to variables with smaller scales, regardless of their actual importance.   \n\nEnhancing Interpretability of Coefficients (in some cases): When independent variables are standardized (e.g., to have a mean of 0 and a standard deviation of 1), their coefficients become directly comparable, indicating the change in the dependent variable associated with a one-standard-deviation change in the predictor. This can help in assessing the relative importance of different predictors.   \n\n Reducing Multicollinearity Induced by Interaction or Polynomial Terms: When creating interaction terms (products of variables) or polynomial terms (e.g., X \n2\n ), scaling the original variables (especially centering them around zero) can help reduce the artificial multicollinearity that might be introduced due to the different scales and ranges.   \n\n Distance-Based Algorithms: If the regression analysis involves or is compared with distance-based algorithms (like K-Nearest Neighbors), scaling is crucial to ensure that distances are not dominated by variables with larger magnitudes.   \n\nHowever, it's important to note that scaling is not always strictly necessary for the mathematical correctness of the linear regression model itself, especially when solved analytically using Ordinary Least Squares (OLS). The coefficients will adjust to the scale of the variables. Nevertheless, scaling often leads to more stable and interpretable models, particularly when employing gradient-based optimization or regularization. The choice of scaling method (e.g., standardization or normalization) can depend on the specific data and the goals of the analysis.",
      "metadata": {}
    },
    {
      "id": "b24a209c-43d3-4ff3-8fe4-41eb5be751fa",
      "cell_type": "markdown",
      "source": "23. What is polynomial regression\n\nPolynomial regression is a type of regression analysis that models the relationship between an independent variable and a dependent variable by fitting a curved line to the data. Unlike simple linear regression, which uses a straight line, polynomial regression can capture non-linear relationships.\n\nThink of it like trying to fit a flexible curve through your data points instead of a rigid straight ruler. The \"degree\" of the polynomial determines how many bends or curves the fitted line can have. A degree of 1 is just a straight line (simple linear regression). A degree of 2 allows for one bend (like a U-shape), a degree of 3 allows for two bends (like an S-shape), and so on.\n\nPolynomial regression is useful when a straight line doesn't accurately represent the pattern in your data. It can provide a better fit for relationships that are curved or change direction. However, it's important to be careful not to use a polynomial with too high a degree, as this can lead to the model fitting the random noise in your data instead of the true underlying relationship. This is called overfitting, and it means the model might perform poorly when you try to use it to predict new data.\n\nChoosing the right degree for the polynomial is a key part of using this technique. You want a curve that fits the general trend of your data without being too wiggly or following every single data point too closely.",
      "metadata": {}
    },
    {
      "id": "3b78d7bc-6a3f-43c2-abca-4cf8a9d5cdef",
      "cell_type": "markdown",
      "source": "24.  How does polynomial regression differ from linear regression\n\nLinear regression assumes a straightforward, constant relationship between variables, like drawing the best possible straight line through a scatter of points. It works well when the change in one variable is consistently proportional to the change in the other. However, many real-world relationships are more complex, exhibiting curves or bends. This is where polynomial regression comes in.   \n\nPolynomial regression is like using a flexible curve instead of a straight line to fit the data. It can model relationships where the rate of change isn't constant. Imagine trying to fit a line to data that looks like a U-shape or an S-shape; a straight line wouldn't capture the trend accurately. Polynomial regression achieves this by adding terms with the independent variable raised to different powers (like squared, cubed, etc.) to the regression equation. The \"degree\" of the polynomial determines how many curves or bends the fitted line can have. A higher degree allows for more complex shapes but also increases the risk of overfitting, where the model fits the random noise in the data rather than the true underlying pattern.   \n\nWhile polynomial regression can provide a much better fit for non-linear data compared to linear regression, it's crucial to choose the degree of the polynomial carefully. The goal is to find a curve that captures the essential trend without becoming too wiggly or overly sensitive to individual data points. Unlike the simple, direct interpretation of the slope in linear regression, the coefficients in higher-degree polynomial regression become more complex to interpret in isolation, as the effect of the independent variable on the dependent variable changes depending on the value of the independent variable itself.",
      "metadata": {}
    },
    {
      "id": "db3b6d58-2467-4f1f-9ff7-e29a515b3f46",
      "cell_type": "markdown",
      "source": "25. When is polynomial regression used\n\nPolynomial regression is used when the relationship between an independent variable and a dependent variable is non-linear, meaning a straight line cannot adequately describe the pattern in the data. It's employed in various fields where such curved relationships naturally occur.   \n\nHere are some specific scenarios and fields where polynomial regression is commonly used:\n\nModeling Physical Phenomena: Many physical processes exhibit non-linear behavior. For example, the trajectory of a projectile under gravity, the relationship between temperature and the rate of a chemical reaction, or the power output of a solar panel relative to the angle of sunlight often follow curves.   \nGrowth and Decay Processes: Biological growth (e.g., tissue growth, bacterial population growth) and decay processes (e.g., radioactive decay) frequently follow non-linear patterns that can be modeled using polynomial regression.   \nEngineering and Manufacturing: Analyzing system performance curves, such as the relationship between engine speed and fuel consumption, or the stress-strain curve of a material, often requires fitting polynomial models.\nFinance and Economics: Modeling stock market trends (though caution is needed due to volatility), the relationship between interest rates and economic growth, or price elasticity of demand can sometimes benefit from polynomial regression to capture non-linear effects.   \nHealthcare and Epidemiology: Predicting the progression of diseases, modeling drug responses at different dosages, or analyzing mortality rates based on age can involve non-linear relationships suitable for polynomial regression.   \nAgriculture and Environmental Science: Predicting crop yield based on factors like temperature and rainfall, where the relationship might have an optimal point, or modeling the distribution of pollutants from a source can utilize polynomial regression.   \nMachine Learning and Data Science: As a flexible modeling technique, polynomial regression is used to capture complex patterns in datasets that linear models would underfit. It's a foundational technique and can be a building block for more advanced non-linear models.   \nIn essence, polynomial regression is chosen when a visual inspection of the data or prior knowledge suggests a curved relationship between the variables, and a linear model would likely lead to a poor fit and inaccurate predictions. It provides the flexibility to model these non-linearities, offering a better understanding of the underlying relationship and potentially more accurate predictions within the observed data range.",
      "metadata": {}
    },
    {
      "id": "01e1a6df-8d64-444f-91f1-7b57deac65df",
      "cell_type": "markdown",
      "source": "26. What is the general equation for polynomial regression\n\na model where the dependent variable is predicted by a combination of the independent variable raised to different integer powers. This creates a relationship that is no longer restricted to a straight line, as in linear regression, but can instead follow a curve.\n\nThe model includes a constant term (the intercept) and a series of terms where the independent variable is raised to increasing powers (e.g., x, x², x³, and so on), up to a chosen degree. Each of these power terms has a corresponding coefficient that the regression analysis estimates from the data. These coefficients determine the shape and position of the fitted curve.\n\nThe \"degree\" of the polynomial is the highest power of the independent variable in the model. A first-degree polynomial is equivalent to simple linear regression. As the degree increases, the model becomes more flexible and can fit more complex curves in the data. For example, a second-degree polynomial can model a parabolic relationship, while a third-degree polynomial can model relationships with inflections.\n\nIt's important to note that while polynomial regression models non-linear relationships between the variables, it is still considered a linear model in terms of the coefficients. The regression techniques used aim to find the best values for these coefficients to minimize the difference between the predicted and observed values of the dependent variable. The model also includes an error term to account for the variability that the polynomial function does not explain. The choice of the degree of the polynomial is a crucial decision that balances the model's ability to fit the data well against the risk of overfitting.",
      "metadata": {}
    },
    {
      "id": "c7849249-eda0-4901-a525-77c2f2939f31",
      "cell_type": "markdown",
      "source": "27.  Can polynomial regression be applied to multiple variables\n\npolynomial regression can be extended to situations with multiple independent variables. Instead of just one predictor, you have several, and the dependent variable is modeled as a polynomial combination of these predictors.\n\nImagine trying to predict something based on not just one factor, but two or more, and the relationship isn't a simple flat plane. Multivariate polynomial regression allows you to fit a complex, curved surface through your data points in this multi-dimensional space.\n\nThe model can include terms where each independent variable is raised to different powers, just like in the single-variable case. However, it also introduces terms that represent the interactions between these different independent variables. For example, if you have two predictors, you might have terms for each predictor squared, as well as a term that's the product of the two predictors.\n\nWhile this approach offers a lot of flexibility to model very intricate relationships, it also comes with challenges. The more predictors you have and the higher the powers you consider, the more complex the model becomes. This increases the risk of overfitting your data, meaning the model learns the noise in your specific dataset rather than the true underlying patterns, and it might not predict new data well. Interpreting the results also becomes more difficult because you have to consider how the different predictors interact with each other in their influence on the dependent variable. Fitting these complex models can also require more computational power.",
      "metadata": {}
    },
    {
      "id": "c818ef93-e8a3-4ab1-b3d3-873eb7e6d387",
      "cell_type": "markdown",
      "source": "28.  What are the limitations of polynomial regression\n\nHere are the key limitations of polynomial regression:\n\nOverfitting: This is a significant risk, especially with high-degree polynomials. The model might fit the training data very closely, including the noise, leading to poor generalization on new, unseen data. The fitted curve can become overly complex and oscillate wildly between data points.\nPoor Extrapolation: Polynomial models often exhibit poor predictive performance outside the range of the observed data. The curve can take unexpected and unrealistic turns beyond the data limits.\nSensitivity to Outliers: Outliers can disproportionately influence the shape of the fitted polynomial, especially those of higher degrees.\nInterpretation of Coefficients: As the degree of the polynomial increases, the interpretation of individual coefficients becomes less intuitive and more complex. The effect of the independent variable on the dependent variable is no longer constant but depends on the value of the independent variable itself.\nMulticollinearity: Higher-order terms of the independent variable  are inherently correlated, which can lead to multicollinearity issues. This can inflate the standard errors of the coefficients, making them unstable and difficult to interpret reliably. Centering the independent variable before creating polynomial terms can help mitigate this.\nGlobal Nature of the Fit: Polynomial regression fits a global curve to the entire dataset. A change in the data at one point can affect the entire fitted curve, which might not be desirable if the underlying relationship varies locally.\nChoosing the Degree: Selecting the appropriate degree for the polynomial is not always straightforward. A low degree might underfit the data, while a high degree might overfit it. Techniques like cross-validation are often needed to find the optimal degree.",
      "metadata": {}
    },
    {
      "id": "99cafebc-6527-4825-8ecf-46208e3b736c",
      "cell_type": "markdown",
      "source": "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial\n\nWe can use several methods to evaluate model fit when selecting the degree of a polynomial in polynomial regression:\n\nAdjusted R-squared: Unlike the regular R-squared, the adjusted R-squared penalizes the addition of unnecessary polynomial terms that do not significantly improve the model's fit. A higher adjusted R-squared generally indicates a better balance between model complexity and goodness of fit. Choose the degree that yields the highest adjusted R-squared.   \n\n Cross-Validation: This technique provides a more robust estimate of the model's performance on unseen data. You can split your data into multiple folds, train the model on a subset of the folds for different polynomial degrees, and evaluate its performance on the remaining fold. The degree that results in the lowest prediction error (e.g., Mean Squared Error) across the folds is often preferred. Common cross-validation techniques include k-fold cross-validation.   \n\nResidual Plots: Examining the residuals (the differences between the observed and predicted values) is crucial. For a good model fit, the residuals should be randomly scattered around zero with no discernible pattern. If you see patterns (like a curve or a funnel shape), it might indicate that the chosen polynomial degree is not adequately capturing the underlying relationship or that heteroscedasticity is present.   \n\nInformation Criteria (AIC, BIC): Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) are statistical measures that assess the trade-off between the model's goodness of fit and its complexity (number of parameters). Lower values of AIC and BIC generally indicate a better model. BIC penalizes model complexity more heavily than AIC, often favoring simpler models.   \n\nF-test (ANOVA for Model Comparison): You can compare the fit of polynomial models of different degrees using an F-test. This test determines if adding higher-order terms significantly improves the model's fit. If the addition of a higher-order term does not lead to a statistically significant improvement, the lower-degree polynomial might be sufficient.\n\nPrediction Error on Test Data: If you have a separate test dataset, you can train polynomial models of different degrees on the training data and evaluate their predictive performance (e.g., using MSE, RMSE, or MAE) on the test data. The degree that yields the lowest error on the test set is a good indicator of the model's generalizability.\n\nBy considering a combination of these methods, you can make a more informed decision about the appropriate degree for your polynomial regression model, balancing the need for a good fit with the risk of overfitting and ensuring good generalization to new data.\n\n",
      "metadata": {}
    },
    {
      "id": "eb97c88d-79bd-42ca-9c4b-5a1ce0d588ba",
      "cell_type": "markdown",
      "source": "30.  Why is visualization important in polynomial regression \n\nVisualization is exceptionally important in polynomial regression for several key reasons, especially given its ability to model non-linear relationships:\n\nUnderstanding the Shape of the Relationship: Unlike linear regression where the relationship is always a straight line, polynomial regression can produce curves of various shapes depending on the degree of the polynomial and the estimated coefficients. Visualizing the fitted polynomial curve overlaid on the scatter plot of the data is crucial for understanding the nature of the modeled relationship. It allows you to see if the curve captures the underlying trend effectively – whether it's U-shaped, S-shaped, or more complex.\n\nIdentifying Non-Linearity: Before even fitting a polynomial model, visualizing the scatter plot of the data can reveal if a linear relationship is inadequate. The presence of a clear curve or bend in the data points suggests that polynomial regression might be a more appropriate approach.   \n\nAssessing Model Fit: Visualizing the fitted curve alongside the data points provides an intuitive way to assess how well the model is capturing the data's pattern. You can see if the curve follows the general trend or if it's missing important features or overemphasizing noise.\n\nDetecting Overfitting: A plot of a high-degree polynomial that closely follows every single data point, including potential outliers, can be a strong visual indicator of overfitting. The curve might exhibit wild oscillations that are unlikely to represent a true underlying relationship. Comparing plots of different degree polynomials can help identify a point where the model starts to overfit.\n\n Examining Residuals: Visualizing the residuals (the differences between the observed and predicted values) against the fitted values or the independent variable is critical for diagnosing problems with the model. In a well-fitted model, the residuals should be randomly scattered around zero with no discernible pattern. Patterns in the residual plot (like a curve, funnel shape, or other systematic deviations) can indicate that the chosen polynomial degree is not appropriate or that other assumptions of the regression are violated (e.g., heteroscedasticity).   \n\nComparing Models of Different Degrees: Plotting the fitted curves for polynomial models of different degrees on the same scatter plot allows for a direct visual comparison of how the complexity of the model affects the fit. This can aid in selecting a degree that balances goodness of fit with model simplicity.\n\nCommunicating Results: Visualizations are powerful tools for communicating the results of a polynomial regression analysis to a wider audience. A clear plot of the fitted curve and the data points can convey the nature of the relationship much more effectively than just presenting coefficients and statistical metrics.\n\nIn summary, visualization in polynomial regression provides essential insights into the shape of the modeled relationship, the quality of the fit, potential issues like overfitting, and helps in comparing different model specifications. It complements statistical metrics and is crucial for building, evaluating, and interpreting polynomial regression models effectively.",
      "metadata": {}
    },
    {
      "id": "9358e238-9328-4969-80ad-3279200089bd",
      "cell_type": "markdown",
      "source": "31.  How is polynomial regression implemented in Python\nPolynomial regression can be implemented in Python using libraries like NumPy and scikit-learn (sklearn). Here's a breakdown of the common methods:\n\nMethod 1: Using NumPy's polyfit function (for fitting the polynomial directly)\n\nNumPy's polyfit function can directly find the coefficients of a polynomial that best fits your data in a least-squares sense.",
      "metadata": {}
    },
    {
      "id": "bdaa39b6-6f10-4247-8a68-22beb4f1e7f6",
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Sample data\nx = np.array([1, 2, 3, 4, 5])\ny = np.array([2, 5, 8, 9, 6])\n\n# Choose the degree of the polynomial\ndegree = 2\n\n# Fit the polynomial\ncoefficients = np.polyfit(x, y, degree)\n\n# Generate points for the polynomial curve\nx_poly = np.linspace(min(x), max(x), 100)\ny_poly = np.polyval(coefficients, x_poly)\n\n# Plot the original data and the fitted polynomial\nplt.scatter(x, y, label='Original Data')\nplt.plot(x_poly, y_poly, color='red', label=f'Polynomial of Degree {degree}')\nplt.xlabel('Independent Variable (x)')\nplt.ylabel('Dependent Variable (y)')\nplt.title('Polynomial Regression using NumPy')\nplt.legend()\nplt.show()\n\nprint(\"Coefficients:\", coefficients)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "Matplotlib is building the font cache; this may take a moment.\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 640x480 with 1 Axes>",
            "image/png": "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"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Coefficients: [-1.   7.2 -4.6]\n"
        }
      ],
      "execution_count": 1
    },
    {
      "id": "e6ed3c4c-df66-4bfc-b8c6-2cee4d9f1070",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}