{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Logistic Regression, and how does it differ from Linear Regression."
      ],
      "metadata": {
        "id": "lW_XUyKGKcyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression is a statistical method primarily used for binary classification tasks, aiming to model the probability of a binary outcome (e.g., yes/no, 0/1) based on one or more independent variables. Unlike Linear Regression, which predicts a continuous outcome, Logistic Regression employs a sigmoid function to transform the linear combination of input features into a probability value between 0 and 1. This probability is then used to classify the observation into one of the two categories based on a chosen threshold.\n",
        "\n",
        "The key difference lies in the type of outcome variable they predict. Linear Regression is suited for predicting continuous numerical values, such as house prices or sales figures, by finding the best-fitting straight line through the data. In contrast, Logistic Regression tackles classification problems where the outcome is categorical. While both techniques model the relationship between independent and dependent variables using coefficients, Logistic Regression uses a non-linear sigmoid function to constrain the output to a probability, making it interpretable for binary decisions. Furthermore, while Linear Regression typically uses ordinary least squares to estimate parameters, Logistic Regression employs maximum likelihood estimation."
      ],
      "metadata": {
        "id": "cWJyR3uvE15z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. What is the mathematical equation of Logistic Regression."
      ],
      "metadata": {
        "id": "B3Cuh9IjFNqx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mathematical equation of Logistic Regression elegantly combines a linear function with a sigmoid function to model the probability of a binary outcome. Initially, a linear predictor z is formed by taking a weighted sum of the input features (x\n",
        "1\n",
        "​\n",
        " ,x\n",
        "2\n",
        "​\n",
        " ,…,x\n",
        "n\n",
        "​\n",
        " ) and their corresponding coefficients (β\n",
        "1\n",
        "​\n",
        " ,β\n",
        "2\n",
        "​\n",
        " ,…,β\n",
        "n\n",
        "​\n",
        " ), along with an intercept term (β\n",
        "0\n",
        "​\n",
        " ). This linear combination, much like in linear regression, captures the additive effect of the independent variables on the underlying log-odds of the event.\n",
        "\n",
        "However, to constrain the output to a probability value between 0 and 1, this linear predictor z is then passed through the sigmoid function, denoted as σ(z). The sigmoid function, mathematically expressed as  \n",
        "1+e\n",
        "−z\n",
        "\n",
        "1\n",
        "​\n",
        " , introduces a non-linearity that squashes any real-valued input into the desired probabilistic range. The resulting value, P(Y=1∣X), represents the conditional probability of the dependent variable Y belonging to the positive class (typically coded as 1) given the set of independent variables X. This probability can then be used to make a binary classification decision based on a chosen threshold, often 0.5. The coefficients in the equation are typically estimated using maximum likelihood estimation, aiming to find the values that best fit the observed binary outcomes in the training data."
      ],
      "metadata": {
        "id": "SEJnAeckFsVt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "y3MytNj8Fux4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Why do we use the Sigmoid function in Logistic Regression."
      ],
      "metadata": {
        "id": "0MqvLHgeFws6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Sigmoid function is a crucial component of Logistic Regression for several key reasons. Primarily, Logistic Regression is designed for binary classification, meaning it predicts the probability of an instance belonging to one of two classes (e.g., 0 or 1, yes or no). The sigmoid function, with its characteristic \"S\" shape, has the essential property of mapping any real-valued number into a probability value between 0 and 1. This makes the output of the Logistic Regression model directly interpretable as the probability of the positive class.\n",
        "\n",
        "Without the sigmoid function, the output of the linear combination of input features could range from negative infinity to positive infinity, which wouldn't be meaningful for representing probabilities. The sigmoid function effectively \"squashes\" this unbounded output into the [0, 1] range, providing a clear and probabilistic interpretation. Furthermore, the sigmoid function is differentiable, a critical property that allows us to use gradient-based optimization algorithms like gradient descent to train the Logistic Regression model efficiently. The derivative of the sigmoid function has a simple and useful form, which simplifies the calculation of gradients during the model training process. In essence, the sigmoid function bridges the gap between a linear model and a probabilistic output suitable for binary classification."
      ],
      "metadata": {
        "id": "Hb-uLTkNF0TY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the cost function of Logistic Regression."
      ],
      "metadata": {
        "id": "cqAGcHdkF5Nu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cost function of Logistic Regression is based on the concept of maximum likelihood estimation, and it measures how well the model's predictions match the actual labels.\n",
        "\n",
        "1. Hypothesis (sigmoid function):\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "𝜃\n",
        "𝑇\n",
        "𝑥\n",
        "h\n",
        "θ\n",
        "​\n",
        " (x)=\n",
        "1+e\n",
        "−θ\n",
        "T\n",
        " x\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "This function outputs a value between 0 and 1, representing the probability that the output is class 1.\n",
        "\n",
        "2. Cost function (Binary Cross-Entropy / Log Loss):\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "=\n",
        "−\n",
        "1\n",
        "𝑚\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑚\n",
        "[\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        ")\n",
        "]\n",
        "J(θ)=−\n",
        "m\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "m\n",
        "​\n",
        " [y\n",
        "(i)\n",
        " log(h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " ))+(1−y\n",
        "(i)\n",
        " )log(1−h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " ))]\n",
        "Where:\n",
        "\n",
        "𝑚\n",
        "m = number of training examples\n",
        "\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "y\n",
        "(i)\n",
        "  = true label for the\n",
        "𝑖\n",
        "i-th training example (either 0 or 1)\n",
        "\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " ) = predicted probability for class 1\n",
        "\n",
        "Why this cost function?\n",
        "When\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "y=1, the cost is\n",
        "−\n",
        "log\n",
        "⁡\n",
        "(\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        ")\n",
        ")\n",
        "−log(h\n",
        "θ\n",
        "​\n",
        " (x))\n",
        "\n",
        "When\n",
        "𝑦\n",
        "=\n",
        "0\n",
        "y=0, the cost is\n",
        "−\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        ")\n",
        ")\n",
        "−log(1−h\n",
        "θ\n",
        "​\n",
        " (x))\n",
        "\n",
        "This penalizes incorrect predictions heavily and encourages the model to output probabilities close to the true labels."
      ],
      "metadata": {
        "id": "jSZgVN0jF74d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yvlWNy2vGO45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is Regularization in Logistic Regression? Why is it needed."
      ],
      "metadata": {
        "id": "fvuptWWGGYrX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization in logistic regression is a technique used to improve the generalization ability of the model by preventing overfitting. Overfitting happens when a model learns not only the underlying pattern in the training data but also the noise, leading to poor performance on new, unseen data. Regularization addresses this by adding a penalty to the cost function, discouraging the model from fitting the training data too closely or assigning overly large weights to features.\n",
        "\n",
        "In the context of logistic regression, the regularized cost function includes an additional term that penalizes large coefficient values. This helps keep the model simpler and more robust. One common form is L2 regularization (also known as Ridge regularization), where the penalty term is the sum of the squares of the model’s parameters. The updated cost function is the original logistic loss plus the regularization term, scaled by a factor\n",
        "𝜆\n",
        "λ, which controls the strength of the penalty. A larger\n",
        "𝜆\n",
        "λ leads to more regularization (simpler model), while a smaller\n",
        "𝜆\n",
        "λ allows the model to fit the training data more closely.\n",
        "\n",
        "The key idea behind regularization is that simpler models are less likely to overfit, especially when dealing with high-dimensional data or a small training set. By controlling the complexity of the model, regularization helps ensure that the model captures the essential trends in the data without being overly sensitive to random fluctuations or noise."
      ],
      "metadata": {
        "id": "UUX5Ja3kGcDs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain the difference between Lasso, Ridge, and Elastic Net regression"
      ],
      "metadata": {
        "id": "gHUnxwDZGntH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasso, Ridge, and Elastic Net are all regularization techniques used to prevent overfitting in linear and logistic regression by adding penalty terms to the cost function. While they share a common goal, they differ in how they apply penalties to the model coefficients.\n",
        "\n",
        "1. Ridge Regression (L2 Regularization)\n",
        "Ridge regression adds a penalty equal to the sum of the squares of the coefficients to the cost function. This technique shrinks the coefficients but does not force any of them to be exactly zero.\n",
        "\n",
        "Penalty term:\n",
        "𝜆\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝜃\n",
        "𝑗\n",
        "2\n",
        "λ∑\n",
        "j=1\n",
        "n\n",
        "​\n",
        " θ\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "\n",
        "Effect: Reduces model complexity by shrinking coefficients; all features remain in the model.\n",
        "\n",
        "Use case: When all features are potentially useful and multicollinearity exists.\n",
        "\n",
        "2. Lasso Regression (L1 Regularization)\n",
        "Lasso (Least Absolute Shrinkage and Selection Operator) adds a penalty equal to the sum of the absolute values of the coefficients. This can shrink some coefficients to exactly zero, effectively performing feature selection.\n",
        "\n",
        "Penalty term:\n",
        "𝜆\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "∣\n",
        "𝜃\n",
        "𝑗\n",
        "∣\n",
        "λ∑\n",
        "j=1\n",
        "n\n",
        "​\n",
        " ∣θ\n",
        "j\n",
        "​\n",
        " ∣\n",
        "\n",
        "Effect: Encourages sparsity in the model; some coefficients become exactly zero.\n",
        "\n",
        "Use case: When you suspect only a subset of features are important.\n",
        "\n",
        "3. Elastic Net Regression\n",
        "Elastic Net combines both L1 (Lasso) and L2 (Ridge) penalties. It strikes a balance between shrinking coefficients and selecting important features, making it especially useful when you have many correlated features.\n",
        "\n",
        "Penalty term:\n",
        "𝜆\n",
        "[\n",
        "𝛼\n",
        "∑\n",
        "∣\n",
        "𝜃\n",
        "𝑗\n",
        "∣\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝛼\n",
        ")\n",
        "∑\n",
        "𝜃\n",
        "𝑗\n",
        "2\n",
        "]\n",
        "λ[α∑∣θ\n",
        "j\n",
        "​\n",
        " ∣+(1−α)∑θ\n",
        "j\n",
        "2\n",
        "​\n",
        " ]\n",
        "where\n",
        "𝛼\n",
        "∈\n",
        "[\n",
        "0\n",
        ",\n",
        "1\n",
        "]\n",
        "α∈[0,1] controls the mix between Lasso and Ridge.\n",
        "\n",
        "Effect: Performs both feature selection and coefficient shrinkage.\n",
        "\n",
        "Use case: When you have many features, some correlated, and want both regularization and feature selection."
      ],
      "metadata": {
        "id": "AyFT8_P_GrVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. When should we use Elastic Net instead of Lasso or Ridge."
      ],
      "metadata": {
        "id": "6Z_wdsDLG0oR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Elastic Net should be used instead of Lasso or Ridge regression when you want the benefits of both feature selection and coefficient stability, especially in cases where the data has highly correlated features or when the number of features exceeds the number of observations. Lasso tends to select only one variable from a group of correlated ones and discard the rest, which might not be ideal if those features carry useful information. Ridge, while good at handling multicollinearity, includes all features and doesn’t perform feature selection. Elastic Net addresses both of these limitations by combining the L1 and L2 penalties, allowing it to both shrink and select groups of correlated variables together. It is also more stable than Lasso when the number of features is much larger than the number of samples, and it provides flexibility through a mixing parameter that can be tuned depending on whether more Lasso-like or Ridge-like behavior is desired. This makes Elastic Net a strong choice for high-dimensional datasets or when feature selection and generalization are both important."
      ],
      "metadata": {
        "id": "cKi-otTvG33g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the impact of the regularization parameter (λ) in Logistic Regression."
      ],
      "metadata": {
        "id": "d56-njVHHCPU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The regularization parameter, usually denoted by λ (lambda), plays a crucial role in logistic regression with regularization, as it controls the strength of the penalty applied to the model’s coefficients. It directly affects how much regularization is applied, influencing both the complexity of the model and its performance.\n",
        "\n",
        "When λ is large, the penalty term dominates the cost function. This causes the model to shrink the coefficients significantly, often pushing them closer to zero. As a result, the model becomes simpler and less sensitive to noise in the training data, which helps prevent overfitting. However, if λ is too large, it can lead to underfitting, where the model becomes too simplistic to capture important patterns in the data, resulting in poor performance on both training and test sets.\n",
        "\n",
        "Conversely, when λ is small, the regularization effect is weak, and the model behaves more like standard logistic regression without regularization. This allows the model to fit the training data more closely, potentially capturing complex relationships. However, with little to no regularization, the model is at a higher risk of overfitting, especially if the dataset has many features or noise.\n",
        "\n",
        "Therefore, selecting the right value for λ is a trade-off between bias and variance. A good λ value helps achieve the right balance, leading to better generalization on unseen data. Typically, this value is determined using cross-validation, where the model’s performance is tested on a validation set for various λ values to find the optimal one."
      ],
      "metadata": {
        "id": "ZfqHAjWFHFPm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What are the key assumptions of Logistic Regression.\n"
      ],
      "metadata": {
        "id": "mJCkFtGTHRWU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression, while powerful and widely used for binary classification tasks, relies on several key assumptions to produce reliable and interpretable results. Understanding these assumptions helps ensure that the model is appropriate for your data and that its predictions can be trusted.\n",
        "\n",
        "Linear Relationship between Features and the Log-Odds:\n",
        "Logistic regression assumes a linear relationship between the independent variables and the logit (log-odds) of the dependent variable, not the outcome itself. This means the log-odds of the outcome (rather than the probability) must be linearly related to the predictors.\n",
        "\n",
        "Independence of Observations:\n",
        "The observations in the dataset should be independent of each other. This is especially important in time series data or clustered data (like survey responses from the same group), where observations might influence one another. Violating this assumption can lead to biased coefficient estimates.\n",
        "\n",
        "No Multicollinearity:\n",
        "Logistic regression assumes that the independent variables are not highly correlated with each other. Multicollinearity (when two or more predictors are highly correlated) can make it difficult to estimate the coefficients accurately and interpret their individual effects.\n",
        "\n",
        "Large Sample Size:\n",
        "Logistic regression generally requires a large sample size, especially when there are multiple predictors. This helps ensure stable and reliable estimates of the model parameters, and improves the model's ability to detect true effects.\n",
        "\n",
        "Binary or Dichotomous Outcome Variable:\n",
        "The dependent variable in logistic regression should be binary (e.g., yes/no, 0/1, success/failure). For multi-class classification, extensions like multinomial logistic regression are used instead.\n",
        "\n",
        "No Strong Outliers or Influential Points:\n",
        "Outliers or highly influential points can disproportionately affect the logistic regression model. It's important to check for and address such values during data preprocessing.\n",
        "\n",
        "Independent Variables are Meaningfully Related to the Outcome:\n",
        "Although not a technical assumption, logistic regression works best when predictors have a real relationship with the outcome. Irrelevant or noisy features can reduce model performance and interpretability."
      ],
      "metadata": {
        "id": "ORJKtnSuHT27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are some alternatives to Logistic Regression for classification tasks."
      ],
      "metadata": {
        "id": "kIu4SWjbHdb8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several alternatives to logistic regression for classification tasks, each with its strengths and weaknesses. The choice of method depends on the nature of your data, the complexity of the problem, and the performance criteria you prioritize (e.g., accuracy, interpretability, computation time). Here are some popular alternatives:\n",
        "\n",
        "1. Decision Trees\n",
        "Overview: Decision trees are a non-linear classifier that split the data based on feature values to make decisions. The tree is constructed by recursively splitting the data into subsets based on the feature that best separates the data points.\n",
        "\n",
        "Advantages: Easy to interpret, handle both numerical and categorical data, and can model non-linear relationships.\n",
        "\n",
        "Disadvantages: Prone to overfitting, especially with deep trees; sensitive to small variations in data.\n",
        "\n",
        "2. Random Forest\n",
        "Overview: Random Forest is an ensemble method that combines many decision trees to improve accuracy. It works by constructing multiple decision trees on random subsets of the data and then averaging their predictions for classification.\n",
        "\n",
        "Advantages: Robust against overfitting, handles both classification and regression tasks, and works well with large datasets.\n",
        "\n",
        "Disadvantages: Less interpretable than a single decision tree, can be computationally expensive, and slower to make predictions due to the large number of trees.\n",
        "\n",
        "3. Support Vector Machines (SVM)\n",
        "Overview: SVM is a powerful classifier that finds a hyperplane (in higher dimensions) that best separates the data points of different classes. It is particularly effective in high-dimensional spaces.\n",
        "\n",
        "Advantages: Effective in high-dimensional spaces, works well with complex decision boundaries, and robust to overfitting (especially in high-dimensional spaces).\n",
        "\n",
        "Disadvantages: Computationally expensive, especially with large datasets, and difficult to interpret. The choice of kernel and hyperparameters (like C and gamma) is crucial.\n",
        "\n",
        "4. K-Nearest Neighbors (KNN)\n",
        "Overview: KNN is a simple algorithm that classifies a data point based on the majority class of its nearest neighbors in the feature space.\n",
        "\n",
        "Advantages: Simple, easy to understand and implement, and no training phase required (instance-based learning).\n",
        "\n",
        "Disadvantages: Computationally expensive during prediction (because it requires distance calculations for all training points), sensitive to irrelevant features, and struggles with high-dimensional data (curse of dimensionality).\n",
        "\n",
        "5. Naive Bayes\n",
        "Overview: Naive Bayes is a probabilistic classifier based on Bayes' theorem, assuming that features are conditionally independent given the class. It works well when the assumption of independence holds or is approximately true.\n",
        "\n",
        "Advantages: Simple, fast, works well with small datasets and high-dimensional data (e.g., text classification), and performs well with categorical features.\n",
        "\n",
        "Disadvantages: The assumption of conditional independence often doesn't hold in real-world datasets, limiting its performance in certain cases.\n",
        "\n",
        "6. Gradient Boosting Machines (GBM)\n",
        "Overview: Gradient Boosting is an ensemble technique that builds decision trees sequentially, where each tree attempts to correct the errors of the previous one. Popular algorithms include XGBoost, LightGBM, and CatBoost.\n",
        "\n",
        "Advantages: Often provides state-of-the-art performance, robust to overfitting when tuned properly, and handles various types of data.\n",
        "\n",
        "Disadvantages: Can be computationally expensive, requires careful tuning, and is less interpretable than some other models.\n",
        "\n",
        "7. Neural Networks\n",
        "Overview: Neural networks are powerful models that can capture highly complex patterns by using multiple layers of interconnected nodes. For classification tasks, a common type of neural network is the feedforward neural network.\n",
        "\n",
        "Advantages: Highly flexible and can model very complex relationships. They perform exceptionally well on large datasets and tasks like image recognition, natural language processing, and more.\n",
        "\n",
        "Disadvantages: Require a large amount of data for training, computationally intensive, difficult to interpret (a \"black box\" model), and require careful tuning of hyperparameters (e.g., learning rate, number of layers).\n",
        "\n",
        "8. Linear Discriminant Analysis (LDA)\n",
        "Overview: LDA is a linear classifier that works by finding a projection of the data that maximizes the separation between classes. It assumes that the features are normally distributed and have the same covariance matrix for each class.\n",
        "\n",
        "Advantages: Effective when the class distributions are roughly Gaussian, works well when the classes are well-separated, and has a strong theoretical foundation.\n",
        "\n",
        "Disadvantages: Assumes normally distributed features with the same variance across classes, which may not always hold in practice.\n",
        "\n",
        "9. Elastic Net\n",
        "Overview: Elastic Net is a combination of Lasso (L1) and Ridge (L2) regularization, which can be used for classification tasks in a similar way as logistic regression but with the added benefit of both feature selection and regularization.\n",
        "\n",
        "Advantages: Works well when there are many correlated features, and it can handle high-dimensional data more effectively.\n",
        "\n",
        "Disadvantages: Like logistic regression, it assumes a linear relationship between features and the log-odds of the outcome."
      ],
      "metadata": {
        "id": "S8kTZBWzHhQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What are Classification Evaluation Metrics."
      ],
      "metadata": {
        "id": "oE864HCZHtq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification evaluation metrics are used to assess the performance of a classification model. These metrics provide insights into how well the model is predicting the different classes and can highlight areas of strength or weakness, especially when dealing with imbalanced datasets or multi-class problems. Here are the most commonly used classification evaluation metrics:\n",
        "\n",
        "1. Accuracy\n",
        "Definition: Accuracy measures the proportion of correctly predicted instances out of the total instances.\n",
        "\n",
        "Formula:\n",
        "\n",
        "Accuracy\n",
        "=\n",
        "True Positives (TP) + True Negatives (TN)\n",
        "Total Instances (TP + TN + FP + FN)\n",
        "Accuracy=\n",
        "Total Instances (TP + TN + FP + FN)\n",
        "True Positives (TP) + True Negatives (TN)\n",
        "​\n",
        "\n",
        "Use case: Accuracy is straightforward and useful when the classes are balanced. However, it can be misleading in imbalanced datasets, where the model could predict the majority class well but still perform poorly overall.\n",
        "\n",
        "2. Precision (Positive Predictive Value)\n",
        "Definition: Precision measures the proportion of positive predictions that are actually correct (i.e., how many of the predicted positive instances are truly positive).\n",
        "\n",
        "Formula:\n",
        "\n",
        "Precision\n",
        "=\n",
        "True Positives (TP)\n",
        "True Positives (TP) + False Positives (FP)\n",
        "Precision=\n",
        "True Positives (TP) + False Positives (FP)\n",
        "True Positives (TP)\n",
        "​\n",
        "\n",
        "Use case: Precision is particularly important when the cost of false positives is high, such as in fraud detection or medical diagnoses (e.g., incorrectly diagnosing a healthy person as sick).\n",
        "\n",
        "3. Recall (Sensitivity, True Positive Rate)\n",
        "Definition: Recall measures the proportion of actual positive instances that were correctly identified by the model (i.e., how many of the true positive instances were correctly predicted).\n",
        "\n",
        "Formula:\n",
        "\n",
        "Recall\n",
        "=\n",
        "True Positives (TP)\n",
        "True Positives (TP) + False Negatives (FN)\n",
        "Recall=\n",
        "True Positives (TP) + False Negatives (FN)\n",
        "True Positives (TP)\n",
        "​\n",
        "\n",
        "Use case: Recall is crucial when the cost of false negatives is high, such as in cancer detection (where missing a positive case can have severe consequences).\n",
        "\n",
        "4. F1 Score\n",
        "Definition: The F1 score is the harmonic mean of precision and recall. It provides a balanced measure of both precision and recall, especially useful when the data is imbalanced.\n",
        "\n",
        "Formula:\n",
        "\n",
        "F1 Score\n",
        "=\n",
        "2\n",
        "×\n",
        "Precision\n",
        "×\n",
        "Recall\n",
        "Precision\n",
        "+\n",
        "Recall\n",
        "F1 Score=2×\n",
        "Precision+Recall\n",
        "Precision×Recall\n",
        "​\n",
        "\n",
        "Use case: The F1 score is ideal when you need a balance between precision and recall and when you have an imbalanced dataset.\n",
        "\n",
        "5. ROC Curve (Receiver Operating Characteristic Curve)\n",
        "Definition: The ROC curve is a graphical representation of the performance of a binary classifier as its discrimination threshold is varied. It plots the True Positive Rate (Recall) on the y-axis against the False Positive Rate (1 - Specificity) on the x-axis.\n",
        "\n",
        "Use case: The ROC curve is useful for comparing different models and for evaluating how well the model distinguishes between the two classes across various threshold values.\n",
        "\n",
        "6. AUC (Area Under the ROC Curve)\n",
        "Definition: AUC represents the area under the ROC curve and provides an aggregate measure of model performance across all possible classification thresholds. AUC ranges from 0 to 1, with 1 indicating perfect classification and 0.5 indicating random guessing.\n",
        "\n",
        "Use case: AUC is particularly helpful for evaluating binary classification models, especially when dealing with imbalanced datasets.\n",
        "\n",
        "7. Confusion Matrix\n",
        "Definition: A confusion matrix is a table that summarizes the performance of a classification algorithm by showing the actual versus predicted classifications. It includes:\n",
        "\n",
        "True Positives (TP): Correctly predicted positive instances.\n",
        "\n",
        "True Negatives (TN): Correctly predicted negative instances.\n",
        "\n",
        "False Positives (FP): Negative instances incorrectly classified as positive.\n",
        "\n",
        "False Negatives (FN): Positive instances incorrectly classified as negative.\n",
        "\n",
        "Use case: A confusion matrix provides a detailed view of how well the model is performing across different classes. It's the basis for many other evaluation metrics.\n",
        "\n",
        "8. Specificity (True Negative Rate)\n",
        "Definition: Specificity measures the proportion of actual negative instances that were correctly identified as negative by the model.\n",
        "\n",
        "Formula:\n",
        "\n",
        "Specificity\n",
        "=\n",
        "True Negatives (TN)\n",
        "True Negatives (TN) + False Positives (FP)\n",
        "Specificity=\n",
        "True Negatives (TN) + False Positives (FP)\n",
        "True Negatives (TN)\n",
        "​\n",
        "\n",
        "Use case: Specificity is important when the cost of false positives is high and you want to ensure that negative cases are correctly identified, such as in spam filtering.\n",
        "\n",
        "9. Logarithmic Loss (Log Loss)\n",
        "Definition: Log loss measures the accuracy of the classifier by penalizing false classifications with a higher cost as the predicted probability diverges from the actual class label. It calculates the cross-entropy between the true labels and predicted probabilities.\n",
        "\n",
        "Formula:\n",
        "\n",
        "Log Loss\n",
        "=\n",
        "−\n",
        "1\n",
        "𝑚\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑚\n",
        "[\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑝\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑝\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "]\n",
        "Log Loss=−\n",
        "m\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "m\n",
        "​\n",
        " [y\n",
        "(i)\n",
        " log(p\n",
        "(i)\n",
        " )+(1−y\n",
        "(i)\n",
        " )log(1−p\n",
        "(i)\n",
        " )]\n",
        "where\n",
        "𝑝\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "p\n",
        "(i)\n",
        "  is the predicted probability and\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "y\n",
        "(i)\n",
        "  is the true label.\n",
        "\n",
        "Use case: Log loss is used when evaluating models that output probabilities, and it's particularly useful in models where you care about confidence in predictions (e.g., in probabilistic classification tasks).\n",
        "\n",
        "10. Matthews Correlation Coefficient (MCC)\n",
        "Definition: The Matthews Correlation Coefficient is a balanced metric that takes into account true and false positives, as well as true and false negatives. It ranges from -1 (perfectly incorrect) to +1 (perfectly correct), with 0 indicating no better than random prediction.\n",
        "\n",
        "Formula:\n",
        "\n",
        "MCC\n",
        "=\n",
        "TP\n",
        "×\n",
        "TN\n",
        "−\n",
        "FP\n",
        "×\n",
        "FN\n",
        "(\n",
        "TP\n",
        "+\n",
        "FP\n",
        ")\n",
        "(\n",
        "TP\n",
        "+\n",
        "FN\n",
        ")\n",
        "(\n",
        "TN\n",
        "+\n",
        "FP\n",
        ")\n",
        "(\n",
        "TN\n",
        "+\n",
        "FN\n",
        ")\n",
        "MCC=\n",
        "(TP+FP)(TP+FN)(TN+FP)(TN+FN)\n",
        "​\n",
        "\n",
        "TP×TN−FP×FN\n",
        "​\n",
        "\n",
        "Use case: MCC is useful for imbalanced datasets, as it takes all elements of the confusion matrix into account."
      ],
      "metadata": {
        "id": "DviSIZvgHwYF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How does class imbalance affect Logistic Regression.\n"
      ],
      "metadata": {
        "id": "aGRvkfwhIHfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class imbalance can significantly affect Logistic Regression by biasing the model towards the majority class, which leads to suboptimal performance, especially for the minority class. In an imbalanced dataset, the model may predict the majority class most of the time to minimize the overall error, resulting in poor predictions for the minority class. This causes accuracy to become a misleading metric, as the model could have high accuracy by simply predicting the majority class, even if it fails to correctly predict instances of the minority class.\n",
        "\n",
        "Moreover, class imbalance can lead to biased coefficients, where the model assigns more importance to features related to the majority class, neglecting features that could be useful for the minority class. This makes the model less generalizable, particularly when deployed in real-world scenarios where both classes are important.\n",
        "\n",
        "To address class imbalance in Logistic Regression, several techniques can be used:\n",
        "\n",
        "Resampling: Either oversample the minority class or undersample the majority class to balance the dataset. Synthetic sampling techniques like SMOTE can also be used.\n",
        "\n",
        "Class weights: Assign higher weights to the minority class to make the model pay more attention to those instances, helping to balance the model’s predictions.\n",
        "\n",
        "Evaluation metrics: Use metrics like precision, recall, F1 score, or balanced accuracy instead of plain accuracy to better assess model performance on both classes.\n",
        "\n",
        "Threshold adjustment: Adjust the decision threshold to increase the model's sensitivity to the minority class.\n",
        "\n",
        "These strategies help mitigate the impact of class imbalance and improve the model's performance across both classes."
      ],
      "metadata": {
        "id": "5tiQ-Tx-INb5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is Hyperparameter Tuning in Logistic Regression.\n"
      ],
      "metadata": {
        "id": "aDXTKm3uIXQX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tuning in Logistic Regression refers to the process of selecting the best set of hyperparameters that maximize the model's performance. Hyperparameters are parameters that are not learned from the data during the training process but are set before training begins. In Logistic Regression, hyperparameter tuning helps optimize the model to better fit the data, improving generalization and reducing overfitting.\n",
        "\n",
        "Key Hyperparameters in Logistic Regression:\n",
        "Regularization Strength (C):\n",
        "\n",
        "Description: The regularization parameter, denoted as C, controls the strength of regularization applied to the model. A small value of C means stronger regularization (more penalty on large coefficients), which can reduce overfitting but may cause underfitting. A larger value of C means weaker regularization, allowing the model to fit the training data more closely.\n",
        "\n",
        "Tuning Goal: Find the value of C that balances bias and variance, avoiding overfitting or underfitting.\n",
        "\n",
        "Regularization Type (Penalty):\n",
        "\n",
        "Description: Logistic Regression can use different types of regularization. The two most common types are L1 (Lasso) and L2 (Ridge). L1 regularization tends to create sparse models by setting some coefficients to zero, leading to feature selection. L2 regularization, on the other hand, penalizes large coefficients but keeps all features.\n",
        "\n",
        "Tuning Goal: Choose between L1 or L2 regularization based on the data and whether feature selection is desired. ElasticNet is another option that combines L1 and L2 regularization.\n",
        "\n",
        "Solver:\n",
        "\n",
        "Description: The solver is the algorithm used to optimize the logistic regression model. Common solvers include 'liblinear', 'newton-cg', 'lbfgs', and 'saga'. Some solvers work better with large datasets, while others are better for small datasets.\n",
        "\n",
        "Tuning Goal: Select the solver that works best with your dataset, especially if the dataset is large or sparse.\n",
        "\n",
        "Max Iterations:\n",
        "\n",
        "Description: The maximum number of iterations for the optimization algorithm to converge to a solution. If the algorithm doesn’t converge within the set number of iterations, the solution may not be accurate.\n",
        "\n",
        "Tuning Goal: Set an appropriate value for the maximum number of iterations to ensure convergence.\n",
        "\n",
        "Hyperparameter Tuning Methods:\n",
        "Grid Search:\n",
        "\n",
        "Description: Grid search involves specifying a set of hyperparameters and exhaustively searching through all possible combinations to find the best performing set.\n",
        "\n",
        "Advantage: Comprehensive, but computationally expensive.\n",
        "\n",
        "Random Search:\n",
        "\n",
        "Description: Instead of searching all combinations, random search selects random combinations of hyperparameters, which can be more efficient and often gives comparable results to grid search.\n",
        "\n",
        "Advantage: Faster than grid search and can find good hyperparameter combinations with fewer evaluations.\n",
        "\n",
        "Cross-Validation:\n",
        "\n",
        "Description: Cross-validation involves splitting the data into multiple subsets and training the model on different training sets while evaluating it on a separate validation set. This helps ensure that the model generalizes well and is not overfitting.\n",
        "\n",
        "Advantage: Reduces the risk of overfitting and provides a better estimate of model performance."
      ],
      "metadata": {
        "id": "moYa2gRSIcxl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What are different solvers in Logistic Regression? Which one should be used."
      ],
      "metadata": {
        "id": "qAvRNhesIlH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Logistic Regression, the solver is the algorithm used to optimize the model by finding the best parameters. Different solvers have varying strengths based on dataset size, regularization type, and computational efficiency. Here are the common solvers:\n",
        "\n",
        "liblinear: Suitable for small to medium datasets, this solver uses coordinate descent and supports both L1 (Lasso) and L2 (Ridge) regularization. It’s particularly useful when L1 regularization is required, as it performs feature selection. However, it’s not ideal for large datasets due to slower convergence.\n",
        "\n",
        "newton-cg: This is a second-order optimization method using Newton’s method. It’s efficient for medium to large datasets and works well with L2 regularization. It converges faster than first-order methods but is computationally more expensive.\n",
        "\n",
        "lbfgs: A quasi-Newton method that approximates the Hessian matrix and is more memory-efficient for large datasets. It is effective for L2 regularization and large datasets but can be slower than newton-cg on very small datasets.\n",
        "\n",
        "saga: This solver is an extension of SGD (Stochastic Gradient Descent), handling both L1 and L2 regularization efficiently. It works well for large datasets and is ideal for sparse data.\n",
        "\n",
        "sgd: Based on Stochastic Gradient Descent, this solver is ideal for very large datasets and incremental learning, but may require careful tuning of the learning rate.\n",
        "\n",
        "Which Solver to Use?\n",
        "Use liblinear for small datasets with L1 regularization.\n",
        "\n",
        "Use lbfgs or newton-cg for large datasets with L2 regularization.\n",
        "\n",
        "Use saga for large datasets with L1 regularization or sparse data."
      ],
      "metadata": {
        "id": "oxiXt4SCIoil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.How is Logistic Regression extended for multiclass classification."
      ],
      "metadata": {
        "id": "bPrZwymhI2mr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression is naturally designed for binary classification, but it can be extended to multiclass classification using two main techniques: One-vs-Rest (OvR) and Multinomial Logistic Regression.\n",
        "\n",
        "1. One-vs-Rest (OvR)\n",
        "In the OvR approach, a separate binary classifier is trained for each class. For a dataset with\n",
        "𝐾\n",
        "K classes,\n",
        "𝐾\n",
        "K classifiers are created. Each classifier learns to distinguish one class from all the others (i.e., it predicts whether the instance belongs to a specific class or not). When making predictions, the class corresponding to the classifier with the highest predicted probability is selected.\n",
        "\n",
        "Advantages: Simple to implement and works well with independent classes. It can be computationally efficient for small datasets. Disadvantages: Does not capture inter-class relationships and may suffer from class imbalance.\n",
        "\n",
        "2. Multinomial Logistic Regression (Softmax Regression)\n",
        "This approach uses the softmax function to model the probability distribution across all classes in a single classifier. The softmax function calculates the probability of each class based on the input features, ensuring that the probabilities sum to 1. The model is trained to minimize the cross-entropy loss, which is suitable for multi-class classification.\n",
        "\n",
        "Advantages: Captures relationships between classes and provides a unified model, making it suitable for complex multiclass problems. Disadvantages: More computationally expensive than OvR, especially with a large number of classes."
      ],
      "metadata": {
        "id": "oAUiSSsCI8il"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What are the advantages and disadvantages of Logistic Regression."
      ],
      "metadata": {
        "id": "u2K_YyZAJOi_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression is a widely used algorithm for classification tasks due to its simplicity and efficiency. However, like any model, it has both advantages and disadvantages. Here's a breakdown:\n",
        "\n",
        "Advantages of Logistic Regression:\n",
        "Simplicity and Interpretability:\n",
        "\n",
        "Logistic Regression is easy to understand and implement. The model’s output is the probability of an instance belonging to a particular class, which is interpretable and can be explained using the coefficients of the model.\n",
        "\n",
        "Efficiency:\n",
        "\n",
        "It is computationally efficient, especially for small to medium-sized datasets. Training a logistic regression model is fast compared to more complex models, making it ideal for quick predictions.\n",
        "\n",
        "Works Well for Linearly Separable Data:\n",
        "\n",
        "Logistic Regression performs well when the classes are linearly separable, i.e., when a straight line (or hyperplane in higher dimensions) can separate the classes.\n",
        "\n",
        "Probability Output:\n",
        "\n",
        "The model not only predicts the class but also provides the probability that an instance belongs to a certain class, which can be useful for decision-making and threshold adjustment.\n",
        "\n",
        "Regularization:\n",
        "\n",
        "Logistic Regression allows for regularization (L1 and L2), which helps prevent overfitting by penalizing large coefficients, especially in high-dimensional data.\n",
        "\n",
        "Disadvantages of Logistic Regression:\n",
        "Assumes Linearity:\n",
        "\n",
        "Logistic Regression assumes that the relationship between the independent variables and the log-odds of the dependent variable is linear. If the data is not linearly separable, the model’s performance may be poor.\n",
        "\n",
        "Sensitive to Outliers:\n",
        "\n",
        "Logistic Regression can be sensitive to outliers, which may skew the results and lead to suboptimal predictions.\n",
        "\n",
        "Not Ideal for Complex Relationships:\n",
        "\n",
        "It struggles to capture complex patterns in the data. For datasets with non-linear decision boundaries or interactions between features, more complex models like Decision Trees or Neural Networks might be more effective.\n",
        "\n",
        "Limited to Binary and Multiclass Problems:\n",
        "\n",
        "While it can be extended to multiclass classification using techniques like One-vs-Rest or Multinomial Logistic Regression, logistic regression is fundamentally designed for binary classification tasks.\n",
        "\n",
        "Requires Feature Scaling:\n",
        "\n",
        "Logistic Regression can be sensitive to the scale of the input features. Feature scaling (e.g., normalization or standardization) is often necessary to improve model performance and convergence speed."
      ],
      "metadata": {
        "id": "AVmiQyoKJRbW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are some use cases of Logistic Regression."
      ],
      "metadata": {
        "id": "cWVJpyVSJZid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression is a versatile and widely used algorithm for classification tasks. Below are some common use cases where logistic regression is applied:\n",
        "\n",
        "1. Spam Email Detection:\n",
        "Use Case: Identifying whether an email is spam or not spam (ham) based on features like the presence of certain words, the sender's address, and email structure.\n",
        "\n",
        "Why Logistic Regression: It's a binary classification problem, where the output is either spam (1) or not spam (0). Logistic Regression is efficient and interpretable for such tasks.\n",
        "\n",
        "2. Customer Churn Prediction:\n",
        "Use Case: Predicting whether a customer will leave (churn) or stay with a service, such as a subscription-based service or telecom provider.\n",
        "\n",
        "Why Logistic Regression: It can model the likelihood of a customer churning based on features like usage patterns, service plans, customer behavior, and demographics.\n",
        "\n",
        "3. Credit Scoring and Loan Default Prediction:\n",
        "Use Case: Determining the likelihood that a borrower will default on a loan or credit card payment based on historical data and financial indicators.\n",
        "\n",
        "Why Logistic Regression: It’s a binary classification problem where the output is either default (1) or no default (0), and logistic regression provides a probability that can be used to make decisions.\n",
        "\n",
        "4. Disease Diagnosis (e.g., Diabetes, Heart Disease):\n",
        "Use Case: Predicting whether a patient has a certain disease (e.g., diabetes, heart disease) based on factors like age, blood pressure, cholesterol levels, and other medical history.\n",
        "\n",
        "Why Logistic Regression: Logistic regression is commonly used in healthcare to predict the likelihood of disease occurrence, especially when the relationship between the features and the disease is expected to be linear or nearly linear.\n",
        "\n",
        "5. Marketing Campaign Effectiveness:\n",
        "Use Case: Determining whether a customer will respond to a marketing campaign (e.g., will they click on an ad or buy a product?).\n",
        "\n",
        "Why Logistic Regression: The goal is to classify customers based on their likelihood of responding to the campaign (yes/no), and logistic regression can estimate probabilities of these outcomes.\n",
        "\n",
        "6. Web Page Click Prediction:\n",
        "Use Case: Predicting whether a user will click on a particular link or advertisement on a webpage.\n",
        "\n",
        "Why Logistic Regression: Logistic regression is used to model the probability of a click (1) versus no click (0) based on user behavior data, demographics, and other relevant features.\n",
        "\n",
        "7. Fraud Detection:\n",
        "Use Case: Detecting fraudulent transactions in financial systems (e.g., credit card fraud detection).\n",
        "\n",
        "Why Logistic Regression: Fraud detection can be framed as a binary classification problem (fraud or not fraud). Logistic regression can be used to model the probability of a transaction being fraudulent based on various transaction features.\n",
        "\n",
        "8. Elections and Political Polling:\n",
        "Use Case: Predicting the outcome of an election or polling survey based on demographic data and previous voting patterns.\n",
        "\n",
        "Why Logistic Regression: Logistic regression can predict the likelihood of a candidate winning or a party receiving a certain percentage of the vote, based on historical voting trends and demographic features.\n",
        "\n",
        "9. Medical Risk Prediction:\n",
        "Use Case: Estimating the likelihood of a patient developing a medical condition or the risk of complications (e.g., predicting the risk of stroke or cancer relapse).\n",
        "\n",
        "Why Logistic Regression: It provides a clear, interpretable model to estimate the probability of medical events based on factors like age, genetics, lifestyle, and previous medical conditions.\n",
        "\n",
        "10. Image Classification (Binary):\n",
        "Use Case: Classifying images as belonging to one of two categories, such as distinguishing between pictures of dogs and cats, or identifying cancerous vs. non-cancerous tumors in medical images.\n",
        "\n",
        "Why Logistic Regression: Logistic regression can be applied to image classification by converting pixel values into features. While more advanced models like CNNs are typically used for image classification, logistic regression can still be effective for simpler binary image tasks."
      ],
      "metadata": {
        "id": "56y8sLhoJjMo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is the difference between Softmax Regression and Logistic Regression."
      ],
      "metadata": {
        "id": "_9NHIh3dJsHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression and Softmax Regression are both used for classification tasks, but they differ primarily in the types of problems they solve and the way they handle output predictions. Here’s a breakdown of the key differences:\n",
        "\n",
        "1. Type of Classification Problem\n",
        "Logistic Regression: Primarily used for binary classification problems, where the goal is to classify instances into one of two classes. The output of logistic regression is a probability that an instance belongs to a particular class, which is then thresholded to make the final binary decision (e.g., class 0 or class 1).\n",
        "\n",
        "Softmax Regression: An extension of logistic regression, Softmax Regression (also called Multinomial Logistic Regression) is used for multiclass classification problems, where there are more than two classes. The softmax function is used to calculate a probability distribution across multiple classes, and the model outputs the class with the highest probability.\n",
        "\n",
        "2. Output Function\n",
        "Logistic Regression: In logistic regression, the output is a single probability value between 0 and 1. It is calculated using the sigmoid function (logistic function), which maps the linear combination of input features to a probability for a binary outcome:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "(\n",
        "𝑤\n",
        "𝑇\n",
        "𝑥\n",
        "+\n",
        "𝑏\n",
        ")\n",
        "P(y=1∣x)=\n",
        "1+e\n",
        "−(w\n",
        "T\n",
        " x+b)\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "Softmax Regression: In softmax regression, the output is a probability distribution over multiple classes. The softmax function is applied to the raw output (logits) for each class, ensuring that the probabilities sum to 1:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "𝑘\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "𝑒\n",
        "𝜃\n",
        "𝑘\n",
        "𝑇\n",
        "𝑥\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝐾\n",
        "𝑒\n",
        "𝜃\n",
        "𝑗\n",
        "𝑇\n",
        "𝑥\n",
        "P(y=k∣x)=\n",
        "∑\n",
        "j=1\n",
        "K\n",
        "​\n",
        " e\n",
        "θ\n",
        "j\n",
        "T\n",
        "​\n",
        " x\n",
        "\n",
        "e\n",
        "θ\n",
        "k\n",
        "T\n",
        "​\n",
        " x\n",
        "\n",
        "​\n",
        "\n",
        "where\n",
        "𝐾\n",
        "K is the total number of classes, and\n",
        "𝜃\n",
        "𝑘\n",
        "θ\n",
        "k\n",
        "​\n",
        "  is the parameter vector for class\n",
        "𝑘\n",
        "k.\n",
        "\n",
        "3. Objective\n",
        "Logistic Regression: The objective in logistic regression is to predict the probability that an instance belongs to class 1 (positive class) versus class 0 (negative class). The model is trained to minimize the binary cross-entropy loss.\n",
        "\n",
        "Softmax Regression: In softmax regression, the goal is to predict the probabilities of the instance belonging to one of\n",
        "𝐾\n",
        "K classes. The model is trained to minimize the categorical cross-entropy loss (also known as multiclass log loss), which is a generalization of binary cross-entropy for multiclass problems.\n",
        "\n",
        "4. Number of Classes\n",
        "Logistic Regression: Handles binary classification, meaning there are only two possible classes (e.g., class 0 and class 1).\n",
        "\n",
        "Softmax Regression: Handles multiclass classification, where there are more than two classes. The number of outputs is equal to the number of classes, and each class gets a separate set of parameters (one for each class).\n",
        "\n",
        "5. Model Representation\n",
        "Logistic Regression: For binary classification, logistic regression has a single parameter vector\n",
        "𝜃\n",
        "θ, which corresponds to a single decision boundary that separates the two classes.\n",
        "\n",
        "Softmax Regression: For multiclass classification, softmax regression has multiple parameter vectors (one for each class). These vectors correspond to different decision boundaries, and each class has its own set of weights and bias."
      ],
      "metadata": {
        "id": "cSWltg3hJwer"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.  How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification."
      ],
      "metadata": {
        "id": "3A2_6alQJ4ss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choosing between One-vs-Rest (OvR) and Softmax Regression (Multinomial Logistic Regression) for multiclass classification depends on several factors, including the complexity of the problem, the number of classes, computational efficiency, and the relationships between the classes. Here's a guide to help make the decision:\n",
        "\n",
        "1. Number of Classes\n",
        "One-vs-Rest (OvR): This approach can be used for multiclass classification problems, but it treats each class as a separate binary classification task. For a dataset with\n",
        "𝐾\n",
        "K classes, OvR trains\n",
        "𝐾\n",
        "K separate binary classifiers. Each classifier predicts whether the input belongs to that class or not.\n",
        "\n",
        "Softmax Regression: This method is inherently designed for multiclass classification. It models the problem with a single classifier that outputs a probability distribution over all\n",
        "𝐾\n",
        "K classes using the softmax function.\n",
        "\n",
        "When to Use:\n",
        "\n",
        "If the number of classes is large and the relationships between classes are not important, OvR can be a simpler and more efficient approach.\n",
        "\n",
        "If the number of classes is smaller or the classes are mutually exclusive and the relationships between them are important, Softmax Regression is often preferred because it provides a direct solution with one model.\n",
        "\n",
        "2. Complexity of Class Relationships\n",
        "One-vs-Rest (OvR): Since OvR trains separate classifiers for each class, it does not account for any relationships or interactions between the classes. Each classifier is independent of the others.\n",
        "\n",
        "Softmax Regression: Softmax Regression models the probability distribution over all classes and captures the relationships between classes directly. It is better at modeling dependencies among classes, as the softmax function ensures that the total probability sums to 1 across all classes.\n",
        "\n",
        "When to Use:\n",
        "\n",
        "Use OvR when you believe the classes are largely independent of each other.\n",
        "\n",
        "Use Softmax Regression when the classes are not independent and there is a need to capture their relationships.\n",
        "\n",
        "3. Computational Efficiency\n",
        "One-vs-Rest (OvR): The computational cost increases with the number of classes because you need to train a separate binary classifier for each class. In situations with many classes, this can be computationally expensive.\n",
        "\n",
        "Softmax Regression: Softmax Regression involves training a single model, making it computationally more efficient than training multiple classifiers in the OvR approach, especially for smaller datasets with a large number of classes.\n",
        "\n",
        "When to Use:\n",
        "\n",
        "OvR can be more computationally efficient when the number of classes is relatively small, as each binary classifier is simpler to train.\n",
        "\n",
        "Softmax Regression is typically more efficient in cases with many classes, as only one model needs to be trained.\n",
        "\n",
        "4. Performance Considerations\n",
        "One-vs-Rest (OvR): OvR can be less accurate when the classes are imbalanced or when the decision boundaries between classes are complex. In addition, OvR might face challenges if the classifiers output multiple high probabilities, leading to potential conflicts (i.e., both classifiers being confident about different classes for the same input).\n",
        "\n",
        "Softmax Regression: Softmax Regression directly handles the multiclass nature of the problem and avoids the potential issues with OvR, like multiple classifiers having high probabilities for different classes. It typically provides better performance in situations with complex decision boundaries.\n",
        "\n",
        "When to Use:\n",
        "\n",
        "OvR might work well for simple problems where the classes are relatively well-separated and the decision boundaries are not complex.\n",
        "\n",
        "Softmax Regression is preferred when there are complex decision boundaries between classes or when performance on more nuanced, overlapping classes is critical.\n",
        "\n",
        "5. Interpretability\n",
        "One-vs-Rest (OvR): OvR provides interpretable results because each binary classifier corresponds to the likelihood of an instance belonging to a specific class versus all others. This can be easier to understand, especially when the task involves a small number of classes.\n",
        "\n",
        "Softmax Regression: While Softmax Regression provides a probability distribution, it can be harder to interpret in terms of individual class decision boundaries, especially when there are many classes.\n",
        "\n",
        "When to Use:\n",
        "\n",
        "Use OvR when interpretability is a priority and when you need to understand each classifier’s decision-making process.\n",
        "\n",
        "Use Softmax Regression when the focus is more on the overall performance and you’re okay with less individual interpretability."
      ],
      "metadata": {
        "id": "Yae_PXDgJ76h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How do we interpret coefficients in Logistic Regression?"
      ],
      "metadata": {
        "id": "y83zzz15KJpH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpreting the coefficients in Logistic Regression helps understand the relationship between the input features and the probability of the target outcome. The model predicts the probability of a binary outcome, typically coded as 0 or 1, using a logistic function. The coefficients represent the impact of each feature on the log-odds of the outcome.\n",
        "\n",
        "Log-Odds Interpretation:\n",
        "The logistic regression model is expressed as:\n",
        "\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "1\n",
        "−\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        ")\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "log(\n",
        "1−P(y=1∣X)\n",
        "P(y=1∣X)\n",
        "​\n",
        " )=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        "\n",
        "Where the left side represents the log-odds of the outcome, and the right side is a linear combination of the features.\n",
        "\n",
        "Coefficient Interpretation:\n",
        "Sign of Coefficients:\n",
        "\n",
        "Positive Coefficient (\n",
        "𝛽\n",
        ">\n",
        "0\n",
        "β>0): A positive coefficient means that as the value of the feature increases, the log-odds of the positive outcome (e.g.,\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "y=1) increase, making the outcome more likely.\n",
        "\n",
        "Negative Coefficient (\n",
        "𝛽\n",
        "<\n",
        "0\n",
        "β<0): A negative coefficient means that as the feature increases, the log-odds of the positive outcome decrease, making the outcome less likely.\n",
        "\n",
        "Magnitude of Coefficients: The absolute value of the coefficient represents the strength of the effect. Larger magnitudes indicate a stronger influence on the outcome.\n",
        "\n",
        "Exponential of Coefficients (Odds Ratio):\n",
        "To interpret coefficients more intuitively, we exponentiate them to get the odds ratio:\n",
        "\n",
        "Odds Ratio\n",
        "=\n",
        "𝑒\n",
        "𝛽\n",
        "Odds Ratio=e\n",
        "β\n",
        "\n",
        "Odds Ratio > 1: The feature increases the odds of the outcome.\n",
        "\n",
        "Odds Ratio < 1: The feature decreases the odds of the outcome.\n",
        "\n",
        "Thus, the coefficient and odds ratio provide insight into how each feature influences the likelihood of the outcome."
      ],
      "metadata": {
        "id": "AJPrYfOBKNkJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practical**"
      ],
      "metadata": {
        "id": "Gm01SJdPKmiw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "W4D08Hv4KYsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
        "Regression, and prints the model accuracy"
      ],
      "metadata": {
        "id": "bzJNH5tUKrga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w88UnPA6Kvcj",
        "outputId": "ffb4a7ae-e9c7-4f98-caba-e0a79ac995de"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracyC"
      ],
      "metadata": {
        "id": "BrQKMUPRK14S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with L1 regularization (Lasso)\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with L1 Regularization (Lasso): {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpksPr10LBVp",
        "outputId": "0268cf9d-41c3-485f-abc8-b0ad214498fb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L1 Regularization (Lasso): 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients"
      ],
      "metadata": {
        "id": "bGI3dPxNLI9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with L2 regularization (Ridge)\n",
        "model = LogisticRegression(penalty='l2', solver='liblinear', max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with L2 Regularization (Ridge): {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Print the coefficients (weights) of the model\n",
        "print(\"Model Coefficients (Weights):\")\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9X1RJIwLOZk",
        "outputId": "c781da88-fe41-457a-84d0-2fb31633c14d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L2 Regularization (Ridge): 100.00%\n",
            "Model Coefficients (Weights):\n",
            "[[ 0.3711229   1.409712   -2.15210117 -0.95474179]\n",
            " [ 0.49400451 -1.58897112  0.43717015 -1.11187838]\n",
            " [-1.55895271 -1.58893375  2.39874554  2.15556209]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')"
      ],
      "metadata": {
        "id": "ey8VeoX3LT8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with Elastic Net regularization\n",
        "# Set l1_ratio between 0 and 1 (e.g., 0.5 for an equal mix of L1 and L2)\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', max_iter=200, l1_ratio=0.5)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with Elastic Net Regularization: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Print the coefficients (weights) of the model\n",
        "print(\"Model Coefficients (Weights):\")\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfcWDw8yLkpu",
        "outputId": "5b67d660-482b-418d-d5ee-0b93dc511c4c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with Elastic Net Regularization: 100.00%\n",
            "Model Coefficients (Weights):\n",
            "[[ 0.38702957  1.77237897 -2.41983777 -0.70886334]\n",
            " [ 0.07822625  0.          0.         -0.57985862]\n",
            " [-1.25904702 -1.53069476  2.59669944  2.08095053]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'"
      ],
      "metadata": {
        "id": "D0oHnpy6Lr6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with One-vs-Rest (OvR) strategy for multiclass classification\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with One-vs-Rest Strategy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Print the coefficients (weights) of the model\n",
        "print(\"Model Coefficients (Weights):\")\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dplD1l6YLzdB",
        "outputId": "dc1ce34d-2ae8-4c6a-f6d7-3386625434ca"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with One-vs-Rest Strategy: 100.00%\n",
            "Model Coefficients (Weights):\n",
            "[[ 0.3711229   1.409712   -2.15210117 -0.95474179]\n",
            " [ 0.49400451 -1.58897112  0.43717015 -1.11187838]\n",
            " [-1.55895271 -1.58893375  2.39874554  2.15556209]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy"
      ],
      "metadata": {
        "id": "nfS9XOCVL6XJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
        "    'penalty': ['l1', 'l2'],       # Regularization types (L1 or L2)\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, verbose=1)\n",
        "\n",
        "# Perform the grid search to find the best hyperparameters\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found by GridSearchCV\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the best model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the Best Model: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Nf1AIDaL-rY",
        "outputId": "6fb7964a-d92f-4b61-ab0f-c6ae2459d8a9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "Best Parameters: {'C': 1, 'penalty': 'l2'}\n",
            "Accuracy of the Best Model: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "25 fits failed out of a total of 50.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "25 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan 0.85833333        nan 0.93333333        nan 0.96666667\n",
            "        nan 0.94166667        nan 0.95      ]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracyC"
      ],
      "metadata": {
        "id": "qPkYS28MMFKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Initialize StratifiedKFold with 5 splits\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# List to store the accuracy for each fold\n",
        "accuracies = []\n",
        "\n",
        "# Perform Stratified K-Fold Cross-Validation\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Train the model on the training data\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test data\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate and store the accuracy for the current fold\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# Calculate the average accuracy across all folds\n",
        "average_accuracy = np.mean(accuracies)\n",
        "\n",
        "# Print the average accuracy\n",
        "print(f\"Average Accuracy using Stratified K-Fold Cross-Validation: {average_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LahmcCqMOEv",
        "outputId": "b67cc7f3-2e9a-4bd0-bb6d-192588066baa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy using Stratified K-Fold Cross-Validation: 96.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy."
      ],
      "metadata": {
        "id": "QxWDhFrLMVgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Load the dataset from a CSV file\n",
        "# Replace 'your_dataset.csv' with the path to your CSV file\n",
        "df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Display the first few rows of the dataset to understand its structure\n",
        "print(df.head())\n",
        "\n",
        "# Assuming the last column is the target variable (y), and all others are features (X)\n",
        "X = df.iloc[:, :-1]  # Features (all columns except the last one)\n",
        "y = df.iloc[:, -1]   # Target (the last column)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "ZFz7z57RMrka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracyM"
      ],
      "metadata": {
        "id": "rJkqRwiEMxwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "\n",
        "from scipy.stats import uniform, loguniform\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Define the parameter distribution for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'C': loguniform(1e-6, 1e+6),  # Regularization strength (log scale)\n",
        "    'penalty': ['l1', 'l2'],       # Regularization types (L1 or L2)\n",
        "    'solver': ['liblinear', 'saga']  # Solvers that support L1 and L2 penalties\n",
        "}\n",
        "\n",
        "# Initialize RandomizedSearchCV with 5-fold cross-validation\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist,\n",
        "                                   n_iter=100, cv=5, random_state=42, n_jobs=-1, verbose=1)\n",
        "\n",
        "# Perform the randomized search to find the best hyperparameters\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found by RandomizedSearchCV\n",
        "print(f\"Best Parameters: {random_search.best_params_}\")\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the best model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the Best Model: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "durm-P2_M3Dd",
        "outputId": "16cd61c4-ee7b-4b7b-ff8d-0dfdfa51ccca"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
            "Best Parameters: {'C': np.float64(1.8741051434617306), 'penalty': 'l1', 'solver': 'saga'}\n",
            "Accuracy of the Best Model: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10.  Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy"
      ],
      "metadata": {
        "id": "sHflevvsNBYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "logreg = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Wrap Logistic Regression in OneVsOneClassifier\n",
        "ovo_model = OneVsOneClassifier(logreg)\n",
        "\n",
        "# Train the model on the training data\n",
        "ovo_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = ovo_model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of One-vs-One Logistic Regression Model: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeyDanr7NGJ3",
        "outputId": "9c694981-b82c-48a2-e11b-45067648222c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of One-vs-One Logistic Regression Model: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classificationM"
      ],
      "metadata": {
        "id": "z7DxDawWNNxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# For binary classification, let's consider only two classes (class 0 and class 1)\n",
        "X = X[y != 2]  # Select only the data points belonging to class 0 and class 1\n",
        "y = y[y != 2]  # Corresponding labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix using seaborn heatmap\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 0\", \"Class 1\"], yticklabels=[\"Class 0\", \"Class 1\"])\n",
        "plt.title(\"Confusion Matrix for Binary Logistic Regression\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "-2BSSRgYNT-6",
        "outputId": "148c68c0-d95a-47a9-ca72-f782180bd5ef"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-ScoreM"
      ],
      "metadata": {
        "id": "x6t5yF54NctB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# For binary classification, let's consider only two classes (class 0 and class 1)\n",
        "X = X[y != 2]  # Select only the data points belonging to class 0 and class 1\n",
        "y = y[y != 2]  # Corresponding labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Calculate Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print Precision, Recall, and F1-Score\n",
        "print(f\"Precision: {precision * 100:.2f}%\")\n",
        "print(f\"Recall: {recall * 100:.2f}%\")\n",
        "print(f\"F1-Score: {f1 * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8A_bHh-XNjdJ",
        "outputId": "f0d2e87b-92c9-40b6-ef23-850c500dc41e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 100.00%\n",
            "Precision: 100.00%\n",
            "Recall: 100.00%\n",
            "F1-Score: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance"
      ],
      "metadata": {
        "id": "bl4oDX1gNr3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "# Create an imbalanced binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2,\n",
        "                           n_redundant=10, n_classes=2, weights=[0.9, 0.1], flip_y=0, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with class weights\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the classification report and confusion matrix\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "\n",
        "# Visualize the confusion matrix using matplotlib\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(2)\n",
        "plt.xticks(tick_marks, ['Class 0', 'Class 1'])\n",
        "plt.yticks(tick_marks, ['Class 0', 'Class 1'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "\n",
        "# Annotate confusion matrix\n",
        "thresh = cm.max() / 2.\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636
        },
        "id": "27me3BbCN10I",
        "outputId": "2831f091-e3a4-4f41-99bf-98e5e68d791c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.90      0.94       180\n",
            "           1       0.51      0.95      0.67        20\n",
            "\n",
            "    accuracy                           0.91       200\n",
            "   macro avg       0.75      0.93      0.81       200\n",
            "weighted avg       0.95      0.91      0.92       200\n",
            "\n",
            "Confusion Matrix:\n",
            "[[162  18]\n",
            " [  1  19]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling"
      ],
      "metadata": {
        "id": "SMFg3untOtmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# --- Without Feature Scaling ---\n",
        "# Train the model on the original data (without scaling)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred_without_scaling = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy without feature scaling\n",
        "accuracy_without_scaling = accuracy_score(y_test, y_pred_without_scaling)\n",
        "print(f\"Accuracy without scaling: {accuracy_without_scaling * 100:.2f}%\")\n",
        "\n",
        "# --- With Feature Scaling (Standardization) ---\n",
        "# Apply standardization (z-score scaling) to the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the model on the scaled data\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred_with_scaling = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate the accuracy with feature scaling\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "print(f\"Accuracy with scaling: {accuracy_with_scaling * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaLZj_udO7Yy",
        "outputId": "a9a9038f-2de2-4223-cf4b-a2ddfd78e690"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 100.00%\n",
            "Accuracy with scaling: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#16.  Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC scoreM"
      ],
      "metadata": {
        "id": "C0chkt0OPDkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "\n",
        "# Load the Iris dataset (binary classification)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# For ROC-AUC, we need a binary classification problem. We'll only use classes 0 and 1.\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the probabilities on the test data\n",
        "y_probs = model.predict_proba(X_test)[:, 1]  # Get probabilities for class 1\n",
        "\n",
        "# Calculate the ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_probs)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Plot the ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', label='ROC Curve (area = %0.4f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line for random guessing\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR)')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "MAoSD36TPHDL",
        "outputId": "38ae4e62-45bf-4e84-d191-148a64bcee6f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 1.0000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAArMAAAIjCAYAAAAQgZNYAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAjsJJREFUeJzs3XlcVFX/B/DPzDAzgKyyiyjuiruohPsCoiYugUuWWz2apW2mlZVbT6Xlk1k9lmYpWZYLuOAuikuaueO+K+5souzMDDPn94cP83MElEHgMvB5v16+as6cufczc2bgy5l7z5UJIQSIiIiIiCyQXOoAREREREQlxWKWiIiIiCwWi1kiIiIislgsZomIiIjIYrGYJSIiIiKLxWKWiIiIiCwWi1kiIiIislgsZomIiIjIYrGYJSIiIiKLxWKWqJz4+vpi9OjRUseocrp164Zu3bpJHeOpZs6cCZlMhpSUFKmjVDgymQwzZ84slW3Fx8dDJpMhIiKiVLYHAIcOHYJKpcL169dLbZulbdiwYRgyZIjUMYjKBItZqhQiIiIgk8mM/6ysrODt7Y3Ro0fj9u3bUser0LKysvDvf/8bLVq0gK2tLRwdHdG5c2csW7YMlnK167Nnz2LmzJmIj4+XOkoBer0eS5cuRbdu3VC9enWo1Wr4+vpizJgxOHLkiNTxSsUff/yB+fPnSx3DRHlm+vjjj/Hiiy+idu3axrZu3bqZ/EyysbFBixYtMH/+fBgMhkK3c+/ePUyZMgWNGjWCtbU1qlevjpCQEGzcuLHIfaenp2PWrFlo2bIl7OzsYGNjg2bNmuGDDz7AnTt3jP0++OADREVF4cSJE8V+XlXhvUuVg0xYym8roieIiIjAmDFj8Omnn6JOnTrIzc3FP//8g4iICPj6+uL06dOwtraWNKNGo4FcLodSqZQ0x6MSExPRs2dPnDt3DsOGDUPXrl2Rm5uLqKgo7N27F0OHDsXy5cuhUCikjvpEkZGRGDx4MHbt2lVgFlar1QIAVCpVuefKycnBCy+8gK1bt6JLly4IDQ1F9erVER8fj1WrVuHixYu4ceMGatasiZkzZ2LWrFlITk6Gq6truWd9Fv369cPp06fL7I+J3NxcWFlZwcrK6pkzCSGg0WigVCpL5X0dFxeH1q1b4++//0ZgYKCxvVu3brhy5Qpmz54NAEhJScEff/yBw4cP46OPPsLnn39usp0LFy6gZ8+eSE5OxpgxY9C2bVs8ePAAy5cvR1xcHCZPnoy5c+eaPObq1asICgrCjRs3MHjwYHTq1AkqlQonT57En3/+ierVq+PixYvG/gEBAWjUqBGWLVv21OdlznuXSHKCqBJYunSpACAOHz5s0v7BBx8IAGLlypUSJZNWTk6O0Ov1Rd4fEhIi5HK5WL9+fYH7Jk+eLACIOXPmlGXEQmVmZprVf/Xq1QKA2LVrV9kEKqEJEyYIAOKbb74pcF9eXp6YO3euuHnzphBCiBkzZggAIjk5uczyGAwGkZ2dXerbff7550Xt2rVLdZt6vV7k5OSU+PFlkakwb731lqhVq5YwGAwm7V27dhVNmzY1acvJyRG1a9cW9vb2Ii8vz9iu1WpFs2bNhK2trfjnn39MHpOXlyeGDh0qAIgVK1YY23U6nWjZsqWwtbUVf/31V4FcaWlp4qOPPjJp+89//iOqVasmMjIynvq8zHnvPotnHWciIYRgMUuVQlHF7MaNGwUA8cUXX5i0nzt3ToSFhQlnZ2ehVquFv79/oQXd/fv3xTvvvCNq164tVCqV8Pb2FiNGjDApOHJzc8X06dNFvXr1hEqlEjVr1hRTpkwRubm5JtuqXbu2GDVqlBBCiMOHDwsAIiIiosA+t27dKgCIDRs2GNtu3bolxowZI9zd3YVKpRJ+fn7il19+MXncrl27BADx559/io8//ljUqFFDyGQycf/+/UJfswMHDggA4pVXXin0fp1OJxo0aCCcnZ2NBdC1a9cEADF37lwxb948UatWLWFtbS26dOkiTp06VWAbxXmd88du9+7d4vXXXxdubm7CyclJCCFEfHy8eP3110XDhg2FtbW1qF69uggPDxfXrl0r8PjH/+UXtl27dhVdu3Yt8DqtXLlSfPbZZ8Lb21uo1WrRo0cPcenSpQLP4b///a+oU6eOsLa2Fu3atRN79+4tsM3C3Lx5U1hZWYng4OAn9suXX8xeunRJjBo1Sjg6OgoHBwcxevRokZWVZdJ3yZIlonv37sLNzU2oVCrRpEkT8cMPPxTYZu3atcXzzz8vtm7dKvz9/YVarTYWJ8XdhhBCbN68WXTp0kXY2dkJe3t70bZtW7F8+XIhxMPX9/HX/tEisrifDwBiwoQJ4vfffxd+fn7CyspKrF271njfjBkzjH3T09PF22+/bfxcurm5iaCgIHH06NGnZsp/Dy9dutRk/+fOnRODBw8Wrq6uwtraWjRs2LBAMViYWrVqidGjRxdoL6yYFUKI8PBwAUDcuXPH2Pbnn38KAOLTTz8tdB8PHjwQTk5OonHjxsa2FStWCADi888/f2rGfCdOnBAAxJo1a57Yz9z37qhRowr9wyH/Pf2owsZ51apVwtnZudDXMS0tTajVavHee+8Z24r7nqKqo/jf2RBZoPyvGJ2dnY1tZ86cQceOHeHt7Y0PP/wQ1apVw6pVqzBw4EBERUVh0KBBAIDMzEx07twZ586dwyuvvII2bdogJSUF0dHRuHXrFlxdXWEwGNC/f3/s27cP48aNQ5MmTXDq1Cl88803uHjxItatW1dorrZt26Ju3bpYtWoVRo0aZXLfypUr4ezsjJCQEAAPDwV47rnnIJPJMHHiRLi5uWHLli149dVXkZ6ejnfeecfk8f/+97+hUqkwefJkaDSaIr9e37BhAwBg5MiRhd5vZWWF4cOHY9asWdi/fz+CgoKM9y1btgwZGRmYMGECcnNz8e2336JHjx44deoUPDw8zHqd873xxhtwc3PD9OnTkZWVBQA4fPgw/v77bwwbNgw1a9ZEfHw8fvzxR3Tr1g1nz56Fra0tunTpgrfeegvfffcdPvroIzRp0gQAjP8typw5cyCXyzF58mSkpaXhq6++wksvvYSDBw8a+/z444+YOHEiOnfujHfffRfx8fEYOHAgnJ2dn/r16pYtW5CXl4cRI0Y8sd/jhgwZgjp16mD27Nk4duwYfv75Z7i7u+PLL780ydW0aVP0798fVlZW2LBhA9544w0YDAZMmDDBZHsXLlzAiy++iNdeew1jx45Fo0aNzNpGREQEXnnlFTRt2hRTp06Fk5MTjh8/jq1bt2L48OH4+OOPkZaWhlu3buGbb74BANjZ2QGA2Z+P2NhYrFq1ChMnToSrqyt8fX0LfY3Gjx+PyMhITJw4EX5+frh37x727duHc+fOoU2bNk/MVJiTJ0+ic+fOUCqVGDduHHx9fXHlyhVs2LChwOEAj7p9+zZu3LiBNm3aFNnncfknoDk5ORnbnvZZdHR0xIABA/Drr7/i8uXLqF+/PqKjowHArPeXn58fbGxssH///gKfv0eV9L1bXI+Pc4MGDTBo0CCsWbMGixYtMvmZtW7dOmg0GgwbNgyA+e8pqiKkrqaJSkP+7NyOHTtEcnKyuHnzpoiMjBRubm5CrVabfB3Ws2dP0bx5c5O/4g0Gg+jQoYNo0KCBsW369OlFzmLkf6X422+/CblcXuBrvoULFwoAYv/+/ca2R2dmhRBi6tSpQqlUitTUVGObRqMRTk5OJrOlr776qvDy8hIpKSkm+xg2bJhwdHQ0zprmzzjWrVu3WF8lDxw4UAAocuZWCCHWrFkjAIjvvvtOCPH/s1o2Njbi1q1bxn4HDx4UAMS7775rbCvu65w/dp06dTL56lUIUejzyJ9RXrZsmbHtSYcZFDUz26RJE6HRaIzt3377rQBgnGHWaDTCxcVFtGvXTuh0OmO/iIgIAeCpM7PvvvuuACCOHz/+xH758mexHp8pHzRokHBxcTFpK+x1CQkJEXXr1jVpq127tgAgtm7dWqB/cbbx4MEDYW9vLwICAgp8Ffzo1+pFfaVvzucDgJDL5eLMmTMFtoPHZmYdHR3FhAkTCvR7VFGZCpuZ7dKli7C3txfXr18v8jkWZseOHQW+RcnXtWtX0bhxY5GcnCySk5PF+fPnxZQpUwQA8fzzz5v0bdWqlXB0dHzivubNmycAiOjoaCGEEK1bt37qYwrTsGFD0adPnyf2Mfe9a+7MbGHjvG3btkJfy759+5q8J815T1HVwdUMqFIJCgqCm5sbfHx8EB4ejmrVqiE6Oto4i5aamorY2FgMGTIEGRkZSElJQUpKCu7du4eQkBBcunTJuPpBVFQUWrZsWegMhkwmAwCsXr0aTZo0QePGjY3bSklJQY8ePQAAu3btKjLr0KFDodPpsGbNGmPb9u3b8eDBAwwdOhTAw5NVoqKiEBoaCiGEyT5CQkKQlpaGY8eOmWx31KhRsLGxeeprlZGRAQCwt7cvsk/+fenp6SbtAwcOhLe3t/F2+/btERAQgM2bNwMw73XON3bs2AIn5Dz6PHQ6He7du4f69evDycmpwPM215gxY0xmgDp37gzg4Uk1AHDkyBHcu3cPY8eONTnx6KWXXjKZ6S9K/mv2pNe3MOPHjze53blzZ9y7d89kDB59XdLS0pCSkoKuXbvi6tWrSEtLM3l8nTp1jLP8jyrONmJiYpCRkYEPP/ywwAmU+Z+BJzH389G1a1f4+fk9dbtOTk44ePCgydn6JZWcnIy9e/filVdeQa1atUzue9pzvHfvHgAU+X44f/483Nzc4ObmhsaNG2Pu3Lno379/gWXBMjIynvo+efyzmJ6ebvZ7Kz/r05Z/K+l7t7gKG+cePXrA1dUVK1euNLbdv38fMTExxp+HwLP9zKXKi4cZUKWyYMECNGzYEGlpaViyZAn27t0LtVptvP/y5csQQmDatGmYNm1aodtISkqCt7c3rly5grCwsCfu79KlSzh37hzc3NyK3FZRWrZsicaNG2PlypV49dVXATw8xMDV1dX4gzk5ORkPHjzATz/9hJ9++qlY+6hTp84TM+fL/0WVkZFh8pXno4oqeBs0aFCgb8OGDbFq1SoA5r3OT8qdk5OD2bNnY+nSpbh9+7bJUmGPF23merxwyS9I7t+/DwDGNUPr169v0s/KyqrIr78f5eDgAOD/X8PSyJW/zf3792PGjBk4cOAAsrOzTfqnpaXB0dHReLuo90NxtnHlyhUAQLNmzcx6DvnM/XwU97371VdfYdSoUfDx8YG/vz/69u2LkSNHom7dumZnzP/jpaTPEUCRS9j5+vpi8eLFMBgMuHLlCj7//HMkJycX+MPA3t7+qQXm459FBwcHY3Zzsz6tSC/pe7e4ChtnKysrhIWF4Y8//oBGo4FarcaaNWug0+lMitln+ZlLlReLWapU2rdvj7Zt2wJ4OHvYqVMnDB8+HBcuXICdnZ1xfcfJkycXOlsFFCxensRgMKB58+aYN29eoff7+Pg88fFDhw7F559/jpSUFNjb2yM6OhovvviicSYwP+/LL79c4NjafC1atDC5XZxZWeDhMaXr1q3DyZMn0aVLl0L7nDx5EgCKNVv2qJK8zoXlfvPNN7F06VK88847CAwMhKOjI2QyGYYNG1bkWp3FVdSyTEUVJuZq3LgxAODUqVNo1apVsR/3tFxXrlxBz5490bhxY8ybNw8+Pj5QqVTYvHkzvvnmmwKvS2Gvq7nbKClzPx/Ffe8OGTIEnTt3xtq1a7F9+3bMnTsXX375JdasWYM+ffo8c+7icnFxAfD/fwA9rlq1aibHmnfs2BFt2rTBRx99hO+++87Y3qRJE8TFxeHGjRsF/pjJ9/hnsXHjxjh+/Dhu3rz51J8zj7p//36hf4w+ytz3blHFsV6vL7S9qHEeNmwYFi1ahC1btmDgwIFYtWoVGjdujJYtWxr7POvPXKqcWMxSpaVQKDB79mx0794d//3vf/Hhhx8aZ26USqXJL5nC1KtXD6dPn35qnxMnTqBnz57F+tr1cUOHDsWsWbMQFRUFDw8PpKenG090AAA3NzfY29tDr9c/Na+5+vXrh9mzZ2PZsmWFFrN6vR5//PEHnJ2d0bFjR5P7Ll26VKD/xYsXjTOW5rzOTxIZGYlRo0bh66+/Nrbl5ubiwYMHJv1K8to/Tf4C+JcvX0b37t2N7Xl5eYiPjy/wR8Tj+vTpA4VCgd9//71UT6TZsGEDNBoNoqOjTQofc75eLe426tWrBwA4ffr0E//IK+r1f9bPx5N4eXnhjTfewBtvvIGkpCS0adMGn3/+ubGYLe7+8t+rT/usFya/6Lt27Vqx+rdo0QIvv/wyFi1ahMmTJxtf+379+uHPP//EsmXL8MknnxR4XHp6OtavX4/GjRsbxyE0NBR//vknfv/9d0ydOrVY+8/Ly8PNmzfRv3//J/Yz973r7Oxc4DMJwOwronXp0gVeXl5YuXIlOnXqhNjYWHz88ccmfcryPUWWi8fMUqXWrVs3tG/fHvPnz0dubi7c3d3RrVs3LFq0CHfv3i3QPzk52fj/YWFhOHHiBNauXVugX/4s2ZAhQ3D79m0sXry4QJ+cnBzjWflFadKkCZo3b46VK1di5cqV8PLyMiksFQoFwsLCEBUVVegv20fzmqtDhw4ICgrC0qVLC73C0Mcff4yLFy/i/fffLzCTsm7dOpNjXg8dOoSDBw8aCwlzXucnUSgUBWZKv//++wIzPtWqVQOAQn+hllTbtm3h4uKCxYsXIy8vz9i+fPnyImfiHuXj44OxY8di+/bt+P777wvcbzAY8PXXX+PWrVtm5cqfuX38kIulS5eW+jZ69eoFe3t7zJ49G7m5uSb3PfrYatWqFXrYx7N+Pgqj1+sL7Mvd3R01atSARqN5aqbHubm5oUuXLliyZAlu3Lhhct/TZum9vb3h4+Nj1tWw3n//feh0OpOZxfDwcPj5+WHOnDkFtmUwGPD666/j/v37mDFjhsljmjdvjs8//xwHDhwosJ+MjIwCheDZs2eRm5uLDh06PDGjue/devXqIS0tzTh7DAB3794t9Gfnk8jlcoSHh2PDhg347bffkJeXZ3KIAVA27ymyfJyZpUpvypQpGDx4MCIiIjB+/HgsWLAAnTp1QvPmzTF27FjUrVsXiYmJOHDgAG7dumW83OOUKVOMV5Z65ZVX4O/vj9TUVERHR2PhwoVo2bIlRowYgVWrVmH8+PHYtWsXOnbsCL1ej/Pnz2PVqlXYtm2b8bCHogwdOhTTp0+HtbU1Xn31Vcjlpn9jzpkzB7t27UJAQADGjh0LPz8/pKam4tixY9ixYwdSU1NL/NosW7YMPXv2xIABAzB8+HB07twZGo0Ga9aswe7duzF06FBMmTKlwOPq16+PTp064fXXX4dGo8H8+fPh4uKC999/39inuK/zk/Tr1w+//fYbHB0d4efnhwMHDmDHjh3Gr3fztWrVCgqFAl9++SXS0tKgVqvRo0cPuLu7l/i1UalUmDlzJt5880306NEDQ4YMQXx8PCIiIlCvXr1izQp9/fXXuHLlCt566y2sWbMG/fr1g7OzM27cuIHVq1fj/PnzJjPxxdGrVy+oVCqEhobitddeQ2ZmJhYvXgx3d/dC/3B4lm04ODjgm2++wb/+9S+0a9cOw4cPh7OzM06cOIHs7Gz8+uuvAAB/f3+sXLkSkyZNQrt27WBnZ4fQ0NBS+Xw8LiMjAzVr1kR4eLjxEq47duzA4cOHTWbwi8pUmO+++w6dOnVCmzZtMG7cONSpUwfx8fHYtGkT4uLinphnwIABWLt2bbGORQUeHibQt29f/Pzzz5g2bRpcXFygUqkQGRmJnj17olOnTiZXAPvjjz9w7NgxvPfeeybvFaVSiTVr1iAoKAhdunTBkCFD0LFjRyiVSpw5c8b4rcqjS4vFxMTA1tYWwcHBT81pznt32LBh+OCDDzBo0CC89dZbyM7Oxo8//oiGDRuafaLm0KFD8f3332PGjBlo3rx5gSX2yuI9RZVA+S+gQFT6irpoghAPrzBTr149Ua9ePePST1euXBEjR44Unp6eQqlUCm9vb9GvXz8RGRlp8th79+6JiRMnCm9vb+Pi3KNGjTJZJkur1Yovv/xSNG3aVKjVauHs7Cz8/f3FrFmzRFpamrHf40tz5bt06ZJxYfd9+/YV+vwSExPFhAkThI+Pj1AqlcLT01P07NlT/PTTT8Y++UtOrV692qzXLiMjQ8ycOVM0bdpU2NjYCHt7e9GxY0cRERFRYGmiRy+a8PXXXwsfHx+hVqtF586dxYkTJwpsuziv85PG7v79+2LMmDHC1dVV2NnZiZCQEHH+/PlCX8vFixeLunXrCoVCUayLJjz+OhW1mP53330nateuLdRqtWjfvr3Yv3+/8Pf3F7179y7Gq/vwakk///yz6Ny5s3B0dBRKpVLUrl1bjBkzxmTpo6KuAJb/+jx6oYjo6GjRokULYW1tLXx9fcWXX34plixZUqBf/kUTClPcbeT37dChg7CxsREODg6iffv24s8//zTen5mZKYYPHy6cnJwKXDShuJ8P/G8x/cLgkaW5NBqNmDJlimjZsqWwt7cX1apVEy1btixwwYeiMhU1zqdPnxaDBg0STk5OwtraWjRq1EhMmzat0DyPOnbsmABQYKmooi6aIIQQu3fvLrDcmBBCJCUliUmTJon69esLtVotnJycRFBQkHE5rsLcv39fTJ8+XTRv3lzY2toKa2tr0axZMzF16lRx9+5dk74BAQHi5Zdffupzylfc964QQmzfvl00a9ZMqFQq0ahRI/H7778/8aIJRTEYDMLHx0cAEJ999lmhfYr7nqKqQyZEKZ3tQESVXnx8POrUqYO5c+di8uTJUseRhMFggJubG1544YVCv+qkqqdnz56oUaMGfvvtN6mjFCkuLg5t2rTBsWPHzDohkcgS8JhZIqIi5ObmFjhuctmyZUhNTUW3bt2kCUUVzhdffIGVK1eafcJTeZozZw7Cw8NZyFKlxGNmiYiK8M8//+Ddd9/F4MGD4eLigmPHjuGXX35Bs2bNMHjwYKnjUQUREBAArVYrdYwnWrFihdQRiMoMi1kioiL4+vrCx8cH3333HVJTU1G9enWMHDkSc+bMMbl6GBERSYfHzBIRERGRxeIxs0RERERksVjMEhEREZHFqnLHzBoMBty5cwf29va8FB4RERFRBSSEQEZGBmrUqFHgYkKPq3LF7J07d+Dj4yN1DCIiIiJ6ips3b6JmzZpP7FPlill7e3sAD18cBweHMt+fTqfD9u3b0atXLyiVyjLfH5U+jqHl4xhaPo6hZeP4Wb7yHsP09HT4+PgY67YnqXLFbP6hBQ4ODuVWzNra2sLBwYEfYAvFMbR8HEPLxzG0bBw/yyfVGBbnkFCeAEZEREREFovFLBERERFZLBazRERERGSxWMwSERERkcViMUtEREREFovFLBERERFZLBazRERERGSxWMwSERERkcViMUtEREREFovFLBERERFZLBazRERERGSxWMwSERERkcViMUtEREREFovFLBERERFZLEmL2b179yI0NBQ1atSATCbDunXrnvqY3bt3o02bNlCr1ahfvz4iIiLKPCcRERERVUySFrNZWVlo2bIlFixYUKz+165dw/PPP4/u3bsjLi4O77zzDv71r39h27ZtZZyUiIiIiCoiKyl33qdPH/Tp06fY/RcuXIg6derg66+/BgA0adIE+/btwzfffIOQkJCyivlMhABycxXIygKUSqnTUEnodBxDS8cxtHwcQ8vG8bN8Go0BubkKCCF1koIkLWbNdeDAAQQFBZm0hYSE4J133inyMRqNBhqNxng7PT0dAKDT6aDT6cokZz4hgK5d5fjnn35luh8qa0oAHEPLxjG0fBxDy8bxs1wCbdocx3PP/YMlS15Bjx46ODmV/V7NqdEsqphNSEiAh4eHSZuHhwfS09ORk5MDGxubAo+ZPXs2Zs2aVaB9+/btsLW1LbOswMO/QlnIEhERkSVSqTQIDd2I5s1PAwDatj2M2Ng0WFvry3zf2dnZxe5rUcVsSUydOhWTJk0y3k5PT4ePjw969eoFBweHMt13Vtb///+1a9lwcuJ3K5ZIp9MhNjYWPXr0gJLfj1kkjqHl4xhaNo6f5UlOTsTmzWvx4EEqZDIZAgI6o1GjB+jXrydUqrIfw/xv0ovDoopZT09PJCYmmrQlJibCwcGh0FlZAFCr1VCr1QXalUplmX+gHt28k5OSxayF0ukAa2s9nJzK/j1DZYNjaPk4hpaN42c5hBA4cuQItm3bBr1eDwcHB4SHh8PT0xObN2+GSlU+Y2jOPiyqmA0MDMTmzZtN2mJiYhAYGChRIiIiIqLKIzU1FVu3boXBYEDDhg0xYMAA2Nralvl5Rs9C0mI2MzMTly9fNt6+du0a4uLiUL16ddSqVQtTp07F7du3sWzZMgDA+PHj8d///hfvv/8+XnnlFcTGxmLVqlXYtGmTVE+BiIiIqNJwcXFBSEgI9Ho9nnvuOchkMqkjPZWkxeyRI0fQvXt34+38Y1tHjRqFiIgI3L17Fzdu3DDeX6dOHWzatAnvvvsuvv32W9SsWRM///xzhV2Wi4iIiKgiE0Lg0KFDqF27Njw9PQEA7du3lziVeSQtZrt16wbxhAXLCru6V7du3XD8+PEyTEVERERU+eXk5CA6Ohrnz59H9erV8dprr0GlUkkdy2wWdcwsERERET27W7duITIyEmlpaVAoFAgICLDYk/NYzBIRERFVEUIIHDhwADt37oTBYICzszPCw8NRo0YNqaOVGItZIiIioipAq9UiKioKFy9eBAA0bdoUoaGhhS5haklYzBIRERFVAUqlEnl5eVAoFOjduzf8/f0tYrWCp2ExS0RERFRJCSGg1+thZWUFmUyGQYMGITMz07hyQWXAYpaIiIioEsrKysLatWvh6OiI0NBQAICdnR3s7OwkTla6WMwSERERVTLx8fGIiopCZmYmrKys0KlTJzg7O0sdq0ywmCUiIiKqJAwGA/766y/s2bMHQgi4urpi8ODBlbaQBVjMEhEREVUKmZmZWLNmDa5duwYAaNWqFfr06WORF0IwB4tZIiIiIgsnhMCyZcuQnJwMpVKJ559/Hi1btpQ6VrlgMUtERERk4WQyGYKCghAbG4vw8HC4urpKHancsJglIiIiskAZGRlITU1F7dq1AQANGzZE/fr1IZfLJU5WvljMEhEREVmYy5cvY+3atTAYDHjttdfg5OQEAFWukAVYzBIRERFZDIPBgNjYWOzfvx8A4OnpCYPBIHEqabGYJSIiIrIAaWlpiIqKws2bNwEAbdu2RUhICKysqnY5V7WfPREREZEFuHjxItatW4ecnByo1WqEhoaiadOmUseqEFjMEhEREVVwly5dQk5ODmrUqIHw8PBKfREEc7GYJSIiIqrgQkJC4OTkhICAgCp/WMHjqt4pb0REREQV3Pnz57Fq1SrjyV1WVlbo2LEjC9lC8BUhIiIiqiDy8vIQExODQ4cOAQCOHz8Of39/iVNVbCxmiYiIiCqA1NRUREZG4u7duwCAwMBAtGrVStpQFoDFLBEREZHEzpw5gw0bNkCj0cDGxgYDBw5Ew4YNpY5lEVjMEhEREUnor7/+QmxsLADAx8cHYWFhcHR0lDiV5eAJYEREREQSatiwIZRKJTp16oTRo0ezkDUTZ2aJiIiIytm9e/fg4uICAPDw8MCbb74Je3t7iVNZJs7MEhEREZUTnU6HDRs24IcffsCtW7eM7SxkS44zs0RERETlIDk5GZGRkUhKSgIA3L59GzVr1pQ4leVjMUtERERUxuLi4rB582bodDpUq1YNL7zwAurWrSt1rEqBxSwRERFRGdFqtdi8eTNOnDgBAKhTpw5eeOEF2NnZSZys8mAxS0RERFRGTp8+jRMnTkAmk6Fbt27o1KkT5HKeslSaWMwSERERlZHWrVvj9u3baN68OXx9faWOUynxTwMiIiKiUqLRaBATEwONRgMAkMlkCA0NZSFbhjgzS0RERFQKEhISEBkZiXv37iErKwsDBw6UOlKVwGKWiIiI6BkIIXD06FFs3boVer0eDg4OaNOmjdSxqgwWs0REREQllJubi40bN+LMmTMAHl6adsCAAbC1tZU4WdXBYpaIiIioBJKSkrBixQrcv38fcrkcQUFBeO655yCTyaSOVqWwmCUiIiIqAVtbW2i1Wjg6OiI8PJxX85IIi1kiIiKiYtLpdFAqlQAAOzs7vPTSS3BycoKNjY3EyaouLs1FREREVAy3bt3CggULcPr0aWObl5cXC1mJsZglIiIiegIhBA4cOIClS5ciLS0N+/fvhxBC6lj0PzzMgIiIiKgI2dnZWL9+PS5evAgA8PPzQ2hoKE/yqkBYzBIREREV4ubNm4iMjER6ejoUCgV69+4Nf39/FrIVDItZIiIiosfcv38fERERMBgMqF69OgYPHgxPT0+pY1EhWMwSERERPcbZ2RkBAQHIzMzE888/D7VaLXUkKgKLWSIiIiIA8fHxcHZ2hqOjIwAgKCgIMpmMhxVUcFzNgIiIiKo0g8GAPXv2YNmyZYiMjIRerwcAyOVyFrIWgDOzREREVGVlZmZizZo1uHbtGgDAxcUFBoMBCoVC4mRUXCxmiYiIqEq6du0aoqKikJWVBaVSib59+6JVq1ZSxyIzsZglIiKiKiX/sIK9e/cCANzd3REeHg43NzeJk1FJsJglIiKiKsVgMODChQsAgNatW6NPnz5QKpUSp6KSYjFLREREVYqVlRXCw8Nx9+5dNG/eXOo49IxYzBIREVGlZjAYEBsbC5VKhS5dugAAXF1d4erqKnEyKg0sZomIiKjSSktLQ1RUFG7evAmZTIamTZvCxcVF6lhUiljMEhERUaV08eJFrFu3Djk5OVCr1QgNDWUhWwmxmCUiIqJKRa/XY+fOnThw4AAAwMvLC+Hh4ahevbrEyagssJglIiKiSkMIgd9//x3x8fEAgPbt2yM4OBhWVix5KiuOLBEREVUa+cfFJiQkoH///mjSpInUkaiMsZglIiIii5aXl4f09HTjYQT+/v5o3Lgx7OzsJE5G5UEudQAiIiKikrp//z6WLFmCZcuWIScnB8DD2VkWslUHZ2aJiIjIIp09exbR0dHQaDSwsbHBvXv3ULNmTaljUTljMUtEREQWJS8vD9u2bcORI0cAAD4+PggLC4Ojo6PEyUgKLGaJiIjIYty7dw+RkZFISEgAAHTs2BHdu3eHQqGQOBlJhcUsERERWYzdu3cjISEBtra2GDRoEOrXry91JJIYi1kiIiKyGH369AEABAcHw8HBQeI0VBFwNQMiIiKqsJKTk7Fr1y4IIQAAtra2CAsLYyFLRpyZJSIiogrpxIkT2LRpE3Q6HapXr46WLVtKHYkqIBazREREVKFotVps2bIFcXFxAIA6deqgXr160oaiCovFLBEREVUYSUlJWL16NVJSUiCTydC1a1d07twZcjmPjKTCsZglIiKiCuHUqVOIjo5GXl4e7OzsEBYWBl9fX6ljUQXHYpaIiIgqhGrVqiEvLw/16tXDoEGDUK1aNakjkQVgMUtERESS0Wq1UKlUAIC6deti9OjRqFWrFmQymcTJyFLwABQiIiIqd0IIHDlyBN9++y1SU1ON7bVr12YhS2ZhMUtERETlSqPRICoqCps2bUJ2djaOHDkidSSyYJIXswsWLICvry+sra0REBCAQ4cOPbH//Pnz0ahRI9jY2MDHxwfvvvsucnNzyyktERERPYs7d+5g0aJFOHPmDORyOYKDgxEcHCx1LLJgkh4zu3LlSkyaNAkLFy5EQEAA5s+fj5CQEFy4cAHu7u4F+v/xxx/48MMPsWTJEnTo0AEXL17E6NGjIZPJMG/ePAmeARERERWHEAKHDx9GbGws9Ho9HB0dER4ejpo1a0odjSycpDOz8+bNw9ixYzFmzBj4+flh4cKFsLW1xZIlSwrt//fff6Njx44YPnw4fH190atXL7z44otPnc0lIiIiaaWmpiImJgZ6vR6NGzfGa6+9xkKWSoVkM7NarRZHjx7F1KlTjW1yuRxBQUE4cOBAoY/p0KEDfv/9dxw6dAjt27fH1atXsXnzZowYMaLI/Wg0Gmg0GuPt9PR0AIBOp4NOpyulZ1O4h5tXPrK/Mt0dlZH890lZv1+o7HAMLR/H0LLpdDo4OzvDYDDAz88Pbdu2hUwm43hakPL+DJqzH8mK2ZSUFOj1enh4eJi0e3h44Pz584U+Zvjw4UhJSUGnTp0ghEBeXh7Gjx+Pjz76qMj9zJ49G7NmzSrQvn37dtja2j7bk3iK3FwFgH4AgNjYWFhb68t0f1S2YmJipI5Az4hjaPk4hpZDCIH79+/D2dkZMpkMcrkcrq6uSE5OxpYtW6SORyVUXp/B7OzsYve1qHVmd+/ejS+++AI//PADAgICcPnyZbz99tv497//jWnTphX6mKlTp2LSpEnG2+np6fDx8UGvXr3g4OBQpnmzsv7//3v06AEnJ2WZ7o/Khk6nQ0xMDIKDg6FUcgwtEcfQ8nEMLUtOTg42btyIGzduwNvbG506dUJMTAx69erF8bNQ5f0ZzP8mvTgkK2ZdXV2hUCiQmJho0p6YmAhPT89CHzNt2jSMGDEC//rXvwAAzZs3R1ZWFsaNG4ePP/640Os2q9VqqNXqAu1KpbLMB+PRzZfH/qhscQwtH8fQ8nEMK76bN28iMjIS6enpUCgUcHZ2No4Zx8/yldcYmrMPyU4AU6lU8Pf3x86dO41tBoMBO3fuRGBgYKGPyc7OLlCwKhQKAA+/ziAiIiJpCCGwb98+LF26FOnp6ahevTr+9a9/oV27dlJHo0pO0sMMJk2ahFGjRqFt27Zo37495s+fj6ysLIwZMwYAMHLkSHh7e2P27NkAgNDQUMybNw+tW7c2HmYwbdo0hIaGGotaIiIiKl9ZWVlYt24dLl++DABo1qwZ+vXrV+g3o0SlTdJidujQoUhOTsb06dORkJCAVq1aYevWrcaTwm7cuGEyE/vJJ59AJpPhk08+we3bt+Hm5obQ0FB8/vnnUj0FIiKiKi8nJwfXr1+HlZUV+vTpg9atW/OStFRuJD8BbOLEiZg4cWKh9+3evdvktpWVFWbMmIEZM2aUQzIiIiIqDldXV7zwwgtwdnYusEoRUVmT/HK2REREZFkyMzPx+++/4/r168a2xo0bs5AlSbCYJSIiomK7evUqFi5ciCtXriA6OhoGg0HqSFTFSX6YAREREVV8BoMBe/bswd69ewEAbm5uGDx4cKHLYhKVJxazRERE9EQZGRlYs2YN4uPjAQCtW7dGnz59uGYsVQgsZomIiKhIaWlp+Omnn5CdnQ2lUol+/fqhRYsWUsciMmIxS0REREVycHBAnTp1kJKSgsGDB8PFxUXqSEQmWMwSERGRifT0dKhUKlhbW0MmkyE0NBRyuZyHFVCFxKO2iYiIyOjixYtYuHAhoqOjjZeKV6vVLGSpwuLMLBEREUGv12Pnzp04cOAAAODBgwfQaDSwtraWOBnRk7GYJSIiquIePHiAqKgo3Lp1CwDQvn17BAcHw8qKZQJVfHyXEhERVWHnz5/H+vXrkZubC7VajQEDBqBJkyZSxyIqNhazREREVZROp8OWLVuQm5sLb29vhIWFwdnZWepYRGZhMUtERFRFKZVKhIWF4fz58+jZsycUCoXUkYjMxmKWiIioCjl79izy8vKMFz6oVasWatWqJXEqopJjMUtERFQF5OXlYdu2bThy5AisrKzg7e3NCyBQpcBiloiIqJK7d+8eIiMjkZCQAAAICAiAk5OTtKGISgmLWSIiokrs9OnT2LBhA7RaLWxtbTFw4EA0aNBA6lhEpYbFLBERUSUkhMCmTZtw9OhRAA+PjQ0LC4ODg4PEyYhKF4tZIiKiSkgmk8HW1hYA0LlzZ3Tr1g1yOa9iT5UPi1kiIqJKRKvVQqVSAQC6deuGBg0awMfHR+JURGWHf6IRERFVAlqtFuvXr0dERATy8vIAAHK5nIUsVXqcmSUiIrJwSUlJiIyMRHJyMmQyGeLj41G/fn2pYxGVCxazREREFkoIgbi4OGzevBl5eXmws7NDWFgYfH19pY5GVG5YzBIREVkgjUaDTZs24dSpUwCAevXqYdCgQahWrZrEyYjKF4tZIiIiC7Rx40acPn0aMpkM3bt3R6dOnSCTyaSORVTuWMwSERFZoB49eiAxMRH9+vVDrVq1pI5DJBmuZkBERGQBNBoNzpw5Y7zt7OyM119/nYUsVXmcmSUiIqrg7t69i9WrV+P+/ftQq9XGlQp4WAERi1kiIqIKSwiBw4cPY/v27dDr9XB0dIS1tbXUsYgqFBazREREFVBubi6io6Nx7tw5AECjRo0wYMAA2NjYSJyMqGJhMUtERFTB3L59G5GRkXjw4AHkcjmCg4MREBDAwwqICsFiloiIqIJJSUnBgwcP4OTkhPDwcHh7e0sdiajCYjFLRERUAQghjDOvLVu2hFarRfPmzXmMLNFTcGkuIiIiid28eRNLlixBdna2sa1du3YsZImKgcUsERGRRIQQ2L9/P5YuXYpbt24hNjZW6khEFoeHGRAREUkgKysL69atw+XLlwEAzZo1Q3BwsMSpiCwPi1kiIqJydv36dURFRSEjIwNWVlbo3bs32rRpw9UKiEqAxSwREVE5On/+PFatWgUhBFxcXDB48GB4eHhIHYvIYpW4mL1x4wauX7+O7OxsuLm5oWnTplCr1aWZjYiIqNLx9fWFk5MTfHx88Pzzz0OlUkkdiciimVXMxsfH48cff8SKFStw69YtCCGM96lUKnTu3Bnjxo1DWFgY5HKeW0ZERAQAiYmJcHd3h0wmg7W1Nf71r3/BxsaGhxUQlYJiV5xvvfUWWrZsiWvXruGzzz7D2bNnkZaWBq1Wi4SEBGzevBmdOnXC9OnT0aJFCxw+fLgscxMREVV4BoMBu3fvxsKFC3HkyBFju62tLQtZolJS7JnZatWq4erVq3BxcSlwn7u7O3r06IEePXpgxowZ2Lp1K27evIl27dqValgiIiJLkZGRgTVr1iA+Ph4AkJSUJG0gokqq2MXs7Nmzi73R3r17lygMERFRZXDlyhWsXbsWWVlZUCqV6NevH1q0aCF1LKJKqVQPbM3NzcV//vOf0twkERGRxTAYDIiNjcXvv/+OrKwseHh4YNy4cSxkicqQ2cVscnIyNm7ciO3bt0Ov1wMAdDodvv32W/j6+mLOnDmlHpKIiMgSJCYmYt++fQAAf39/vPrqq3B1dZU4FVHlZtZqBvv27UO/fv2Qnp4OmUyGtm3bYunSpRg4cCCsrKwwc+ZMjBo1qqyyEhERVWheXl4IDg6Gvb09mjVrJnUcoirBrJnZTz75BH379sXJkycxadIkHD58GIMGDcIXX3yBs2fPYvz48bCxsSmrrERERBWKXq/Hzp07kZycbGwLDAxkIUtUjswqZk+dOoVPPvkEzZo1w6effgqZTIavvvoK4eHhZZWPiIioQkpLS0NERAT27duHyMhI46F3RFS+zDrM4P79+8Zjf2xsbGBra8u/PomIqMq5cOEC1q1bh9zcXKjVanTt2hUKhULqWERVktmXsz179iwSEhIAAEIIXLhwAVlZWSZ9eNYmERFVRnq9HjExMTh48CAAoEaNGggPD4ezs7PEyYiqLrOL2Z49e5pcxrZfv34AAJlMBiEEZDIZv2ohIqJKJysrC3/88Qfu3LkDAHjuuecQFBTEGVkiiZlVzF67dq2schAREVVoNjY2sLKygrW1NQYOHIhGjRpJHYmIYGYxW7t27bLKQUREVOHk5eVBJpNBoVBALpcjLCwMBoMBTk5OUkcjov8xazWDrKwsvP766/D29oabmxuGDRtmshwJERFRZZGamopffvkFMTExxjYHBwcWskQVjFnF7LRp0/Dbb7+hX79+GD58OGJjYzFu3LiyykZERCSJ06dPY9GiRUhISMCpU6eQnZ0tdSQiKoJZhxmsXbsWS5cuxeDBgwEAI0eOxHPPPYe8vDxYWZl9LhkREVGFotPpsHXrVhw7dgwAUKtWLYSFhcHW1lbiZERUFLMq0Fu3bqFjx47G2/7+/lAqlbhz5w5q1apV6uGIiIjKS0pKClavXo2kpCQAQOfOndGtWzfI5WZ9iUlE5cysYtZgMECpVJpuwMqKS3EREZFFy8vLw7Jly5CRkYFq1aph0KBBqFevntSxiKgYzCpmhRDo2bOnySEF2dnZCA0NhUqlMrblfz1DRERkCaysrBASEoIjR47ghRdegL29vdSRiKiYzCpmZ8yYUaBtwIABpRaGiIiovCQlJSEnJ8e47GTTpk3h5+cHmUwmcTIiModZxeyYMWNQs2ZNHj9EREQWSwiBuLg4bN68GSqVCuPHjzfOxLKQJbI8ZhWzderUwd27d+Hu7l5WeYiIiMqMVqvFpk2bcPLkSQAPVyvgBA2RZTP7mFkiIiJLlJiYiNWrV+PevXuQyWTo3r07OnXqxNlYIgtn9uKw/NATEZElEULg2LFj2Lp1K/Ly8mBvb4+wsDBeop2okjC7mJ02bdpTF4+eN29eiQMRERGVJplMhps3byIvLw/169fHoEGDeBEEokrE7GL21KlTJstwPY4zt0REVBEIIYy/k/r27YuaNWvC39+fv6eIKhmzi9m1a9fyBDAiIqqwhBA4fPgw4uPjMXjwYMhkMqhUKrRt21bqaERUBswqZvnXLBERVWS5ubnYsGEDzp49CwA4d+4c/Pz8JE5FRGWJqxkQEVGlcPv2bURGRuLBgweQy+UIDg5GkyZNpI5FRGXMrGJ26dKlcHR0LKssREREZhNC4ODBg4iJiYHBYICTkxPCw8Ph7e0tdTQiKgfFLmb/+ecfjBo1qlh9s7Ozce3aNTRt2rTEwYiIiIpjy5YtOHz4MACgSZMm6N+/P6ytrSVORUTlpdiXPRkxYgRCQkKwevVqZGVlFdrn7Nmz+Oijj1CvXj0cPXq01EISEREVpWXLllCpVOjTpw8GDx7MQpaoiin2zOzZs2fx448/4pNPPsHw4cPRsGFD1KhRA9bW1rh//z7Onz+PzMxMDBo0CNu3b0fz5s3LMjcREVVRQggkJibC09MTAODt7Y133nkHNjY2EicjIikUe2ZWqVTirbfewoULF3DgwAGMHTsWzZo1g7e3N7p164ZFixbhzp07+PPPP80qZBcsWABfX19YW1sjICAAhw4demL/Bw8eYMKECfDy8oJarUbDhg2xefPmYu+PiIgsV3Z2Nv7880/8/PPPSEhIMLazkCWqusxeZxYA2rZtWyrr9a1cuRKTJk3CwoULERAQgPnz5yMkJAQXLlwodC1brVaL4OBguLu7IzIyEt7e3rh+/TqcnJyeOQsREVVsmZmZ+OWXX5CRkQGFQoGUlBTj7CwRVV0lKmZLy7x58zB27FiMGTMGALBw4UJs2rQJS5YswYcfflig/5IlS5Camoq///4bSqUSAODr61uekYmIqJwJIbB//35cvnwZAODi4oLBgwfDw8ND4mREVBFIVsxqtVocPXoUU6dONbbJ5XIEBQXhwIEDhT4mOjoagYGBmDBhAtavXw83NzcMHz4cH3zwARQKRaGP0Wg00Gg0xtvp6ekAAJ1OB51OV4rPqKCHm1c+sr8y3R2Vkfz3SVm/X6jscAwtV1ZWFqKjo3Ht2jUAgJ+fH/r27QuVSsXxtCD8DFq+8h5Dc/YjWTGbkpICvV5f4C9rDw8PnD9/vtDHXL16FbGxsXjppZewefNmXL58GW+88QZ0Oh1mzJhR6GNmz56NWbNmFWjfvn07bG1tn/2JPEFurgJAPwBAbGwsrK31Zbo/KlsxMTFSR6BnxDG0PElJSbhz5w5kMhlq1qwJpVKJHTt2SB2LSoifQctXXmOYnZ1d7L6SHmZgLoPBAHd3d/z0009QKBTw9/fH7du3MXfu3CKL2alTp2LSpEnG2+np6fDx8UGvXr3g4OBQpnkfXcGsR48ecHJSlun+qGzodDrExMQgODjYeHgLWRaOoeUSQmDbtm1o2bIljh8/zjG0UPwMWr7yHsP8b9KL45mL2dzc3BKt6efq6gqFQoHExEST9keXW3mcl5cXlEqlySEFTZo0QUJCArRaLVQqVYHHqNVqqNXqAu1KpbLMB+PRzZfH/qhscQwtH8ew4svIyMCePXsQEhJiHKvQ0FDodDocP36cY2jhOH6Wr7zG0Jx9FHtprkcZDAb8+9//hre3N+zs7HD16lUAwLRp0/DLL78UaxsqlQr+/v7YuXOnyXZ37tyJwMDAQh/TsWNHXL58GQaDwdh28eJFeHl5FVrIEhGR5bhy5QoWLVqEo0eP8utoIiq2EhWzn332GSIiIvDVV1+ZFJHNmjXDzz//XOztTJo0CYsXL8avv/6Kc+fO4fXXX0dWVpZxdYORI0eanCD2+uuvIzU1FW+//TYuXryITZs24YsvvsCECRNK8jSIiKgCMBgMiI2Nxe+//46srCy4u7ujffv2UsciIgtRosMMli1bhp9++gk9e/bE+PHjje0tW7Ys8uStwgwdOhTJycmYPn06EhIS0KpVK2zdutV4UtiNGzcgl/9/ve3j44Nt27bh3XffRYsWLeDt7Y23334bH3zwQUmeBhERSSw9PR1RUVG4ceMGAKBNmzbo3bs3v4omomIrUTF7+/Zt1K9fv0C7wWAwe8mGiRMnYuLEiYXet3v37gJtgYGB+Oeff8zaBxERVTw3btzAypUrkZ2dDZVKhdDQUDRr1kzqWERkYUpUzPr5+eGvv/5C7dq1TdojIyPRunXrUglGRESVm6OjI4QQ8PT0RHh4OFxcXKSOREQWqETF7PTp0zFq1Cjcvn0bBoMBa9aswYULF7Bs2TJs3LixtDMSEVEl8egKOI6Ojhg5ciRcXV1hZWVRK0USUQVSohPABgwYgA0bNmDHjh2oVq0apk+fjnPnzmHDhg0IDg4u7YxERFQJXLhwAd999x0uXLhgbPP09GQhS0TPpMQ/QTp37sylU4iI6Kn0ej127NhhPN/h8OHDaNSokcSpiKiyKNHMbN26dXHv3r0C7Q8ePEDdunWfORQREVUO9+/fx9KlS42FbEBAAF588UWJUxFRZVKimdn4+Hjo9foC7RqNBrdv337mUEREZPnOnTuH9evXQ6PRwNraGgMGDEDjxo2ljkVElYxZxWx0dLTx/7dt2wZHR0fjbb1ej507d8LX17fUwhERkWW6e/cuVq1aBQCoWbMmwsLC4OTkJG0oIqqUzCpmBw4cCACQyWQYNWqUyX1KpRK+vr74+uuvSy0cERFZJi8vL7Rt2xYqlQo9evSAQqGQOhIRVVJmFbMGgwEAUKdOHRw+fBiurq5lEoqIiCzP2bNnUatWLdjZ2QEA+vbtC5lMJnEqIqrsSnTM7LVr10o7BxERWSidTodt27bh6NGjqFOnDl5++WXI5XIWskRULkq8NFdWVhb27NmDGzduQKvVmtz31ltvPXMwIiKq+FJSUhAZGYnExEQAgLe3t8SJiKiqKVExe/z4cfTt2xfZ2dnIyspC9erVkZKSAltbW7i7u7OYJSKqAk6ePImNGzdCp9PB1tYWL7zwAurVqyd1LCKqYkq0zuy7776L0NBQ3L9/HzY2Nvjnn39w/fp1+Pv74z//+U9pZyQiogpEp9MhOjoaa9euhU6ng6+vL8aPH89ClogkUaJiNi4uDu+99x7kcjkUCgU0Gg18fHzw1Vdf4aOPPirtjEREVIEIIXDz5k0AQNeuXTFixAjY29tLnIqIqqoSHWagVCohlz+sg93d3XHjxg00adIEjo6Oxh9wRERUuQghIJPJoFKpEB4ejqysLF71kYgkV6JitnXr1jh8+DAaNGiArl27Yvr06UhJScFvv/2GZs2alXZGIiKSkFarxebNm+Hh4YHAwEAAgIeHh8SpiIgeKtFhBl988QW8vLwAAJ9//jmcnZ3x+uuvIzk5GYsWLSrVgEREJJ3ExEQsXrwYJ06cQGxsLDIzM6WORERkokQzs23btjX+v7u7O7Zu3VpqgYiISHpCCBw7dgxbt25FXl4e7O3tERYWZrwgAhFRRVGimdmiHDt2DP369SvNTRIRUTnTaDRYs2YNNm7ciLy8PNSvXx+vvfYaateuLXU0IqICzC5mt23bhsmTJ+Ojjz7C1atXAQDnz5/HwIED0a5dO+Mlb4mIyPLo9Xr88ssvOH36NGQyGYKCgjB8+HBUq1ZN6mhERIUy6zCDX375BWPHjkX16tVx//59/Pzzz5g3bx7efPNNDB06FKdPn0aTJk3KKisREZUxhUKB1q1b459//kF4eDh8fHykjkRE9ERmFbPffvstvvzyS0yZMgVRUVEYPHgwfvjhB5w6dQo1a9Ysq4xERFSGcnNzkZWVBRcXFwDAc889h9atW8Pa2lriZERET2dWMXvlyhUMHjwYAPDCCy/AysoKc+fOZSFLRGSh7ty5g9WrV0OhUGDs2LFQq9WQyWQsZInIYphVzObk5MDW1hYAIJPJoFarjUt0ERGR5RBC4ODBg4iJiYHBYICTkxMyMjKgVquljkZEZBazl+b6+eefjUuz5OXlISIiAq6uriZ93nrrrdJJR0REpS4nJwfR0dE4f/48AKBx48YYMGAAZ2OJyCKZVczWqlULixcvNt729PTEb7/9ZtJHJpOxmCUiqqBu3bqFyMhIpKWlQaFQoFevXmjXrh1kMpnU0YiISsSsYjY+Pr6MYhARUXnYs2cP0tLS4OzsjPDwcNSoUUPqSEREz6REVwAjIiLLNGDAAOzevRvBwcE8PpaIKoVSvQIYERFVLDdu3MCuXbuMt+3s7NCvXz8WskRUaXBmloioEhJCYN++fdi1axeEEPDy8kLjxo2ljkVEVOpYzBIRVTJZWVlYu3Ytrly5AgBo0aIF6tatK3EqIqKywWKWiKgSiY+PR1RUFDIzM2FlZYW+ffuiVatWXK2AiCqtEhezV65cwdKlS3HlyhV8++23cHd3x5YtW1CrVi00bdq0NDMSEVExHDhwADExMRBCwNXVFYMHD4a7u7vUsYiIylSJTgDbs2cPmjdvjoMHD2LNmjXIzMwEAJw4cQIzZswo1YBERFQ81atXhxACrVq1wtixY1nIElGVUKJi9sMPP8Rnn32GmJgYqFQqY3uPHj3wzz//lFo4IiJ6stzcXOP/N2rUCGPHjsWAAQNMfjYTEVVmJSpmT506hUGDBhVod3d3R0pKyjOHIiKiJzMYDIiNjcX333+PtLQ0YzsvgkBEVU2JilknJyfcvXu3QPvx48fh7e39zKGIiKho6enpWLZsGf766y9kZ2fj7NmzUkciIpJMiU4AGzZsGD744AOsXr0aMpkMBoMB+/fvx+TJkzFy5MjSzkhERP9z+fJlrF27FtnZ2VCpVAgNDUWzZs2kjkVEJJkSFbNffPEFJkyYAB8fH+j1evj5+UGv12P48OH45JNPSjsjEVGVp9frsWvXLuzfvx8A4OnpifDwcLi4uEicjIhIWiUqZlUqFRYvXoxp06bh9OnTyMzMROvWrdGgQYPSzkdERAAOHjxoLGTbtWuHXr16wcqKS4UTEZXoJ+G+ffvQqVMn1KpVC7Vq1SrtTERE9Jh27drhwoULCAgIgJ+fn9RxiIgqjBKdANajRw/UqVMHH330EU88ICIqA3q9HkeOHIHBYAAAKJVKjB49moUsEdFjSlTM3rlzB++99x727NmDZs2aoVWrVpg7dy5u3bpV2vmIiKqcBw8eYOnSpdi0aRP++usvYzsvSUtEVFCJillXV1dMnDgR+/fvx5UrVzB48GD8+uuv8PX1RY8ePUo7IxFRlXHu3DksWrQIt2/fhrW1NTw8PKSORERUoT3z2QN16tTBhx9+iJYtW2LatGnYs2dPaeQiIqpS8vLyEBMTg0OHDgEAatasibCwMDg5OUkbjIiognumYnb//v1Yvnw5IiMjkZubiwEDBmD27NmllY2IqEpITU1FZGSk8WI0gYGB6NmzJxQKhcTJiIgqvhIVs1OnTsWKFStw584dBAcH49tvv8WAAQNga2tb2vmIiCo9rVaLpKQk2NjYYODAgWjYsKHUkYiILEaJitm9e/diypQpGDJkCFxdXUs7ExFRpSeEMJ7QlX8BBC8vLzg6OkqcjIjIspSomM1fuJuIiMx37949rFmzBn379oW3tzcAoHHjxhKnIiKyTMUuZqOjo9GnTx8olUpER0c/sW///v2fORgRUWV06tQpbNy4EVqtFlu2bMGrr77KJbeIiJ5BsYvZgQMHIiEhAe7u7hg4cGCR/WQyGfR6fWlkIyKqNHQ6HbZs2YLjx48DAHx9ffHCCy+wkCUiekbFLmbzr0Lz+P8TEdGTJScnIzIyEklJSQCArl27okuXLpDLS7TUNxERPaJEP0mXLVsGjUZToF2r1WLZsmXPHIqIqLJISkrC4sWLkZSUhGrVqmHkyJHo1q0bC1kiolJSop+mY8aMQVpaWoH2jIwMjBkz5plDERFVFm5ubqhTpw7q1KmD8ePHo06dOlJHIiKqVEq0msGjS8o86tatW1xWhoiqvKSkJDg5OUGlUkEmkyEsLAxWVlacjSUiKgNmFbOtW7eGTCaDTCZDz549YWX1/w/X6/W4du0aevfuXeohiYgsgRACx48fx5YtW+Dn54eBAwdCJpNBpVJJHY2IqNIyq5jNX8UgLi4OISEhsLOzM96nUqng6+uLsLCwUg1IRGQJNBoNNm3ahFOnTgEAsrOzodfrTf7oJyKi0mfWT9kZM2YAeLikzNChQ2FtbV0moYiILElCQgJWr16N1NRU4zdXHTp04LJbRETloERTBqNGjSrtHEREFkcIgSNHjmDbtm3Q6/VwcHBAeHg4fHx8pI5GRFRlFLuYrV69Oi5evAhXV1c4Ozs/ccYhNTW1VMIREVVkubm52LNnD/R6PRo2bIgBAwbA1tZW6lhERFVKsYvZb775Bvb29sb/59dnRFTV2djY4IUXXkBiYiKee+45/lwkIpJAsYvZRw8tGD16dFlkISKq0IQQOHToEOzt7eHn5wcAqFu3LurWrStxMiKiqqtEx8weO3YMSqUSzZs3BwCsX78eS5cuhZ+fH2bOnMllaIio0snJyUF0dDTOnz8PlUqFmjVrwsHBQepYRERVXolW8H7ttddw8eJFAMDVq1cxdOhQ2NraYvXq1Xj//fdLNSARkdRu3bqFRYsW4fz581AoFOjZs6fxsCsiIpJWiWZmL168iFatWgEAVq9eja5du+KPP/7A/v37MWzYMMyfP78UIxIRSUMIgQMHDmDnzp0wGAxwdnZGeHg4atSoIXU0IiL6nxJfztZgMAAAduzYgX79+gEAfHx8kJKSUnrpiIgkYjAYsHLlSuO3UE2bNkVoaCjUarXEyYiI6FElKmbbtm2Lzz77DEFBQdizZw9+/PFHAMC1a9fg4eFRqgGJiKQgl8tRvXp1KBQK9O7dG/7+/lytgIioAipRMTt//ny89NJLWLduHT7++GPUr18fABAZGYkOHTqUakAiovIihIBGozFe3TAoKAht2rSBm5ubxMmIiKgoJSpmW7RoYbz++KPmzp0LhULxzKGIiMpbVlYW1q1bB41Gg1GjRkGhUEChULCQJSKq4EpUzOY7evQozp07BwDw8/NDmzZtSiUUEVF5io+Px5o1a5CRkQErKyskJCTA29tb6lhERFQMJSpmk5KSMHToUOzZswdOTk4AgAcPHqB79+5YsWIFZzKIyCIYDAb89ddf2LNnD4QQcHV1xeDBg+Hu7i51NCIiKqYSrTP75ptvIjMzE2fOnEFqaipSU1Nx+vRppKen46233irtjEREpS4zMxO///47du/eDSEEWrVqhbFjx7KQJSKyMCWamd26dSt27NiBJk2aGNv8/PywYMEC9OrVq9TCERGVlbVr1+LatWtQKpV4/vnn0bJlS6kjERFRCZRoZtZgMECpVBZoVyqVxvVnzbFgwQL4+vrC2toaAQEBOHToULEet2LFCshkMgwcONDsfRJR1danTx/UrFkT48aNYyFLRGTBSlTM9ujRA2+//Tbu3LljbLt9+zbeffdd9OzZ06xtrVy5EpMmTcKMGTNw7NgxtGzZEiEhIUhKSnri4+Lj4zF58mR07ty5JE+BiKoYnU6HM2fOGG+7urrilVdegaurq4SpiIjoWZWomP3vf/+L9PR0+Pr6ol69eqhXrx7q1KmD9PR0fP/992Zta968eRg7dizGjBkDPz8/LFy4ELa2tliyZEmRj9Hr9XjppZcwa9Ys1K1btyRPgYiqkKtXr+L8+fOIjo7G9evXje28CAIRkeUr0TGzPj4+OHbsGHbu3GlcmqtJkyYICgoyaztarRZHjx7F1KlTjW1yuRxBQUE4cOBAkY/79NNP4e7ujldffRV//fXXE/eh0Wig0WiMt9PT0wE8nKXR6XRm5TXXw80rH9lfme6Oykj++6Ss3y9U+gwGA/bs2WP8eeLu7g61Ws2xtED8HFo2jp/lK+8xNGc/ZhezK1euRHR0NLRaLXr27Ik333zT3E0YpaSkQK/XF7gEroeHB86fP1/oY/bt24dffvkFcXFxxdrH7NmzMWvWrALt27dvh62trdmZzZGbqwDQDwAQGxsLa2t9me6PylZMTIzUEcgMWq0W169fR1ZWFoCHhxV4enri4MGDEiejZ8HPoWXj+Fm+8hrD7OzsYvc1q5j98ccfMWHCBDRo0AA2NjZYs2YNrly5grlz55odsiQyMjIwYsQILF68uNjHuU2dOhWTJk0y3k5PT4ePjw969eoFBweHsooKAPjf71AAD48zdnIqeNIcVXw6nQ4xMTEIDg4u9MRHqnguX76MDRs2ICcnB2q1GiEhIbhx4wbH0ILxc2jZOH6Wr7zHMP+b9OIwq5j973//ixkzZmDGjBkAgN9//x2vvfZaiYtZV1dXKBQKJCYmmrQnJibC09OzQP8rV64gPj4eoaGhxrb81ROsrKxw4cIF1KtXz+QxarUaarW6wLaUSmWZD8ajmy+P/VHZ4hhajszMTOTk5MDLywvh4eGwt7fHjRs3OIaVAMfQsnH8LF95jaE5+zDrBLCrV69i1KhRxtvDhw9HXl4e7t69a85mjFQqFfz9/bFz505jm8FgwM6dOxEYGFigf+PGjXHq1CnExcUZ//Xv3x/du3dHXFwcfHx8SpSDiCyfEML4/23btsWAAQPwyiuvoHr16hKmIiKismbWzKxGo0G1atWMt+VyOVQqFXJyckocYNKkSRg1ahTatm2L9u3bY/78+cjKysKYMWMAACNHjoS3tzdmz54Na2trNGvWzOTx+ZfTfbydiKqO8+fPY+/evRg5ciSsra0hk8nQqlUrqWMREVE5MPsEsGnTppmcOKXVavH555/D0dHR2DZv3rxib2/o0KFITk7G9OnTkZCQgFatWmHr1q3Gk8Ju3LgBubxEK4gRUSWXl5eHHTt2GE/q+vvvv9GjRw+JUxERUXkyq5jt0qULLly4YNLWoUMHXL161Xi7JOs2Tpw4ERMnTiz0vt27dz/xsREREWbvj4gsX2pqKiIjI42HOQUGBqJr164SpyIiovJmVjH7tMKSiKg8nDlzBhs2bIBGo4GNjQ0GDhyIhg0bSh2LiIgkUKKLJhARSeXo0aPYuHEjgIcXcAkPDy/zZfaIiKjiYjFLRBalSZMm2Lt3L1q0aIHu3bvzmHoioiqOxSwRVXg3b940Lr1na2uLN954o9D1o4mIqOrhlAYRVVg6nQ7R0dFYsmSJySWsWcgSEVE+zswSUYWUnJyMyMhIJCUlAXh4OWsiIqLHlXhm9q+//sLLL7+MwMBA3L59GwDw22+/Yd++faUWjoiqphMnTmDx4sVISkpCtWrVMGLECHTu3FnqWEREVAGVqJiNiopCSEgIbGxscPz4cWg0GgBAWloavvjii1INSERVh1arxfr167Fu3TrodDrUrVsX48ePR926daWORkREFVSJitnPPvsMCxcuxOLFi6FUKo3tHTt2xLFjx0otHBFVLXfu3EFcXBxkMhm6d++Ol156CXZ2dlLHIiKiCqxEx8xeuHABXbp0KdDu6OiIBw8ePGsmIqqifH190atXL3h5ecHX11fqOEREZAFKNDPr6emJy5cvF2jft28fvw4komLTaDTYsGEDUlNTjW2BgYEsZImIqNhKVMyOHTsWb7/9Ng4ePAiZTIY7d+5g+fLlmDx5Ml5//fXSzkhElVBCQgIWL16MY8eOYe3atRBCSB2JiIgsUIkOM/jwww9hMBjQs2dPZGdno0uXLlCr1Zg8eTLefPPN0s5IRJWIEAJHjx7F1q1bodfr4eDggODgYMhkMqmjERGRBSpRMSuTyfDxxx9jypQpuHz5MjIzM+Hn58cTNYjoiXJzc7Fx40acOXMGANCwYUMMGDAAtra2EicjIiJL9UwXTVCpVPDz8yutLERUid2/fx+//fYb7t+/D7lcjqCgIDz33HOckSUiomdSomK2e/fuT/wFFBsbW+JARFQ5OTg4wMbGBgaDAeHh4ahZs6bUkYiIqBIoUTHbqlUrk9s6nQ5xcXE4ffo0Ro0aVRq5iKgSyM3NhUqlglwuh0KhwJAhQ6BSqWBjYyN1NCIiqiRKVMx+8803hbbPnDkTmZmZzxSIiCqH27dvIzIyEs2aNUPPnj0BPFyLmoiIqDSVaGmuorz88stYsmRJaW6SiCyMEAIHDhzAkiVL8ODBA5w9exZarVbqWEREVEk90wlgjztw4ACsra1Lc5NEZEFycnKwbt06XLx4EQDg5+eH0NBQqFQqiZMREVFlVaJi9oUXXjC5LYTA3bt3ceTIEUybNq1UghGRZbl58yYiIyORnp4OhUKB3r17w9/fn6sVEBFRmSpRMfv4cW9yuRyNGjXCp59+il69epVKMCKyHLm5uVi+fDk0Gg2qV6+OwYMHw9PTU+pYRERUBZhdzOr1eowZMwbNmzeHs7NzWWQiIgtjbW2N3r174+rVq3j++eehVquljkRERFWE2SeAKRQK9OrVCw8ePCiDOERkKa5fv46bN28ab7dq1QqDBg1iIUtEROWqRKsZNGvWDFevXi3tLERkAQwGA/bu3Ytff/0Vq1evRnZ2tvE+Hh9LRETlrUTF7GeffYbJkydj48aNuHv3LtLT003+EVHllJmZieXLl2PXrl0QQqBu3bqwsirVRVGIiIjMYtZvoU8//RTvvfce+vbtCwDo37+/yUyMEAIymQx6vb50UxKR5K5du4aoqChkZWVBqVSib9++Ba4GSEREVN7MKmZnzZqF8ePHY9euXWWVh4gqGCEEdu/ejb179wIA3N3dER4eDjc3N4mTERERmVnMCiEAAF27di2TMERUMaWkpAAAWrdujT59+kCpVEqciIiI6CGzD3bjCR5EVUP+YUMymQyhoaFo2rQp/Pz8pI5FRERkwuxitmHDhk8taFNTU0sciIikZTAYEBsbi/v37yM8PBwymQzW1tYsZImIqEIyu5idNWtWgSuAEVHlkJaWhqioKOP6sdevX4evr6+0oYiIiJ7A7GJ22LBhcHd3L4ssRCShixcvYt26dcjJyYFarUZoaCgLWSIiqvDMKmZ5vCxR5aPX67Fz504cOHAAAODl5YXw8HBUr15d4mRERERPV6LVDIio8oiKisK5c+cAAO3bt0dwcDAvhEBERBbDrN9YBoOhrHIQkUQCAgJw/fp1hIaGonHjxlLHISIiMgunX4iqmLy8PCQkJKBmzZoAgNq1a+Ptt9+GSqWSOBkREZH55FIHIKLyc//+fSxZsgTLli1DcnKysZ2FLBERWSrOzBJVEWfPnkV0dDQ0Gg1sbGyQmZnJS9ISEZHFYzFLVMnl5eVh27ZtOHLkCADAx8cHYWFhXC+aiIgqBRazRJXYvXv3EBkZiYSEBABAx44d0b17dygUComTERERlQ4Ws0SV2MmTJ5GQkABbW1sMGjQI9evXlzoSERFRqWIxS1SJde3aFVqtFoGBgXBwcJA6DhERUanjagZElUhKSgrWrVuHvLw8AIBcLkdISAgLWSIiqrQ4M0tUSZw4cQKbNm2CTqeDg4MDevToIXUkIiKiMsdilsjCabVabNmyBXFxcQCAOnXqoH379tKGIiIiKicsZoksWFJSEiIjI5GcnAyZTIauXbuic+fOkMt5BBEREVUNLGaJLNT58+cRFRWFvLw82NnZISwsDL6+vlLHIiIiKlcsZokslLu7OxQKBWrXro1BgwahWrVqUkciIiIqdyxmiSxIVlaWsWitXr06Xn31Vbi6ukImk0mcjIiISBo8sI7IAgghcOTIEcyfPx9Xrlwxtru5ubGQJSKiKo0zs0QVXG5uLjZu3IgzZ84AAE6fPo169epJnIqIiKhiYDFLVIHduXMHkZGRuH//PuRyOXr27InAwECpYxEREVUYLGaJKiAhBA4dOoSYmBjo9Xo4OjoiPDwcNWvWlDoaERFRhcJilqgCunbtGrZu3QoAaNy4Mfr37w8bGxuJUxEREVU8LGaJKqC6deuiTZs2cHd3R/v27XmSFxERURFYzBJVAPmrFTRt2hS2trYAgNDQUIlTERERVXxcmotIYtnZ2VixYgU2b96MdevWQQghdSQiIiKLwZlZIgndvHkTkZGRSE9Ph0KhQIMGDaSOREREZFFYzBJJQAiB/fv3IzY2FkIIVK9eHYMHD4anp6fU0YiIiCwKi1micpadnY21a9fi8uXLAIBmzZqhX79+UKvVEicjIiKyPCxmicqZXC5HSkoKrKys0KdPH7Ru3ZqrFRAREZUQi1micpB/UpdMJoO1tTWGDBkCuVwODw8PiZMRERFZNq5mQFTGMjMz8fvvv+PIkSPGNi8vLxayREREpYAzs0Rl6Nq1a4iKikJWVhbu3r2LFi1a8NhYIiKiUsRilqgMGAwG7NmzB3v37gUAuLm5YfDgwSxkiYiIShmLWaJSlpGRgTVr1iA+Ph4A0Lp1a/Tp0wdKpVLaYERERJUQi1miUqTVavHTTz8hMzMTSqUS/fr1Q4sWLaSORUREVGmxmCUqRSqVCu3atcPZs2cxePBguLi4SB2JiIioUmMxS/SM0tPTodPpjIVrp06d0KFDB1hZ8eNFRERU1rg0F9EzuHjxIhYuXIhVq1ZBp9MBeHhRBBayRERE5YO/cYlKQK/XY+fOnThw4AAAwMnJCTk5OTzJi4iIqJyxmCUy04MHDxAVFYVbt24BANq3b4/g4GDOxhIREUmgQhxmsGDBAvj6+sLa2hoBAQE4dOhQkX0XL16Mzp07w9nZGc7OzggKCnpif6LSdP78eSxatAi3bt2CWq3GkCFD0KdPHxayREREEpG8mF25ciUmTZqEGTNm4NixY2jZsiVCQkKQlJRUaP/du3fjxRdfxK5du3DgwAH4+PigV69euH37djknp6pGCIEDBw4gNzcXNWrUwGuvvYYmTZpIHYuIiKhKk7yYnTdvHsaOHYsxY8bAz88PCxcuhK2tLZYsWVJo/+XLl+ONN95Aq1at0LhxY/z8888wGAzYuXNnOSenqkYmk+GFF15Ap06d8Morr8DZ2VnqSERERFWepN+NarVaHD16FFOnTjW2yeVyBAUFGU+seZrs7GzodDpUr1690Ps1Gg00Go3xdnp6OgBAp9MZzz4vKw83r3xkf2W6OyoD586dQ0JCAoCHY2hra4suXbrAYDDAYDBInI6KK/+zXtafeSo7HEPLxvGzfOU9hubsR9JiNiUlBXq9Hh4eHibtHh4eOH/+fLG28cEHH6BGjRoICgoq9P7Zs2dj1qxZBdq3b98OW1tb80ObITdXAaAfACA2NhbW1voy3R+VHoPBgDt37iAlJQUAUK9ePcTExEicip4Vx9DycQwtG8fP8pXXGGZnZxe7r0WftTJnzhysWLECu3fvhrW1daF9pk6dikmTJhlvp6enG4+zdXBwKNN8WVn///89evSAkxOXbbIEqampWLt2rbGQbd++PTQaDYKDg7n0loXS6XSIiYnhGFowjqFl4/hZvvIew/xv0otD0mLW1dUVCoUCiYmJJu2JiYnw9PR84mP/85//YM6cOdixYwdatGhRZD+1Wg21Wl2gXalUlvlgPLr58tgfPbtTp05h48aN0Gq1sLW1xaBBg1C7dm1s3ryZY1gJcAwtH8fQsnH8LF95jaE5+5D0BDCVSgV/f3+Tk7fyT+YKDAws8nFfffUV/v3vf2Pr1q1o27ZteUSlKmDbtm1Ys2YNtFotateujddeew3169eXOhYRERE9geSHGUyaNAmjRo1C27Zt0b59e8yfPx9ZWVkYM2YMAGDkyJHw9vbG7NmzAQBffvklpk+fjj/++AO+vr7Gk3Ps7OxgZ2cn2fMgy1ezZk0AQOfOndGtWzfI5ZIv9kFERERPIXkxO3ToUCQnJ2P69OlISEhAq1atsHXrVuNJYTdu3DApKn788UdotVqEh4ebbGfGjBmYOXNmeUanSiAzM9P4R1DTpk3h4eEBV1dXiVMRERFRcUlezALAxIkTMXHixELv2717t8nt+Pj4sg9ElZ5Wq8WWLVtw6dIljB8/3ljQspAlIiKyLBWimCUqT0lJSYiMjERycjJkMhmuXr36xJMIiYiIqOJiMUtVhhACcXFx2Lx5M/Ly8mBnZ4ewsDD4+vpKHY2IiIhKiMUsVQlarRYbN27EqVOnADy8CMKgQYNQrVo1iZMRERHRs2AxS1XC3r17cerUKchkMnTv3h2dOnWCTCaTOhYRERE9IxazVCV06dIFd+/eRdeuXVGrVi2p4xAREVEp4UKaVClpNBr8/fffEEIAeHiBjhEjRrCQJSIiqmQ4M0uVzt27dxEZGYnU1FQAQIcOHSRORERERGWFxSxVGkIIHD58GNu3b4der4ejoyNnYomIiCo5FrNUKeTm5iI6Ohrnzp0DADRq1AgDBgyAjY2NxMmIiIioLLGYJYt3584drF69Gg8ePIBcLkdwcDACAgK4WgEREVEVwGKWLJ4QAunp6XByckJ4eDi8vb2ljkRERETlhMUsWSSDwQC5/OFiHN7e3hg6dChq1aoFa2triZMRERFReeLSXGRxbt68iR9++AEJCQnGtoYNG7KQJSIiqoJYzJLFEEJg//79WLp0Ke7du4fY2FipIxEREZHEeJgBWYSsrCysW7cOly9fBgA0a9YM/fr1kzgVERERSY3FLFV4169fR1RUFDIyMmBlZYXevXujTZs2XK2AiIiIWMxSxXbjxg38+uuvEELAxcUFgwcPhoeHh9SxiIiIqIJgMUsVWs2aNeHr6wt7e3s8//zzUKlUUkciIiKiCoTFLFU4N27cgJeXF5RKJeRyOV588UUolUqpYxEREVEFxNUMqMIwGAzYvXs3li5dim3bthnbWcgSERFRUTgzSxVCRkYG1qxZg/j4eACAXq83uTACERERUWFYzJLkrly5gjVr1iA7OxtKpRL9+vVDixYtpI5FREREFoDFLEnGYDBg165d2LdvHwDAw8MD4eHhcHV1lTgZERERWQoWsySZrKwsHD16FADg7++PkJAQHh9LREREZmExS5Kxt7fHwIEDodVq0axZM6njEBERkQViMUvlRq/XIzY2FrVq1UKjRo0AAA0bNpQ4FREREVkynipO5SItLQ0RERH4+++/sX79euTm5kodiYiIiCoBzsxSmbtw4QLWrVuH3NxcqNVqhIaGwtraWupYREREVAmwmKUyo9frERMTg4MHDwIAatSogfDwcDg7O0ucjIiIiCoLFrNUJnQ6HSIiInDnzh0AwHPPPYegoCAoFAqJkxEREVFlwmKWyoRSqYSnpydSU1MxcOBA4wlfRERERKWJxSyVmry8POh0OtjY2AAAevfujS5dusDR0VHiZERERFRZcTUDKhWpqan45ZdfsHr1ahgMBgAPZ2dZyBIREVFZ4swsPbPTp09jw4YN0Gq1sLGxwf379+Hi4iJ1LCIiIqoCWMxSiel0OmzduhXHjh0DANSqVQthYWFwcHCQOBkRERFVFSxmqURSUlIQGRmJxMREAEDnzp3RrVs3yOU8coWIiIjKD4tZMpsQAmvWrEFiYiJsbW3xwgsvoF69elLHIiIioiqIxSyZTSaToX///ti5cyf69+8Pe3t7qSMRERFRFcXvhKlYkpKScPLkSeNtT09PvPTSSyxkiYiISFKcmaUnEkIgLi4OmzdvhsFggIuLC7y9vaWORURERASAxSw9gVarxaZNm4wzsnXr1oWTk5O0oYiIiIgewWKWCpWYmIjVq1fj3r17kMlk6N69Ozp16gSZTCZ1NCIiIiIjFrNUwLFjx7B582bo9XrY29sjLCwMtWvXljoWERERUQEsZqmA3Nxc6PV61K9fH4MGDYKtra3UkYiIiIgKxWKWAAAGg8F4wYPAwEA4OjrCz8+PhxUQERFRhcaluao4IQQOHTqEn376CVqtFsDDdWSbNm3KQpaIiIgqPM7MVmG5ubmIjo7GuXPnADw8Vva5556TOBURERFR8bGYraJu376NyMhIPHjwAHK5HMHBwQgICJA6FhEREZFZWMxWMUIIHDx4EDExMTAYDHByckJ4eDgvhEBEREQWicVsFbN3717s3r0bANCkSRP0798f1tbW0oYiIiIiKiEWs1WMv78/jh8/jg4dOqBdu3Y8yYuIiIgsGovZSk4IgatXr6JevXoAADs7O0ycOBFWVhx6IiIisnxcmqsSy87Oxp9//onff/8dZ86cMbazkCUiIqLKglVNJXX9+nVERUUhIyMDCoUCOp1O6khEREREpY7FbCUjhMC+ffuwa9cuCCHg4uKCwYMHw8PDQ+poRERERKWOxWwlkpWVhTVr1uDq1asAgBYtWuD555+HSqWSOBkRERFR2WAxW4ncvn0bV69ehZWVFfr27YtWrVpxtQIiIiKq1FjMViINGzZEr169UK9ePbi7u0sdh4iIiKjMcTUDC5aRkYFVq1YhLS3N2BYYGMhCloiIiKoMzsxaqCtXrmDt2rXIysqCVqvFyy+/LHUkIiIionLHYtbCGAwG7N69G3/99RcAwN3dHb1795Y4FREREZE0WMxakPT0dERFReHGjRsAgDZt2qB3795QKpUSJyMiIiKSBotZC5GQkIBly5YhJycHKpUKoaGhaNasmdSxiIiIiCTFYtZCuLi4wN7eHo6OjggPD4eLi4vUkYiIiIgkx2K2AsvIyICdnR1kMhmUSiWGDx+OatWqwcqKw0ZEREQEsJitsC5cuIB169YhMDAQXbp0AQA4OjpKnIqIqGLS6/XQ6XRSx6Ai6HQ6WFlZITc3F3q9Xuo4VAJlMYZKpRIKheKZt8NitoLR6/XYsWMH/vnnHwDApUuX0KlTJ8jlXBKYiKgwmZmZuHXrFoQQUkehIggh4OnpiZs3b/LKlBaqLMZQJpOhZs2asLOze6btsJitQO7fv4+oqCjcvn0bABAQEIDg4GAWskRERdDr9bh16xZsbW3h5ubGQqmCMhgMyMzMhJ2dHX+nWajSHkMhBJKTk3Hr1i00aNDgmWZoWcxWEOfOncP69euh0WhgbW2NAQMGoHHjxlLHIiKq0HQ6HYQQcHNzg42NjdRxqAgGgwFarRbW1tYsZi1UWYyhm5sb4uPjodPpWMxauoyMDERFRUGv16NmzZoICwuDk5OT1LGIiCwGZ2SJLE9pfW5ZzFYA9vb26N27N1JTU9GzZ89SORiaiIiIqCpgMSuRM2fOwMnJCd7e3gCAtm3bSpyIiIiIyPLwwJVyptPpsHHjRkRGRiIyMhK5ublSRyIiIrI49+7dg7u7O+Lj46WOQoXYunUrWrVqBYPBUOb7qhDF7IIFC+Dr6wtra2sEBATg0KFDT+y/evVqNG7cGNbW1mjevDk2b95cTkmfTUpKCn755RccPXoUANCsWTOoVCqJUxERUXkbPXo0ZDKZ8aI4derUwfvvv1/oBMfGjRvRtWtX2Nvbw9bWFu3atUNERESh242KikK3bt3g6OgIOzs7tGjRAp9++ilSU1OfmGfXrl3o27cvXFxcYGtrCz8/P7z33nvG1XUqos8//xwDBgyAr6+v1FHKxJkzZxAWFgZfX1/IZDLMnz+/WI87efIkOnfuDGtra/j4+OCrr74q0OdpdZQQAtOnT4eXlxdsbGwQFBSES5cumfRJTU3FSy+9BAcHBzg5OeHVV19FZmam8f7evXtDqVRi+fLl5j95M0lezK5cuRKTJk3CjBkzcOzYMbRs2RIhISFISkoqtP/ff/+NF198Ea+++iqOHz+OgQMHYuDAgTh9+nQ5JzfP+fOn8dNPPyExMRG2trZ4+eWX0bNnT57VSURURfXu3Rt3797F1atX8c0332DRokWYMWOGSZ/vv/8eAwYMQMeOHXHw4EGcPHkSw4YNw/jx4zF58mSTvh9//DGGDh2Kdu3aYcuWLTh9+jS+/vprnDhxAr/99luRORYtWoSgoCB4enoiKioKZ8+excKFC5GWloavv/66xM9Pq9WW+LFPk52djV9++QWvvvrqM22nLDM+q+zsbNStWxdz5syBp6dnsR6Tnp6OXr16oXbt2jh69Cjmzp2LmTNn4qeffjL2KU4d9dVXX+G7777DwoULcfDgQVSrVg19+vQx+WPrpZdewpkzZxATE4ONGzdi7969GDdunEme0aNH47vvvnvGV6IYhMTat28vJkyYYLyt1+tFjRo1xOzZswvtP2TIEPH888+btAUEBIjXXnutWPtLS0sTAERaWlrJQxdTZqYQCoVO9O+/TsycOVPMnDlTREREiPT09DLfN5UerVYr1q1bJ7RardRRqIQ4hpavqDHMyckRZ8+eFTk5OUIIIQyGhz97pfhnMBT/+YwaNUoMGDDApO2FF14QrVu3Nt6+ceOGUCqVYtKkSQUe/9133wkA4p9//hFCCHHw4EEBQMyfP7/Q/d2/f7/Q9ps3bwqVSiXeeeedJz5uxowZomXLlib3ffPNN6J27doFntNnn30mvLy8hK+vr5g6dapo37690Ov14v79+0Kv1wshhGjRooWYNWuW8bGLFy8WjRs3Fmq1WjRq1EgsWLCg0Dz5Vq9eLdzc3Eza8vLyxCuvvCJ8fX2FtbW1aNiwYYHXo7CMQjx8rQcPHiwcHR2Fs7Oz6N+/v7h27ZrxcYcOHRJBQUHCxcVFODg4iC5duoijR48+MWNpql27tvjmm2+e2u+HH34Qzs7OQqPRGNs++OAD0ahRI+Ptp9VRBoNBeHp6irlz5xrvf/DggVCr1eLnn38Wer1enD17VgAQhw8fNvbZsmWLkMlk4vbt28a269evCwDi8uXLheZ9/PP7KHPqNUlPANNqtTh69CimTp1qbJPL5QgKCsKBAwcKfcyBAwcwadIkk7aQkBCsW7eu0P4ajQYajcZ4Oz09HcDDY1fL+tKHOh1gMFjBzi4LANCpUyfj1bx42UXLkT9WHDPLxTG0fEWNYf46swaDAQaDAVlZgIODNN94pacbUK1a8foKIYy5AeD06dP4+++/Ubt2bWPb6tWrodPpMGnSpALHHY4dOxYfffQR/vjjD7Rr1w6///477OzsMH78+EKPUXRwcCi0fdWqVdBqtZg8efITHyf+d3W1R/s83iaEwM6dO2Fvb49t27YZ+82ePRuXL1+Gu7s7hBA4deoUTp48idWrV8NgMGD58uWYPn06vvvuO7Ru3RrHjx/Ha6+9BhsbG4waNarQ12/v3r1o06aNSZ68vDx4e3tj5cqVcHFxwd9//43x48fDw8MDQ4YMKTKjRqNBSEgInnvuOezZswdWVlb4/PPP0bt3b8TFxUGlUiEtLQ0jRozAt99+CyEE5s2bh759++LChQuwt7cvNOPy5cvx+uuvF3pfvk2bNqFz585P7PPo6/2040///vtvdO7cGVZWVsa+wcHB+PLLL3Hv3j04OzvjwIEDePfdd0221atXL6xfvx4GgwFXr15FQkICevToYexjb2+P9u3b4/Dhwxg9ejT2798PJycnkzHo0aMH5HI5Dhw4gEGDBgEAatasCQ8PD+zZswd16tQpkDf/vVXYOrPm/LyWtJhNSUmBXq+Hh4eHSbuHhwfOnz9f6GMSEhIK7Z+QkFBo/9mzZ2PWrFkF2rdv3w5bW9sSJi+e3FwFhOiHdesG4vvv1yMzMxNbt24t031S2YmJiZE6Aj0jjqHle3wMrays4OnpiczMTGi1WmRlAYCTFNGQnp6O4l6yXqfTYdOmTXBwcEBeXh40Gg3kcjm+/PJL46TL6dOn4eDggGrVqhnbHlW7dm2cPXsW6enpOHfuHGrXro2cnBzk5OQUO/OZM2dgb29f5D7yaTQa6PV6kz65ubkwGAwmk0S2trb4+uuvTc4HadasGX799VdMmTIFGRkZWLp0Kdq2bQt3d3ekp6djxowZ+PTTTxEUFAQACAoKwuuvv44ff/zRWBQ97sqVK3BzcyuQ+dHJrtDQUOzduxd//vknevfuXWTGiIgI5OXl4euvvzauezp//nz4+vpi8+bN6NGjR4EVh+bOnYvVq1djy5Ytxm0/rlu3bti7d2+RrykAeHl5PfF1z2cwGJCbm/vUvrdv30atWrVM+lX7319Yly9fRqNGjZCQkAB7e3uTPg4ODrh79y7S09Nx5coVAICtra1JHxcXFyQlJSEjIwPXr1+Hq6trgTzOzs6Ij483affw8MClS5cKza7VapGTk4O9e/ciLy/P5L7s7OynvSxGlX5prqlTp5q8udPT0+Hj44NevXrBwcGhTPctBJCUlI3Y2Fj06zcAKpWyTPdHZUOn0yEmJgbBwcFQKjmGlohjaPmKGsPc3FzcvHkTdnZ2sLa2hr39wxlSKdjaOqC4a8ArlUp069YNP/zwA7KysjB//nxYWVnh5ZdfNvZRqVSQyWRF/q5SKBSwsrKCg4MDFAoFFAqF2b/XlEol5HL5Ux+nVqsLbD//SlD5bUqlEs2bN4erq6vJY0eMGIGlS5diypQpsLOzw9q1a/Huu+/CwcEBWVlZuHbtGt566y288847xsfk5eXB0dGxyFw6nQ729vYF7v/hhx+wdOlS3LhxAzk5OdBqtWjVqtUTM166dAlXr16Fj4+PybZyc3Nx9+5dODg4IDExEdOmTcOePXuQlJQEvV6P7Oxs3Lt3r8iMDg4OxuU3n5VcLoe1tfVTx0mhUEClUpn0s7OzM/43v93Gxsakj42NjfG9ll/8Pv76WllZQa/Xw97evsDY55PJZAVy2tnZQa/XF5o9NzcXNjY26NKlC6ytrU3uK06Rb8xW7J5lwNXVFQqFAomJiSbtiYmJRR7s7OnpaVZ/tVoNtVpdoF2pVJbLLzUnJ8DaWg+Vqnz2R2WnvN4zVHY4hpbv8THU6/WQyWSQy+XGE2qL+Na3QpHJZLCzs0PDhg0BAEuXLkXLli2xdOlS40lNjRo1QlpaGhISElCjRg2Tx2u1Wly5cgXdu3eHXC5Ho0aNsH//fuj1erPe4/n7SExMhJeXV5H9FAoFhBAmJy3nz6Tlt+U/p8dPbB4+fDg+/PBDnDhxAnK5HDdv3sSwYcMgl8uNs2+LFy9GQEBAgX0WdZK0m5sbHjx4YHL/ihUrMGXKFHz99dcIDAyEvb095s6di4MHDz4xY1ZWFvz9/Qs9697NzQ1yuRxjxozBvXv38O2336J27dpQq9UIDAyETqcrMuPy5cvx2muvFf6C/s+WLVuKfZhB/vv8Sby8vJCUlGTSLzk5GQBQo0YNyOVyeHp6Ijk52aRPUlISPD09IZfLje+15ORkk2I8KSkJTZo0gUwmK3Q/eXl5SE1NNe4nX2pqKtzd3QvNLpfLjSt6PP6+Ned9LOmp9CqVCv7+/ti5c6exzWAwYOfOnQgMDCz0MYGBgSb9gYdfOxXVn4iIqKKTy+X46KOP8MknnxgPEwgLC4NSqSx0RYGFCxciKysLL774IoCHBWNmZiZ++OGHQrf/4MGDQtvDw8OhUqkKXb7p0ce5ubkhISHBeJwsAMTFxRXrudWsWRNdu3bF6tWr8ccffyA4OBju7u4AHn4FXaNGDVy9ehX169c3+VfYMZb5WrdujbNnz5q07d+/Hx06dMAbb7yB1q1bo379+savzJ+kTZs2uHTpEtzd3QtkcHR0NG77rbfeQt++fdG0aVOo1WqkpKQ8cbv9+/dHXFzcE/+V9gWTAgMDsXfvXpPjTWNiYtCoUSM4Ozsb+zypjqpTpw48PT1N+qSnp+PgwYNo166dcRsPHjwwLjUKALGxsTAYDCZ/lOTm5uLKlSto3bp1qT7PAp56ilgZW7FihVCr1SIiIkKcPXtWjBs3Tjg5OYmEhAQhhBAjRowQH374obH//v37hZWVlfjPf/4jzp07J2bMmCGUSqU4depUsfZXnqsZCMGzqCsDjqHl4xhavuKuZmApClvNQKfTCW9vb5OzyL/55hshl8vFRx99JM6dOycuX74svv76a6FWq8V7771n8vj3339fKBQKMWXKFPH333+L+Ph4sWPHDhEeHl7kKgdCCLFgwQIhk8nEK6+8Inbv3i3i4+PFvn37xLhx44wrKZw9e1bIZDIxZ84ccfnyZfHf//5XODs7F7qaQWEWLVokvLy8hKurq/jtt99M7lu8eLGwsbER3377rbhw4YI4efKkWLJkifj666+LzHzy5ElhZWUlUlNTjW3ffvutcHBwEFu3bhUXLlwQn3zyiXBwcDBZhaGwjFlZWaJBgwaiW7duYu/eveLq1ati165d4s033xQ3b94UQgjRunVrERwcLM6ePSv++ecf0blzZ2FjY1OsFQZKSqPRiOPHj4vjx48LLy8vMXnyZHH8+HFx6dIlY5/vv/9e9OjRw3j7wYMHwsPDQ4wYMUKcPn1arFixQtja2opFixYZ+xSnjpozZ45wcnIS69evFydPnhQDBgwQderUEXfv3jWuSNG7d2/RunVrcfDgQbFv3z7RoEED8eKLL5o8h127dgk7OzuRlZVV6HMsrdUMJC9mhXg4GLVq1RIqlUq0b9/euNSIEEJ07dpVjBo1yqT/qlWrRMOGDYVKpRJNmzYVmzZtKva+WMySuTiGlo9jaPmqQjErhBCzZ88Wbm5uIjMz09i2fv160blzZ1GtWjVhbW0t/P39xZIlSwrd7sqVK0WXLl2Evb29qFatmmjRooX49NNPi1yaK19MTIwICQkRzs7OwtraWjRu3FhMnjxZ3Llzx9jnxx9/FD4+PqJatWpi5MiR4vPPPy92MXvv3j2hVquFra2tyMjIKHD/8uXLRatWrYRKpRLOzs6iS5cuYs2aNU/M3L59e7Fw4ULj7dzcXDF69Gjh6OgonJycxOuvvy4+/PDDpxazQghx9+5dMXLkSOHq6irUarWoW7euGDt2rLFWOHbsmGjbtq2wtrYWDRo0EKtXry72clklde3aNQGgwL+uXbsa+8yYMcNkDIQQ4sSJE6JTp05CrVYLb29vMWfOnALbflodZTAYxLRp04SHh4dQq9WiZ8+e4ty5cybLq927d0+8+OKLws7OTjg4OIgxY8YUGNtx48Y9cenU0ipmZUI88p1BFZCeng5HR0ekpaWV+QlgwMOD1Ddv3oy+ffvyWD0LxTG0fBxDy1fUGObm5uLatWuoU6dOgRNIqOLIX/XAwcGh1C4WtGnTJkyZMgWnT5/mBYjKgbljmJKSgkaNGuHIkSNFHjLypM+vOfVapV/NgIiIiCqf559/HpcuXcLt27cLrERA0ouPj8cPP/zwxGOfSwuLWSIiIrJIjy7nRRVL27ZtS/0Et6JwXp6IiIiILBaLWSIiIiKyWCxmiYjI4lWxc5mJKoXS+tyymCUiIoulUCgAPLwiFhFZlvzPbf7nuKR4AhgREVksKysr2NraIjk5GUqlkks0VVAGgwFarRa5ubkcIwtV2mNoMBiQnJwMW1tbWFk9WznKYpaIiCxW/nXir127huvXr0sdh4oghEBOTg5sbGwgk8mkjkMlUBZjKJfLUatWrWfeHotZIiKyaCqVCg0aNOChBhWYTqfD3r170aVLF164xEKVxRiqVKpSmeVlMUtERBZPLpfzCmAVmEKhQF5eHqytrVnMWqiKPIY8cIWIiIiILBaLWSIiIiKyWCxmiYiIiMhiVbljZvMX6E1PTy+X/el0OmRnZyM9Pb3CHWNCxcMxtHwcQ8vHMbRsHD/LV95jmF+nFefCClWumM3IyAAA+Pj4SJyEiIiIiJ4kIyMDjo6OT+wjE1XsGoAGgwF37tyBvb19uax1l56eDh8fH9y8eRMODg5lvj8qfRxDy8cxtHwcQ8vG8bN85T2GQghkZGSgRo0aT12+q8rNzMrlctSsWbPc9+vg4MAPsIXjGFo+jqHl4xhaNo6f5SvPMXzajGw+ngBGRERERBaLxSwRERERWSwWs2VMrVZjxowZUKvVUkehEuIYWj6OoeXjGFo2jp/lq8hjWOVOACMiIiKiyoMzs0RERERksVjMEhEREZHFYjFLRERERBaLxSwRERERWSwWs6VgwYIF8PX1hbW1NQICAnDo0KEn9l+9ejUaN24Ma2trNG/eHJs3by6npFQUc8Zw8eLF6Ny5M5ydneHs7IygoKCnjjmVPXM/h/lWrFgBmUyGgQMHlm1Aeipzx/DBgweYMGECvLy8oFar0bBhQ/48lZC54zd//nw0atQINjY28PHxwbvvvovc3NxySkuP27t3L0JDQ1GjRg3IZDKsW7fuqY/ZvXs32rRpA7Vajfr16yMiIqLMcxZK0DNZsWKFUKlUYsmSJeLMmTNi7NixwsnJSSQmJhbaf//+/UKhUIivvvpKnD17VnzyySdCqVSKU6dOlXNyymfuGA4fPlwsWLBAHD9+XJw7d06MHj1aODo6ilu3bpVzcspn7hjmu3btmvD29hadO3cWAwYMKJ+wVChzx1Cj0Yi2bduKvn37in379olr166J3bt3i7i4uHJOTkKYP37Lly8XarVaLF++XFy7dk1s27ZNeHl5iXfffbeck1O+zZs3i48//lisWbNGABBr1659Yv+rV68KW1tbMWnSJHH27Fnx/fffC4VCIbZu3Vo+gR/BYvYZtW/fXkyYMMF4W6/Xixo1aojZs2cX2n/IkCHi+eefN2kLCAgQr732WpnmpKKZO4aPy8vLE/b29uLXX38tq4j0FCUZw7y8PNGhQwfx888/i1GjRrGYlZi5Y/jjjz+KunXrCq1WW14R6QnMHb8JEyaIHj16mLRNmjRJdOzYsUxzUvEUp5h9//33RdOmTU3ahg4dKkJCQsowWeF4mMEz0Gq1OHr0KIKCgoxtcrkcQUFBOHDgQKGPOXDggEl/AAgJCSmyP5Wtkozh47Kzs6HT6VC9evWyiklPUNIx/PTTT+Hu7o5XX321PGLSE5RkDKOjoxEYGIgJEybAw8MDzZo1wxdffAG9Xl9esel/SjJ+HTp0wNGjR42HIly9ehWbN29G3759yyUzPbuKVM9YlfseK5GUlBTo9Xp4eHiYtHt4eOD8+fOFPiYhIaHQ/gkJCWWWk4pWkjF83AcffIAaNWoU+FBT+SjJGO7btw+//PIL4uLiyiEhPU1JxvDq1auIjY3FSy+9hM2bN+Py5ct44403oNPpMGPGjPKITf9TkvEbPnw4UlJS0KlTJwghkJeXh/Hjx+Ojjz4qj8hUCoqqZ9LT05GTkwMbG5tyy8KZWaJnMGfOHKxYsQJr166FtbW11HGoGDIyMjBixAgsXrwYrq6uUsehEjIYDHB3d8dPP/0Ef39/DB06FB9//DEWLlwodTQqht27d+OLL77ADz/8gGPHjmHNmjXYtGkT/v3vf0sdjSwQZ2afgaurKxQKBRITE03aExMT4enpWehjPD09zepPZaskY5jvP//5D+bMmYMdO3agRYsWZRmTnsDcMbxy5Qri4+MRGhpqbDMYDAAAKysrXLhwAfXq1Svb0GSiJJ9DLy8vKJVKKBQKY1uTJk2QkJAArVYLlUpVppnp/5Vk/KZNm4YRI0bgX//6FwCgefPmyMrKwrhx4/Dxxx9DLudcW0VXVD3j4OBQrrOyAGdmn4lKpYK/vz927txpbDMYDNi5cycCAwMLfUxgYKBJfwCIiYkpsj+VrZKMIQB89dVX+Pe//42tW7eibdu25RGVimDuGDZu3BinTp1CXFyc8V///v3RvXt3xMXFwcfHpzzjE0r2OezYsSMuX75s/EMEAC5evAgvLy8WsuWsJOOXnZ1doGDN/8NECFF2YanUVKh6ptxPOatkVqxYIdRqtYiIiBBnz54V48aNE05OTiIhIUEIIcSIESPEhx9+aOy/f/9+YWVlJf7zn/+Ic+fOiRkzZnBpLomZO4Zz5swRKpVKREZGirt37xr/ZWRkSPUUqjxzx/BxXM1AeuaO4Y0bN4S9vb2YOHGiuHDhgti4caNwd3cXn332mVRPoUozd/xmzJgh7O3txZ9//imuXr0qtm/fLurVqyeGDBki1VOo8jIyMsTx48fF8ePHBQAxb948cfz4cXH9+nUhhBAffvihGDFihLF//tJcU6ZMEefOnRMLFizg0lyW7Pvvvxe1atUSKpVKtG/fXvzzzz/G+7p27SpGjRpl0n/VqlWiYcOGQqVSiaZNm4pNmzaVc2J6nDljWLt2bQGgwL8ZM2aUf3AyMvdz+CgWsxWDuWP4999/i4CAAKFWq0XdunXF559/LvLy8so5NeUzZ/x0Op2YOXOmqFevnrC2thY+Pj7ijTfeEPfv3y//4CSEEGLXrl2F/m7LH7dRo0aJrl27FnhMq1athEqlEnXr1hVLly4t99xCCCETgvP5RERERGSZeMwsEREREVksFrNEREREZLFYzBIRERGRxWIxS0REREQWi8UsEREREVksFrNEREREZLFYzBIRERGRxWIxS0REREQWi8UsEVmciIgIODk5SR2jxGQyGdatW/fEPqNHj8bAgQPLJU9FM23aNIwbN67c9zts2DB8/fXX5b5fIno2LGaJSBKjR4+GTCYr8O/y5ctSR0NERIQxj1wuR82aNTFmzBgkJSWVyvbv3r2LPn36AADi4+Mhk8kQFxdn0ufbb79FREREqeyvKDNnzjQ+T4VCAR8fH4wbNw6pqalmbac0C++EhAR8++23+Pjjj022/6T3yqP3q1Qq1K9fH59++iny8vIAALt37zZ5nJubG/r27YtTp06Z7PuTTz7B559/jrT/a+/+Y6Ku/wCOPzkQOH7pSBlcKBLKzaWmJ2hirjQKnD+YqFCydBPJgYipVK4ZQk3MDJy4Moz5I2WhtAwmAeqSPK6VaAJL5IcGoYvV0qYjQZB7f/9w3Dw5MDMV9n09tvvj8/75et8+G6973/s+XLv2n6xFCPFoSDIrhHhswsPDaWlpsXr5+/s/7rAA8PDwoKWlhcuXL/PZZ59RXFzMa6+99p+M7e3tjZOTU59tBg8e/Eh2n59++mlaWlpobm5mz549lJSUEB8f/9Dn7U1OTg4hISH4+flZld/rXumub2hoYN26daSmprJ161arMerq6mhpaaG0tJSbN28ye/ZsOjo6LPVjx44lICCAAwcOPNxFCiH+U5LMCiEeGycnJ7y9va1e9vb2ZGZmMm7cOFxdXRk+fDgJCQm0trb2Ok5VVRUzZszA3d0dDw8PJk2axOnTpy315eXlTJ8+Ha1Wy/Dhw0lKSuLvv//uMzY7Ozu8vb3R6XTMmjWLpKQkjh8/TltbG2azmffeew9fX1+cnJyYMGECJSUllr4dHR0kJibi4+ODs7Mzfn5+bN682Wrs7mMG3QnZxIkTsbOz44UXXgCsdzt37dqFTqfDbDZbxRgREcGyZcss1wUFBRgMBpydnXnqqadIS0uz7E72xsHBAW9vb5588klCQ0NZtGgRx44ds9R3dXURGxuLv78/Wq0WvV7P9u3bLfWpqans27ePgoICy85nWVkZAJcuXSIqKoohQ4bg6elJREQETU1NfcaTl5fH3Llze5T3dq/cXe/n50d8fDyhoaEUFhZajeHl5YW3tzcGg4E33niDS5cuUVtba9Vm7ty55OXl9RmjEKJ/kWRWCNHvaDQasrKyOHfuHPv27ePbb7/lrbfe6rV9TEwMvr6+VFRUcObMGdavX8+gQYMAuHjxIuHh4SxYsIDq6moOHjxIeXk5iYmJ9xWTVqvFbDZz69Yttm/fTkZGBh999BHV1dWEhYUxb948GhoaAMjKyqKwsJBDhw5RV1dHbm4uI0eOtDnuqVOnADh+/DgtLS189dVXPdosWrSIK1eucOLECUvZ1atXKSkpISYmBgCj0ciSJUtYvXo1NTU1ZGdns3fvXjZt2vSP19jU1ERpaSmOjo6WMrPZjK+vL/n5+dTU1JCSksI777zDoUOHAEhOTiYqKspq5zQkJITOzk7CwsJwd3fHaDRiMplwc3MjPDzcajf0TlevXqWmpoagoKB/HHNvtFptr/Ncu3bNkrDeuVaAyZMnc+rUKW7evPnAMQghHhElhBCPwdKlS5W9vb1ydXW1vBYuXGizbX5+vnriiScs13v27FGDBw+2XLu7u6u9e/fa7BsbG6tef/11qzKj0ag0Go1qa2uz2efu8evr61VgYKAKCgpSSiml0+nUpk2brPoEBwerhIQEpZRSq1atUjNnzlRms9nm+IA6fPiwUkqpxsZGBaizZ89atVm6dKmKiIiwXEdERKhly5ZZrrOzs5VOp1NdXV1KKaVefPFFlZ6ebjXG/v37lY+Pj80YlFJq48aNSqPRKFdXV+Xs7KwABajMzMxe+yil1MqVK9WCBQt6jbV7br1eb/Ue3Lx5U2m1WlVaWmpz3LNnzypANTc3W5Xf6165c36z2ayOHTumnJycVHJyslJKqRMnTijA0rd7nfPmzesRQ1VVlQJUU1NTn++BEKL/cHhsWbQQ4v/ejBkz2Llzp+Xa1dUVuL1LuXnzZmpra7l+/Tq3bt2ivb2dGzdu4OLi0mOctWvXsnz5cvbv32/5qjwgIAC4fQShurqa3NxcS3ulFGazmcbGRsaMGWMztmvXruHm5obZbKa9vZ3nnnuOnJwcrl+/zm+//ca0adOs2k+bNo2qqirg9hGBl156Cb1eT3h4OHPmzOHll19+oPcqJiaGuLg4PvnkE5ycnMjNzeWVV15Bo9FY1mkymax2Yru6uvp83wD0ej2FhYW0t7dz4MABKisrWbVqlVWbjz/+mN27d9Pc3ExbWxsdHR1MmDChz3irqqq4cOEC7u7uVuXt7e1cvHjRZp+2tjYAnJ2de9T1dq90O3LkCG5ubnR2dmI2m1m8eDGpqalWbYxGIy4uLvzwww+kp6fz6aef9phHq9UCcOPGjT7XJ4ToPySZFUI8Nq6urowaNcqqrKmpiTlz5hAfH8+mTZvw9PSkvLyc2NhYOjo6bCZlqampLF68mKKiIoqLi9m4cSN5eXnMnz+f1tZWVqxYQVJSUo9+I0aM6DU2d3d3fvrpJzQaDT4+PpYk5/r16/dcl8FgoLGxkeLiYo4fP05UVBShoaF8+eWX9+zbm7lz56KUoqioiODgYIxGI9u2bbPUt7a2kpaWRmRkZI++tpLDbt2//gf44IMPmD17Nmlpabz//vvA7TOsycnJZGRkMHXqVNzd3dm6dSs//vhjn/G2trYyadIkqw8R3YYNG2azz9ChQwH466+/erSxda/cqTvZdXR0RKfT4eDQ88+bv78/Q4YMQa/X88cffxAdHc3Jkyet2nQ/yaG3GIUQ/Y8ks0KIfuXMmTOYzWYyMjIsu47d5zP7EhgYSGBgIGvWrOHVV19lz549zJ8/H4PBQE1NTZ+JkC0ajcZmHw8PD3Q6HSaTieeff95SbjKZmDx5slW76OhooqOjWbhwIeHh4Vy9ehVPT0+r8brPbHZ1dfUZj7OzM5GRkeTm5nLhwgX0ej0Gg8FSbzAYqKuru+913m3Dhg3MnDmT+Ph4yzpDQkJISEiwtLl7Z9XR0bFH/AaDgYMHD+Ll5YWHh8c/mjsgIAAPDw9qamoIDAy8r7jvlezebeXKlWzevJnDhw8zf/58S/nPP/+Mr6+vJbEWQvR/8gMwIUS/MmrUKDo7O9mxYwe//PIL+/fvt/l1cLe2tjYSExMpKyvj119/xWQyUVFRYTk+8Pbbb/P999+TmJhIZWUlDQ0NFBQU3PcPwO705ptvsmXLFg4ePEhdXR3r16+nsrKS1atXA5CZmckXX3xBbW0t9fX15Ofn4+3tbfNRW15eXmi1WkpKSvj999/7fMZpTEwMRUVF7N692/LDr24pKSl8/vnnpKWlce7cOc6fP09eXh4bNmy4r7VNnTqV8ePHk56eDsDo0aM5ffo0paWl1NfX8+6771JRUWHVZ+TIkVRXV1NXV8eff/5JZ2cnMTExDB06lIiICIxGI42NjZSVlZGUlMTly5dtzq3RaAgNDaW8vPy+Yv43XFxciIuLY+PGjSilLOVGo/GBj4QIIR4tSWaFEP3KM888Q2ZmJlu2bGHs2LHk5uZaPdbqbvb29ly5coUlS5YQGBhIVFQUs2bNIi0tDYDx48fz3XffUV9fz/Tp05k4cSIpKSnodLp/HWNSUhJr165l3bp1jBs3jpKSEgoLCxk9ejRw+4jChx9+SFBQEMHBwTQ1NfHNN99Ydprv5ODgQFZWFtnZ2eh0OiIiInqdd+bMmXh6elJXV8fixYut6sLCwjhy5AhHjx4lODiYZ599lm3btvV4Xus/sWbNGnJycrh06RIrVqwgMjKS6OhopkyZwpUrV6x2aQHi4uLQ6/UEBQUxbNgwTCYTLi4unDx5khEjRhAZGcmYMWOIjY2lvb29z53a5cuXk5eX1+MxZA9DYmIi58+fJz8/H7h9nvfrr78mLi7uoc8thPjv2Kk7P5IKIYQQj5FSiilTpliOizxKO3fu5PDhwxw9evSRziuEeDCyMyuEEKLfsLOzY9euXff8Zw8Pw6BBg9ixY8cjn1cI8WBkZ1YIIYQQQgxYsjMrhBBCCCEGLElmhRBCCCHEgCXJrBBCCCGEGLAkmRVCCCGEEAOWJLNCCCGEEGLAkmRWCCGEEEIMWJLMCiGEEEKIAUuSWSGEEEIIMWBJMiuEEEIIIQas/wFD9szupn0ibAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy"
      ],
      "metadata": {
        "id": "vanJbRVlPRHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# For simplicity, we'll use only two classes (binary classification)\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with custom C value (inverse of regularization strength)\n",
        "model = LogisticRegression(C=0.5, max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOM5zl2CPV74",
        "outputId": "f3998fcc-9352-4028-b65d-1f4bc9e1a901"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#18. Write a Python program to train Logistic Regression and identify important features based on model coefficients"
      ],
      "metadata": {
        "id": "bLu4HN2NPdLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# For simplicity, we'll use only two classes (binary classification)\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Standardization (Feature Scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Print the model coefficients\n",
        "coefficients = model.coef_[0]\n",
        "feature_names = data.feature_names\n",
        "\n",
        "print(\"Model Coefficients:\")\n",
        "for feature, coef in zip(feature_names, coefficients):\n",
        "    print(f\"{feature}: {coef:.4f}\")\n",
        "\n",
        "# Identify important features based on the absolute value of coefficients\n",
        "importance = np.abs(coefficients)\n",
        "sorted_indices = np.argsort(importance)[::-1]\n",
        "\n",
        "print(\"\\nImportant Features based on Coefficients:\")\n",
        "for index in sorted_indices:\n",
        "    print(f\"{feature_names[index]}: Importance = {importance[index]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6DJhkrtPhAH",
        "outputId": "8e05a921-3c5a-45e6-9f8b-89c7bb80562a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Coefficients:\n",
            "sepal length (cm): 0.8057\n",
            "sepal width (cm): -1.0671\n",
            "petal length (cm): 1.4642\n",
            "petal width (cm): 1.4738\n",
            "\n",
            "Important Features based on Coefficients:\n",
            "petal width (cm): Importance = 1.4738\n",
            "petal length (cm): Importance = 1.4642\n",
            "sepal width (cm): Importance = 1.0671\n",
            "sepal length (cm): Importance = 0.8057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score"
      ],
      "metadata": {
        "id": "xkfTSYpZPoKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# For simplicity, we'll use only two classes (binary classification)\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Cohen's Kappa score\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "print(f\"Cohen's Kappa Score: {kappa_score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7TRB27IPtCr",
        "outputId": "24c7356b-87a1-4c29-a14c-7bb6583061e5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #20.Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classificatio:"
      ],
      "metadata": {
        "id": "qXY0qP3PPy21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# For simplicity, we'll use only two classes (binary classification)\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the test data\n",
        "y_probs = model.predict_proba(X_test)[:, 1]  # Get the probability for class 1\n",
        "\n",
        "# Calculate precision and recall values\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_probs)\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, color='b', label='Logistic Regression')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "l3dpRNswP5N4",
        "outputId": "191252bb-04bc-4dc5-d638-1b6068dbdd8e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#21.  Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy"
      ],
      "metadata": {
        "id": "2SbXvfOqQB1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# For simplicity, we'll use only two classes (binary classification)\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# List of solvers to compare\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "\n",
        "# Dictionary to store accuracy for each solver\n",
        "accuracy_dict = {}\n",
        "\n",
        "# Loop through each solver and train a Logistic Regression model\n",
        "for solver in solvers:\n",
        "    # Initialize the Logistic Regression model with the current solver\n",
        "    model = LogisticRegression(solver=solver, max_iter=200)\n",
        "\n",
        "    # Train the model on the training data\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test data\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy and store in the dictionary\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_dict[solver] = accuracy\n",
        "\n",
        "# Print the accuracy for each solver\n",
        "print(\"Accuracy for different solvers:\")\n",
        "for solver, accuracy in accuracy_dict.items():\n",
        "    print(f\"Solver: {solver}, Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kPiiK2kQGe3",
        "outputId": "a6fd89dc-ba5e-4ee4-c896-b148d07bfdf4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for different solvers:\n",
            "Solver: liblinear, Accuracy: 100.00%\n",
            "Solver: saga, Accuracy: 100.00%\n",
            "Solver: lbfgs, Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC)M"
      ],
      "metadata": {
        "id": "oAfAjMfcQRsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# For simplicity, we'll use only two classes (binary classification)\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Matthews Correlation Coefficient (MCC)\n",
        "mcc_score = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "# Print the MCC score\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc_score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyfXVmDNQb7J",
        "outputId": "52c754b2-600f-40f9-af58-71dae04f6e26"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scalingM"
      ],
      "metadata": {
        "id": "Spm4yID6QdoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# For simplicity, we'll use only two classes (binary classification)\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train and evaluate the model on raw data\n",
        "model.fit(X_train, y_train)\n",
        "y_pred_raw = model.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# Standardize the data using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train and evaluate the model on standardized data\n",
        "model.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print the accuracy comparison\n",
        "print(f\"Accuracy on raw data: {accuracy_raw * 100:.2f}%\")\n",
        "print(f\"Accuracy on standardized data: {accuracy_scaled * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KJe-2ABQjvK",
        "outputId": "a7178e1a-875f-4a6e-daae-740d92986132"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on raw data: 100.00%\n",
            "Accuracy on standardized data: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validationM\n"
      ],
      "metadata": {
        "id": "ZiAWXa1KQn4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# For simplicity, we'll use only two classes (binary classification)\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "\n",
        "# Define the range of C values to test\n",
        "param_grid = {'C': np.logspace(-4, 4, 20)}\n",
        "\n",
        "# Set up GridSearchCV to perform cross-validation and search for the best C\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Perform the grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best C value and the corresponding accuracy\n",
        "print(f\"Best C value: {grid_search.best_params_['C']}\")\n",
        "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate the model on the test set using the best C value\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy on the test set\n",
        "print(f\"Test set accuracy with optimal C: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBDefumGQw_k",
        "outputId": "849ccfab-2005-4c80-9502-89a45ae17b35"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best C value: 0.012742749857031334\n",
            "Best cross-validation accuracy: 1.0000\n",
            "Test set accuracy with optimal C: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions."
      ],
      "metadata": {
        "id": "jrcTWq9oQ0LR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import joblib\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# For simplicity, we'll use only two classes (binary classification)\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Save the trained model to a file using joblib\n",
        "joblib.dump(model, 'logistic_regression_model.pkl')\n",
        "print(\"Model saved successfully!\")\n",
        "\n",
        "# Load the model from the file\n",
        "loaded_model = joblib.load('logistic_regression_model.pkl')\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# Evaluate the model on the test data using the loaded model\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy of the loaded model on the test set\n",
        "print(f\"Accuracy of the loaded model: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZeMz7pyQ5Rh",
        "outputId": "31667a9d-796e-4d09-d2ae-47ccbc7eb2fb"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully!\n",
            "Model loaded successfully!\n",
            "Accuracy of the loaded model: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ckKkFr8OQ_ow"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}